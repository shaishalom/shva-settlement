

file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\config
-----------------------------------------------------
[core]
	repositoryformatversion = 0
	filemode = false
	bare = false
	logallrefupdates = true
	symlinks = false
	ignorecase = true
[remote "origin"]
	url = ssh://git@gitlab.devops.poalim.bank:31007/m28008doc/duecmcustomerdars.git
	fetch = +refs/heads/*:refs/remotes/origin/*
[branch "master"]
	remote = origin
	merge = refs/heads/master
[branch "dev"]
	remote = origin
	merge = refs/heads/dev


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\applypatch-msg.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to check the commit log message taken by
# applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.  The hook is
# allowed to edit the commit message file.
#
# To enable this hook, rename this file to "applypatch-msg".

. git-sh-setup
commitmsg="$(git rev-parse --git-path hooks/commit-msg)"
test -x "$commitmsg" && exec "$commitmsg" ${1+"$@"}
:


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\commit-msg.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to check the commit log message.
# Called by "git commit" with one argument, the name of the file
# that has the commit message.  The hook should exit with non-zero
# status after issuing an appropriate message if it wants to stop the
# commit.  The hook is allowed to edit the commit message file.
#
# To enable this hook, rename this file to "commit-msg".

# Uncomment the below to add a Signed-off-by line to the message.
# Doing this in a hook is a bad idea in general, but the prepare-commit-msg
# hook is more suited to it.
#
# SOB=$(git var GIT_AUTHOR_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# grep -qs "^$SOB" "$1" || echo "$SOB" >> "$1"

# This example catches duplicate Signed-off-by lines.

test "" = "$(grep '^Signed-off-by: ' "$1" |
	 sort | uniq -c | sed -e '/^[ 	]*1[ 	]/d')" || {
	echo >&2 Duplicate Signed-off-by lines.
	exit 1
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\fsmonitor-watchman.sample
-----------------------------------------------------
#!/usr/bin/perl

use strict;
use warnings;
use IPC::Open2;

# An example hook script to integrate Watchman
# (https://facebook.github.io/watchman/) with git to speed up detecting
# new and modified files.
#
# The hook is passed a version (currently 1) and a time in nanoseconds
# formatted as a string and outputs to stdout all files that have been
# modified since the given time. Paths must be relative to the root of
# the working tree and separated by a single NUL.
#
# To enable this hook, rename this file to "query-watchman" and set
# 'git config core.fsmonitor .git/hooks/query-watchman'
#
my ($version, $time) = @ARGV;

# Check the hook interface version

if ($version == 1) {
	# convert nanoseconds to seconds
	$time = int $time / 1000000000;
} else {
	die "Unsupported query-fsmonitor hook version '$version'.\n" .
	    "Falling back to scanning...\n";
}

my $git_work_tree;
if ($^O =~ 'msys' || $^O =~ 'cygwin') {
	$git_work_tree = Win32::GetCwd();
	$git_work_tree =~ tr/\\/\//;
} else {
	require Cwd;
	$git_work_tree = Cwd::cwd();
}

my $retry = 1;

launch_watchman();

sub launch_watchman {

	my $pid = open2(\*CHLD_OUT, \*CHLD_IN, 'watchman -j --no-pretty')
	    or die "open2() failed: $!\n" .
	    "Falling back to scanning...\n";

	# In the query expression below we're asking for names of files that
	# changed since $time but were not transient (ie created after
	# $time but no longer exist).
	#
	# To accomplish this, we're using the "since" generator to use the
	# recency index to select candidate nodes and "fields" to limit the
	# output to file names only. Then we're using the "expression" term to
	# further constrain the results.
	#
	# The category of transient files that we want to ignore will have a
	# creation clock (cclock) newer than $time_t value and will also not
	# currently exist.

	my $query = <<"	END";
		["query", "$git_work_tree", {
			"since": $time,
			"fields": ["name"],
			"expression": ["not", ["allof", ["since", $time, "cclock"], ["not", "exists"]]]
		}]
	END

	print CHLD_IN $query;
	close CHLD_IN;
	my $response = do {local $/; <CHLD_OUT>};

	die "Watchman: command returned no output.\n" .
	    "Falling back to scanning...\n" if $response eq "";
	die "Watchman: command returned invalid output: $response\n" .
	    "Falling back to scanning...\n" unless $response =~ /^\{/;

	my $json_pkg;
	eval {
		require JSON::XS;
		$json_pkg = "JSON::XS";
		1;
	} or do {
		require JSON::PP;
		$json_pkg = "JSON::PP";
	};

	my $o = $json_pkg->new->utf8->decode($response);

	if ($retry > 0 and $o->{error} and $o->{error} =~ m/unable to resolve root .* directory (.*) is not watched/) {
		print STDERR "Adding '$git_work_tree' to watchman's watch list.\n";
		$retry--;
		qx/watchman watch "$git_work_tree"/;
		die "Failed to make watchman watch '$git_work_tree'.\n" .
		    "Falling back to scanning...\n" if $? != 0;

		# Watchman will always return all files on the first query so
		# return the fast "everything is dirty" flag to git and do the
		# Watchman query just to get it over with now so we won't pay
		# the cost in git to look up each individual file.
		print "/\0";
		eval { launch_watchman() };
		exit 0;
	}

	die "Watchman: $o->{error}.\n" .
	    "Falling back to scanning...\n" if $o->{error};

	binmode STDOUT, ":utf8";
	local $, = "\0";
	print @{$o->{files}};
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\post-update.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to prepare a packed repository for use over
# dumb transports.
#
# To enable this hook, rename this file to "post-update".

exec git update-server-info


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\pre-applypatch.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to verify what is about to be committed
# by applypatch from an e-mail message.
#
# The hook should exit with non-zero status after issuing an
# appropriate message if it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-applypatch".

. git-sh-setup
precommit="$(git rev-parse --git-path hooks/pre-commit)"
test -x "$precommit" && exec "$precommit" ${1+"$@"}
:


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\pre-commit.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to verify what is about to be committed.
# Called by "git commit" with no arguments.  The hook should
# exit with non-zero status after issuing an appropriate message if
# it wants to stop the commit.
#
# To enable this hook, rename this file to "pre-commit".

if git rev-parse --verify HEAD >/dev/null 2>&1
then
	against=HEAD
else
	# Initial commit: diff against an empty tree object
	against=$(git hash-object -t tree /dev/null)
fi

# If you want to allow non-ASCII filenames set this variable to true.
allownonascii=$(git config --bool hooks.allownonascii)

# Redirect output to stderr.
exec 1>&2

# Cross platform projects tend to avoid non-ASCII filenames; prevent
# them from being added to the repository. We exploit the fact that the
# printable range starts at the space character and ends with tilde.
if [ "$allownonascii" != "true" ] &&
	# Note that the use of brackets around a tr range is ok here, (it's
	# even required, for portability to Solaris 10's /usr/bin/tr), since
	# the square bracket bytes happen to fall in the designated range.
	test $(git diff --cached --name-only --diff-filter=A -z $against |
	  LC_ALL=C tr -d '[ -~]\0' | wc -c) != 0
then
	cat <<\EOF
Error: Attempt to add a non-ASCII file name.

This can cause problems if you want to work with people on other platforms.

To be portable it is advisable to rename the file.

If you know what you are doing you can disable this check using:

  git config hooks.allownonascii true
EOF
	exit 1
fi

# If there are whitespace errors, print the offending file names and fail.
exec git diff-index --check --cached $against --


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\pre-push.sample
-----------------------------------------------------
#!/bin/sh

# An example hook script to verify what is about to be pushed.  Called by "git
# push" after it has checked the remote status, but before anything has been
# pushed.  If this script exits with a non-zero status nothing will be pushed.
#
# This hook is called with the following parameters:
#
# $1 -- Name of the remote to which the push is being done
# $2 -- URL to which the push is being done
#
# If pushing without using a named remote those arguments will be equal.
#
# Information about the commits which are being pushed is supplied as lines to
# the standard input in the form:
#
#   <local ref> <local sha1> <remote ref> <remote sha1>
#
# This sample shows how to prevent push of commits where the log message starts
# with "WIP" (work in progress).

remote="$1"
url="$2"

z40=0000000000000000000000000000000000000000

while read local_ref local_sha remote_ref remote_sha
do
	if [ "$local_sha" = $z40 ]
	then
		# Handle delete
		:
	else
		if [ "$remote_sha" = $z40 ]
		then
			# New branch, examine all commits
			range="$local_sha"
		else
			# Update to existing branch, examine new commits
			range="$remote_sha..$local_sha"
		fi

		# Check for WIP commit
		commit=`git rev-list -n 1 --grep '^WIP' "$range"`
		if [ -n "$commit" ]
		then
			echo >&2 "Found WIP commit in $local_ref, not pushing"
			exit 1
		fi
	fi
done

exit 0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\pre-rebase.sample
-----------------------------------------------------
#!/bin/sh
#
# Copyright (c) 2006, 2008 Junio C Hamano
#
# The "pre-rebase" hook is run just before "git rebase" starts doing
# its job, and can prevent the command from running by exiting with
# non-zero status.
#
# The hook is called with the following parameters:
#
# $1 -- the upstream the series was forked from.
# $2 -- the branch being rebased (or empty when rebasing the current branch).
#
# This sample shows how to prevent topic branches that are already
# merged to 'next' branch from getting rebased, because allowing it
# would result in rebasing already published history.

publish=next
basebranch="$1"
if test "$#" = 2
then
	topic="refs/heads/$2"
else
	topic=`git symbolic-ref HEAD` ||
	exit 0 ;# we do not interrupt rebasing detached HEAD
fi

case "$topic" in
refs/heads/??/*)
	;;
*)
	exit 0 ;# we do not interrupt others.
	;;
esac

# Now we are dealing with a topic branch being rebased
# on top of master.  Is it OK to rebase it?

# Does the topic really exist?
git show-ref -q "$topic" || {
	echo >&2 "No such branch $topic"
	exit 1
}

# Is topic fully merged to master?
not_in_master=`git rev-list --pretty=oneline ^master "$topic"`
if test -z "$not_in_master"
then
	echo >&2 "$topic is fully merged to master; better remove it."
	exit 1 ;# we could allow it, but there is no point.
fi

# Is topic ever merged to next?  If so you should not be rebasing it.
only_next_1=`git rev-list ^master "^$topic" ${publish} | sort`
only_next_2=`git rev-list ^master           ${publish} | sort`
if test "$only_next_1" = "$only_next_2"
then
	not_in_topic=`git rev-list "^$topic" master`
	if test -z "$not_in_topic"
	then
		echo >&2 "$topic is already up to date with master"
		exit 1 ;# we could allow it, but there is no point.
	else
		exit 0
	fi
else
	not_in_next=`git rev-list --pretty=oneline ^${publish} "$topic"`
	/usr/bin/perl -e '
		my $topic = $ARGV[0];
		my $msg = "* $topic has commits already merged to public branch:\n";
		my (%not_in_next) = map {
			/^([0-9a-f]+) /;
			($1 => 1);
		} split(/\n/, $ARGV[1]);
		for my $elem (map {
				/^([0-9a-f]+) (.*)$/;
				[$1 => $2];
			} split(/\n/, $ARGV[2])) {
			if (!exists $not_in_next{$elem->[0]}) {
				if ($msg) {
					print STDERR $msg;
					undef $msg;
				}
				print STDERR " $elem->[1]\n";
			}
		}
	' "$topic" "$not_in_next" "$not_in_master"
	exit 1
fi

<<\DOC_END

This sample hook safeguards topic branches that have been
published from being rewound.

The workflow assumed here is:

 * Once a topic branch forks from "master", "master" is never
   merged into it again (either directly or indirectly).

 * Once a topic branch is fully cooked and merged into "master",
   it is deleted.  If you need to build on top of it to correct
   earlier mistakes, a new topic branch is created by forking at
   the tip of the "master".  This is not strictly necessary, but
   it makes it easier to keep your history simple.

 * Whenever you need to test or publish your changes to topic
   branches, merge them into "next" branch.

The script, being an example, hardcodes the publish branch name
to be "next", but it is trivial to make it configurable via
$GIT_DIR/config mechanism.

With this workflow, you would want to know:

(1) ... if a topic branch has ever been merged to "next".  Young
    topic branches can have stupid mistakes you would rather
    clean up before publishing, and things that have not been
    merged into other branches can be easily rebased without
    affecting other people.  But once it is published, you would
    not want to rewind it.

(2) ... if a topic branch has been fully merged to "master".
    Then you can delete it.  More importantly, you should not
    build on top of it -- other people may already want to
    change things related to the topic as patches against your
    "master", so if you need further changes, it is better to
    fork the topic (perhaps with the same name) afresh from the
    tip of "master".

Let's look at this example:

		   o---o---o---o---o---o---o---o---o---o "next"
		  /       /           /           /
		 /   a---a---b A     /           /
		/   /               /           /
	       /   /   c---c---c---c B         /
	      /   /   /             \         /
	     /   /   /   b---b C     \       /
	    /   /   /   /             \     /
    ---o---o---o---o---o---o---o---o---o---o---o "master"


A, B and C are topic branches.

 * A has one fix since it was merged up to "next".

 * B has finished.  It has been fully merged up to "master" and "next",
   and is ready to be deleted.

 * C has not merged to "next" at all.

We would want to allow C to be rebased, refuse A, and encourage
B to be deleted.

To compute (1):

	git rev-list ^master ^topic next
	git rev-list ^master        next

	if these match, topic has not merged in next at all.

To compute (2):

	git rev-list master..topic

	if this is empty, it is fully merged to "master".

DOC_END


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\pre-receive.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to make use of push options.
# The example simply echoes all push options that start with 'echoback='
# and rejects all pushes when the "reject" push option is used.
#
# To enable this hook, rename this file to "pre-receive".

if test -n "$GIT_PUSH_OPTION_COUNT"
then
	i=0
	while test "$i" -lt "$GIT_PUSH_OPTION_COUNT"
	do
		eval "value=\$GIT_PUSH_OPTION_$i"
		case "$value" in
		echoback=*)
			echo "echo from the pre-receive-hook: ${value#*=}" >&2
			;;
		reject)
			exit 1
		esac
		i=$((i + 1))
	done
fi


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\prepare-commit-msg.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to prepare the commit log message.
# Called by "git commit" with the name of the file that has the
# commit message, followed by the description of the commit
# message's source.  The hook's purpose is to edit the commit
# message file.  If the hook fails with a non-zero status,
# the commit is aborted.
#
# To enable this hook, rename this file to "prepare-commit-msg".

# This hook includes three examples. The first one removes the
# "# Please enter the commit message..." help message.
#
# The second includes the output of "git diff --name-status -r"
# into the message, just before the "git status" output.  It is
# commented because it doesn't cope with --amend or with squashed
# commits.
#
# The third example adds a Signed-off-by line to the message, that can
# still be edited.  This is rarely a good idea.

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

/usr/bin/perl -i.bak -ne 'print unless(m/^. Please enter the commit message/..m/^#$/)' "$COMMIT_MSG_FILE"

# case "$COMMIT_SOURCE,$SHA1" in
#  ,|template,)
#    /usr/bin/perl -i.bak -pe '
#       print "\n" . `git diff --cached --name-status -r`
# 	 if /^#/ && $first++ == 0' "$COMMIT_MSG_FILE" ;;
#  *) ;;
# esac

# SOB=$(git var GIT_COMMITTER_IDENT | sed -n 's/^\(.*>\).*$/Signed-off-by: \1/p')
# git interpret-trailers --in-place --trailer "$SOB" "$COMMIT_MSG_FILE"
# if test -z "$COMMIT_SOURCE"
# then
#   /usr/bin/perl -i.bak -pe 'print "\n" if !$first_line++' "$COMMIT_MSG_FILE"
# fi


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.git\hooks\update.sample
-----------------------------------------------------
#!/bin/sh
#
# An example hook script to block unannotated tags from entering.
# Called by "git receive-pack" with arguments: refname sha1-old sha1-new
#
# To enable this hook, rename this file to "update".
#
# Config
# ------
# hooks.allowunannotated
#   This boolean sets whether unannotated tags will be allowed into the
#   repository.  By default they won't be.
# hooks.allowdeletetag
#   This boolean sets whether deleting tags will be allowed in the
#   repository.  By default they won't be.
# hooks.allowmodifytag
#   This boolean sets whether a tag may be modified after creation. By default
#   it won't be.
# hooks.allowdeletebranch
#   This boolean sets whether deleting branches will be allowed in the
#   repository.  By default they won't be.
# hooks.denycreatebranch
#   This boolean sets whether remotely creating branches will be denied
#   in the repository.  By default this is allowed.
#

# --- Command line
refname="$1"
oldrev="$2"
newrev="$3"

# --- Safety check
if [ -z "$GIT_DIR" ]; then
	echo "Don't run this script from the command line." >&2
	echo " (if you want, you could supply GIT_DIR then run" >&2
	echo "  $0 <ref> <oldrev> <newrev>)" >&2
	exit 1
fi

if [ -z "$refname" -o -z "$oldrev" -o -z "$newrev" ]; then
	echo "usage: $0 <ref> <oldrev> <newrev>" >&2
	exit 1
fi

# --- Config
allowunannotated=$(git config --bool hooks.allowunannotated)
allowdeletebranch=$(git config --bool hooks.allowdeletebranch)
denycreatebranch=$(git config --bool hooks.denycreatebranch)
allowdeletetag=$(git config --bool hooks.allowdeletetag)
allowmodifytag=$(git config --bool hooks.allowmodifytag)

# check for no description
projectdesc=$(sed -e '1q' "$GIT_DIR/description")
case "$projectdesc" in
"Unnamed repository"* | "")
	echo "*** Project description file hasn't been set" >&2
	exit 1
	;;
esac

# --- Check types
# if $newrev is 0000...0000, it's a commit to delete a ref.
zero="0000000000000000000000000000000000000000"
if [ "$newrev" = "$zero" ]; then
	newrev_type=delete
else
	newrev_type=$(git cat-file -t $newrev)
fi

case "$refname","$newrev_type" in
	refs/tags/*,commit)
		# un-annotated tag
		short_refname=${refname##refs/tags/}
		if [ "$allowunannotated" != "true" ]; then
			echo "*** The un-annotated tag, $short_refname, is not allowed in this repository" >&2
			echo "*** Use 'git tag [ -a | -s ]' for tags you want to propagate." >&2
			exit 1
		fi
		;;
	refs/tags/*,delete)
		# delete tag
		if [ "$allowdeletetag" != "true" ]; then
			echo "*** Deleting a tag is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/tags/*,tag)
		# annotated tag
		if [ "$allowmodifytag" != "true" ] && git rev-parse $refname > /dev/null 2>&1
		then
			echo "*** Tag '$refname' already exists." >&2
			echo "*** Modifying a tag is not allowed in this repository." >&2
			exit 1
		fi
		;;
	refs/heads/*,commit)
		# branch
		if [ "$oldrev" = "$zero" -a "$denycreatebranch" = "true" ]; then
			echo "*** Creating a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/heads/*,delete)
		# delete branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	refs/remotes/*,commit)
		# tracking branch
		;;
	refs/remotes/*,delete)
		# delete tracking branch
		if [ "$allowdeletebranch" != "true" ]; then
			echo "*** Deleting a tracking branch is not allowed in this repository" >&2
			exit 1
		fi
		;;
	*)
		# Anything else (is there anything else?)
		echo "*** Update hook: unknown type of update to ref $refname of type $newrev_type" >&2
		exit 1
		;;
esac

# --- Finished
exit 0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.gitignore
-----------------------------------------------------
# Created by https://www.gitignore.io/api/eclipse

### Eclipse ###

.metadata
bin/
tmp/
*.tmp
*.bak
*.swp
*~.nib
local.properties
#.settings/
.loadpath
.recommenders

# compiled output
/dist
/tmp
/out-tsc

# dependencies
/node_modules

# IDEs and editors
/.idea
#.project
#.classpath
.c9/
*.launch
.settings/
*.sublime-workspace

# IDE - VSCode
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json

# misc
/.sass-cache
/connect.lock
/coverage
/libpeerconnection.log
npm-debug.log
testem.log
/typings

# e2e
/e2e/*.js
/e2e/*.map

# System Files
.DS_Store
Thumbs.db

# Eclipse Core
#.project

# External tool builders
.externalToolBuilders/

# Locally stored "Eclipse launch configurations"
*.launch

# PyDev specific (Python IDE for Eclipse)
*.pydevproject

# CDT-specific (C/C++ Development Tooling)
.cproject

# JDT-specific (Eclipse Java Development Tools)
#.classpath

# Java annotation processor (APT)
.factorypath

# PDT-specific (PHP Development Tools)
.buildpath

# sbteclipse plugin
.target

# Tern plugin
.tern-project

# TeXlipse plugin
.texlipse

# STS (Spring Tool Suite)
.springBeans

# Code Recommenders
.recommenders/

#NPM
target/
bowerrc.
node_modules/

#JAVA
*.jar
!bnhp_storage_selection_service_iface.jar

#emacs
*~
\#*\#

# documentum composer
C:*
*.dar
*.installparam
.dfsproject
#.dmproject
#.template
quick-publish.sh


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\.gitlab-ci.yml
-----------------------------------------------------
image: "docker-28008-repo.repo.devops.poalim.bank/dctm-build-full:0.0.3"
stages:
  - build_and_deploy_for_dars
  - release

build_and_deploy_for_dars_job:
  stage: build_and_deploy_for_dars
  script:
    - echo "job build_and_deploy dars started"
    - set -x
    - echo "******************************************* start BnhpBusinessDivision *****************************"
    - cd BnhpBusinessDivision  
    - mvn -X clean install -DskipTests
    - echo "******************************************* finish BnhpBusinessDivision *****************************"
    - cd ..
    - pwd
    # - echo "******************************************* start BnhpInfraDFArtifacts *****************************"
    # - cd BnhpInfraDFArtifacts  
    # - mvn -X clean install -DskipTests
    # - echo "******************************************* finish BnhpInfraDFArtifacts *****************************"
    # - cd ..
    # - pwd
    # - echo "******************************************* start BnhpInfraRequests *****************************"
    # - cd BnhpInfraRequests  
    # - mvn -X clean install -DskipTests
    # - echo "******************************************* finish BnhpInfraRequests *****************************"
    # - cd ..
    - echo "******************************************* start MismachiTipulBelikuim *****************************"
    - cd MismachiTipulBelikuim  
    - mvn -X clean install -DskipTests
    - echo "******************************************* finish MismachiTipulBelikuim *****************************"
    - cd ..
    - pwd
    - echo "job build_and_deploy finished"
  only:
    - dev    
    - master
    - fix/dfs-build-errors    
  tags:
    - k8s

release_job:
  stage: release
  variables:
    GIT_STRATEGY: clone
    GIT_DEPTH: 0
  before_script:
#    - git remote set-url origin https://${GITLAB_USER_NAME}:${TOKEN}@${CI_PROJECT_URL:8}.git
    - git config --global user.email ${GITLAB_USER_EMAIL}
    - git config --global user.name ${GITLAB_USER_NAME}
    - echo "Running job"
  script:
    - set -x  
    - mkdir -p ~/.ssh
    - echo "$GITLAB_SSH_KEY" > /root/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan -p 31007 gitlab.devops.poalim.bank >> ~/.ssh/known_hosts
    - git checkout -B "$CI_BUILD_REF_NAME"
#    - mvn -X  -Dmaven.javadoc.skip=true -DignoreSnapshots=true release:prepare -B -Darguments="-DskipTests" -DscmCommentPrefix="[ci skip]" -Dresume=false -DdryRun -DautoVersionSubmodules=true
#    - mvn -X  -Dmaven.javadoc.skip=true -DignoreSnapshots=true release:perform -B -Darguments="-DskipTests" -DscmCommentPrefix="[ci skip]" -Dresume=false -DdryRun -DautoVersionSubmodules=true
    - echo "API released finished"
  only:
    - dev    
    - master
    - fix/dfs-build-errors
  tags:
    - k8s
 



file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\bin-dar\keepme.txt
-----------------------------------------------------
keep me


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\bof_package.xml
-----------------------------------------------------
<project name="bof_package" default="buildAndRefresh">
  <!--This scripts rebuild jar files of BOF and copies them into 
      proper place for composer to see updated files. It's 
      supposed to be run from within eclipse ant builder
      
      For DAR file to be properly built this script must be envoked 
      BEFORE building of dar, that is ant builder must be envoked before 
      Documentum Builder in the eclipse build configuration
  -->
  
  
  <!-- this defines task for building/copiying individual jar -->
  <macrodef name="build_jar_artifact">
    <!-- filename of jar to be built -->
    <attribute name="name"/>
    <!-- files to put into jar (used by jar task) -->
    <attribute name="classfilter"/>
    <!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
    <attribute name="jardef"/>
    <sequential>
      <xmlproperty prefix="@{name}" file="@{jardef}"/>
      <echo message="** rebuilding ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
      <echo message="   class filter: @{classfilter}"/>
      <!--  delete jar file in project root -->
      <delete verbose="false" file="@{name}" failonerror="false"/> 
      
      <!--  make jar file in project root -->			 
      <jar destfile="@{name}" 
	   basedir="target/classes" includes="@{classfilter}">
      </jar>
      <!--  copy jar to proper location (content/...) that is known to composer -->
      <!-- force tries to override read-only protection (those files must not be stored in VCS) --> 
      <move file="@{name}" 
 	    tofile="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"
 	    overwrite="true" 
 	    force="true" 
 	    failonerror="true"
 	    verbose="true"/>
    </sequential>
  </macrodef>
  
  <!-- this defines task for deleting individual jar -->
  <macrodef name="clean_jar_artifact">
    <!-- filename of jar to be built -->
    <attribute name="name"/>
    <!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
    <attribute name="jardef"/>
    <sequential>
      <xmlproperty prefix="@{name}" file="@{jardef}"/>
      <echo message="** cleaning ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
      <!--  delete jar file in project root -->
      <delete file="@{name}" failonerror="true"/>
      <delete verbose="true" file="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}" failonerror="true"/> 
    </sequential>
  </macrodef>

  <target name="clean">
    <clean_jar_artifact 
	name="BnhpDivisionBusinessAspect.jar" 
	jardef="Artifacts/JAR Definitions/bnhp_division_business_aspect.jardef"/>
  </target>
  <target name="cleanAndRefresh">
    <antcall target="clean" />
    <!--eclipse.refreshLocal resource="/" depth="infinite"/-->
  </target>
  
  <target name="build">
    <build_jar_artifact 
	name="BnhpDivisionBusinessAspect.jar" 
	classfilter="bnhp/infra/base/aspect/*"
	jardef="Artifacts/JAR Definitions/bnhp_division_business_aspect.jardef"/>
    <!--<eclipse.refreshLocal resource="/" depth="infinite"/>-->
  </target>
  <target name="buildAndRefresh">
    <antcall target="build" />
    <!--eclipse.refreshLocal resource="/" depth="infinite"/-->
  </target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>

	<artifactId>BnhpBusinessDivision</artifactId>
	<packaging>jar</packaging>
	<name>BnhpBusinessDivision</name>
	<description>BnhpBusinessDivision</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<!--dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency-->

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\src\bnhp\infra\base\aspect\BnhpDivisionBusinessAspect.java
-----------------------------------------------------

package bnhp.infra.base.aspect;

import java.util.Date;

import com.documentum.fc.client.DfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.DfTime;
import com.documentum.fc.common.IDfId;
import com.documentum.fc.common.IDfTime;

public class BnhpDivisionBusinessAspect extends DfDocument {
	@Override
	protected void doSave(boolean keeplock, 
			String  versionLabels, Object[] extendedArgs) throws DfException {
		boolean hasChanged = this.isDirty();
		//DfLogger.debug(this, "BnhpDivisionBusinessAspect::doSave hasChanged="+hasChanged, null,	null);
		super.doSave(keeplock, versionLabels, extendedArgs);
		//if (hasChanged || this.isNew()) {
			//DfLogger.debug(this, "BnhpDivisionBusinessAspect::doSave queuing for indexing", null,	null);
			//requestFTIndexing(this, "dm_save");
		//}
	}
	
	protected void requestFTIndexing(IDfPersistentObject obj, String operation) throws DfException {
		IDfTime time = new DfTime(new Date());
		this.queue("dm_fulltext_index_user", operation , 0, false, time, "");
	}

	@Override
	protected IDfId doCheckin(boolean keeplock,String versionLabels,
			String oldCompoundArchValue,
			String oldSpecialAppValue,
			String newCompoundArchValue,
			String newSpecialAppValue,
		Object[] extraArgs) throws DfException {
		IDfId result =  super.doCheckin(keeplock, 
				versionLabels, oldCompoundArchValue, oldSpecialAppValue,
				newCompoundArchValue, newSpecialAppValue,extraArgs);
		//IDfPersistentObject obj = this.getSession().getObject(result);
		//DfLogger.debug(this, "BnhpDivisionBusinessAspect::doCheckin queuing for indexing", null,	null);
		//requestFTIndexing(obj, "dm_checkin");
		return result;
	}
	
	@Override
	protected void doDestroy(boolean force, Object[] extraArgs) throws DfException {
		//DfLogger.debug(this, "BnhpDivisionBusinessAspect::doDestroy queuing for indexing", null,	null);
		//requestFTIndexing(this, "dm_destroy");
		super.doDestroy(force, extraArgs);
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\target\antrun\build-main.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8" ?>
<project name="maven-antrun-" default="main"  >
<target name="main">
  <property name="project" value="BnhpBusinessDivision"/>
  <property name="headlesscomposer" value="C:/Users/AP068/dctm/HeadlessComposer//7.1"/>
  <ant antfile="${maven.dependency.com.poalim.documentum.build-dar.xml.path}" target="run-build-dar"/>
</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\target\classes\META-INF\maven\com.poalim.documentum\BnhpBusinessDivision\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Thu Mar 11 15:46:18 IST 2021
version=1.0
groupId=com.poalim.documentum
m2e.projectName=BnhpBusinessDivision
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\BnhpBusinessDivision
artifactId=BnhpBusinessDivision


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\target\classes\META-INF\maven\com.poalim.documentum\BnhpBusinessDivision\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>

	<artifactId>BnhpBusinessDivision</artifactId>
	<packaging>jar</packaging>
	<name>BnhpBusinessDivision</name>
	<description>BnhpBusinessDivision</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<!--dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency-->

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\target\maven-archiver\pom.properties
-----------------------------------------------------
#Generated by Maven
#Thu Mar 11 15:43:56 IST 2021
version=1.0
groupId=com.poalim.documentum
artifactId=BnhpBusinessDivision


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpBusinessDivision\version.properties
-----------------------------------------------------
#Thu Nov 03 13:12:48 IST 2016
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\bin-dar\keepme.txt
-----------------------------------------------------
keep me


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\bof_package.xml
-----------------------------------------------------
<project name="bof_package" default="buildAndRefresh">
	<!--This scripts rebuild jar files of BOF and copies them into 
		proper place for composer to see updated files. It's 
		supposed to be run from within eclipse ant builder
		
		For DAR file to be properly built this script must be envoked 
		BEFORE building of dar, that is ant builder must be envoked before 
		Documentum Builder in the eclipse build configuration
    -->
	
 
 	<!-- this defines task for building/copiying individual jar -->
 	<macrodef name="build_jar_artifact">
 		<!-- filename of jar to be built -->
 		<attribute name="name"/>
 		<!-- files to put into jar (used by jar task) -->
 		<attribute name="classfilter"/>
 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
 		<attribute name="jardef"/>
 		<sequential>
 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
			<echo message="** rebuilding ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
			<echo message="   class filter: @{classfilter}"/>
			<!--  delete jar file in project root -->
			<delete verbose="false" file="@{name}" failonerror="false"/> 
			 
			<!--  make jar file in project root -->			 
			<jar destfile="@{name}" 
				basedir="target/classes" includes="@{classfilter}">
			</jar>
 			<!--  copy jar to proper location (content/...) that is known to composer -->
 				 			<!-- force tries to override read-only protection (those files must not be stored in VCS) --> 
 							<move file="@{name}" 
 								tofile="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"
 								overwrite="true" 
 								force="true" 
 								failonerror="true"
 				 				verbose="true"/>
 		</sequential>
 	</macrodef>
 		
	<!-- this defines task for deleting individual jar -->
	<macrodef name="clean_jar_artifact">
	 		<!-- filename of jar to be built -->
	 		<attribute name="name"/>
	 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
	 		<attribute name="jardef"/>
	 		<sequential>
	 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
				<echo message="** cleaning ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
							<!--  delete jar file in project root -->
				<delete file="@{name}" failonerror="true"/>
	 			<delete verbose="true" file="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}" failonerror="true"/> 
	 		</sequential>
	 	</macrodef>

	<target name="clean">
		<clean_jar_artifact 
			name="GeneralDocAspect.jar" 
			jardef="Artifacts/JAR Definitions/general_doc_aspect.jardef"/>
		<clean_jar_artifact 
			name="EmptyDocAspect.jar" 
			jardef="Artifacts/JAR Definitions/empty_doc_aspect.jardef"/>
		<clean_jar_artifact 
			name="customer_relation_impl.jar" 
			jardef="Artifacts/JAR Definitions/customer_relation_impl.jardef"/>
		<clean_jar_artifact 
			name="customer_relation_iface.jar" 
			jardef="Artifacts/JAR Definitions/customer_relation_iface.jardef"/>
		<clean_jar_artifact 
			name="sync_object_iface.jar" 
			jardef="Artifacts/JAR Definitions/sync_object_iface.jardef"/>
		<clean_jar_artifact 
			name="customer_doc_impl.jar" 
			jardef="Artifacts/JAR Definitions/customer_doc_impl.jardef"/>
		<clean_jar_artifact 
			name="customer_doc_iface.jar" 
			jardef="Artifacts/JAR Definitions/customer_doc_iface.jardef"/>
		<clean_jar_artifact 
					name="migration_config_impl.jar" 
					jardef="Artifacts/JAR Definitions/migration_config_impl.jardef"/>
		<clean_jar_artifact 
					name="migration_config_iface.jar" 
					jardef="Artifacts/JAR Definitions/migration_config_iface.jardef"/>
	</target>
	<target name="cleanAndRefresh">
			<antcall target="clean" />
			<eclipse.refreshLocal resource="/" depth="infinite"/>
		</target>
	
	<target name="build">
		<build_jar_artifact 
			name="GeneralDocAspect.jar" 
			classfilter="bnhp/infra/base/aspect/GeneralDocAspect*"
			jardef="Artifacts/JAR Definitions/general_doc_aspect.jardef"/>
		<build_jar_artifact 
			name="EmptyDocAspect.jar" 
			classfilter="bnhp/infra/base/aspect/EmptyDocAspect*"
			jardef="Artifacts/JAR Definitions/empty_doc_aspect.jardef"/>
		<build_jar_artifact 
			name="customer_relation_impl.jar" 
			classfilter="bnhp/infra/base/tbo/CustomerRelation.class,bnhp/infra/base/tbo/TableObject.class,bnhp/infra/base/utils/*.class"
			jardef="Artifacts/JAR Definitions/customer_relation_impl.jardef"/>
		<build_jar_artifact 
			name="customer_relation_iface.jar" 
			classfilter="bnhp/infra/base/tbo/ICustomerRelation*"
			jardef="Artifacts/JAR Definitions/customer_relation_iface.jardef"/>
		<build_jar_artifact 
			name="sync_object_iface.jar" 
			classfilter="bnhp/infra/base/tbo/ISyncObject*"
			jardef="Artifacts/JAR Definitions/sync_object_iface.jardef"/>
		<build_jar_artifact 
			name="customer_doc_impl.jar" 
			classfilter="com/documentum/fc/client/customerdoc/tbo/CustomerDoc.class,bnhp/infra/base/tbo/TableObject.class,bnhp/infra/base/utils/*.class"
			jardef="Artifacts/JAR Definitions/customer_doc_impl.jardef"/>
		<build_jar_artifact 
			name="customer_doc_iface.jar" 
			classfilter="com/documentum/fc/client/customerdoc/tbo/ICustomerDoc*"
			jardef="Artifacts/JAR Definitions/customer_doc_iface.jardef"/>
		<build_jar_artifact 
					name="migration_config_impl.jar" 
					classfilter="bnhp/infra/base/sbo/MigrationConfigService*.class"
					jardef="Artifacts/JAR Definitions/migration_config_impl.jardef"/>
		<build_jar_artifact 
					name="migration_config_iface.jar" 
					classfilter="bnhp/infra/base/sbo/IMigrationConfigService*.class"
					jardef="Artifacts/JAR Definitions/migration_config_iface.jardef"/>
		<!--<eclipse.refreshLocal resource="/" depth="infinite"/>-->
	</target>
	<target name="buildAndRefresh">
		<antcall target="build" />
		<eclipse.refreshLocal resource="/" depth="infinite"/>
	</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\content\21\-1802600621\bnhp_storage_selector_bnhp_customer_doc.xml
-----------------------------------------------------
<?xml version="1.0" ?>
	<StorageRules>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>snif_lelo_niyar</Value>
			</RuleFactor>
			<StorageType>Tofes Peula</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>fax</Value>
			</RuleFactor>
			<StorageType>fax project</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>yesod</Value>
			</RuleFactor>
			<StorageType>Customer Doc</StorageType>
		</RuleLine>
	</StorageRules>
	


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\content\21\-1802600621\bnhp_storage_selector_bnhp_customer_doc[1].xml
-----------------------------------------------------
<?xml version="1.0" ?>
	<StorageRules>
	    <RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>2</Value>
			</RuleFactor>
			<StorageType>filestore_01</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>2</Value>
			</RuleFactor>
			<StorageType>filestore_01</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>filestore_01</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>project_id</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>0</Value>
			</RuleFactor>
			<StorageType>filestore_01</StorageType>
		</RuleLine>
	</StorageRules>
	
	


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>

	<artifactId>BnhpInfraDFArtifacts</artifactId>
	<packaging>jar</packaging>
	<name>BnhpInfraDFArtifacts</name>
	<description>BnhpInfraDFArtifacts</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency>

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\aspect\EmptyDocAspect.java
-----------------------------------------------------
package bnhp.infra.base.aspect;

public class EmptyDocAspect {

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\aspect\GeneralDocAspect.java
-----------------------------------------------------
package bnhp.infra.base.aspect;

import java.util.Date;
import java.util.List;

import bnhp.infra.base.storage.iface.IBnhpStorageSelectionService;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfDocument;
import com.documentum.fc.client.DfTypedObjectException;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.DfTime;
import com.documentum.fc.common.DfUtil;
import com.documentum.fc.common.IDfId;
import com.documentum.fc.common.IDfList;
import com.documentum.fc.common.IDfTime;
import com.documentum.fc.client.aspect.IDfAspects;

public class GeneralDocAspect extends DfDocument {
	private static final String DCTM_DOCUMENT_ID = "dctm_document_id";
	private static final String LEGACY_DOCUMENT_ID = "legacy_document_id";
	private static final String VERSION_TREE_ROOT_IND = "version_tree_root_ind";
	private static final String EXECUTING_DIVISION_ID = "executing_division_id";
	private static final String ACCOUNT_BANK_ID = "account_bank_id";
	private static final String BRANCH_ID = "branch_id";
	private static final String DIVISION_ID = "division_id";
 
	@Override
	protected void doSetString(String name, int pos, String val, Object [] extra) throws DfException {
		int dotPos = 0;
		if (null!=name && (dotPos = name.indexOf('.'))>0) {
			final String aspectName = name.substring(0, dotPos);
			final IDfList aspects =  this.getAspects();
			if (null==aspects || aspects.findStringIndex(aspectName)<0) {
				((IDfAspects)this).attachAspect(aspectName, null);
				//System.err.println( "GeneralDocAspect:: BUGFIX: auto adding aspect for "+name);
				DfLogger.warn(this, "GeneralDocAspect:: BUGFIX: auto adding aspect for "+name, null, null);
			}
		}
		super.doSetString(name,pos,val, extra);
	}
	
	@Override
	protected String doGetString(String name, int pos, Object[] extra) throws DfException {
		try {
			final String result = super.doGetString(name, pos, extra);
			return result;
		} catch (DfTypedObjectException tex) {
			System.err.println("*** got exception: "+tex);
			final String msg = tex.getMessage();
			if (null!=msg && (msg.contains("DM_API_E_BADATTRNAME") ||  msg.contains("Bad attribute name")) && 
					msg.contains("bnhp_division_business")) {
				//System.err.println( "GeneralDocAspect:: BUGFIX: faking getString for attribute"+name);
				DfLogger.warn(this, "GeneralDocAspect:: BUGFIX: faking getString for attribute" +name, null,null);
				return "-1";
			}
			throw tex;
		}
	}


	/**
	 * Some 
	 */
	@Override
	public IDfTime getTime(final String name) throws DfException {
		try {
			final IDfTime result = super.getTime(name);
			return result;
		} catch (DfTypedObjectException tex) {
			System.err.println("*** got exception: "+tex);
			final String msg = tex.getMessage();
			if (null!=msg && (msg.contains("DM_API_E_BADATTRNAME") ||  msg.contains("Bad attribute name")) && 
					msg.contains("bnhp_division_business")) {
				//System.err.println( "GeneralDocAspect:: BUGFIX: faking getTime for attribute"+ name);
				DfLogger.warn(this, "GeneralDocAspect:: BUGFIX: faking getTime for attribute" +name, null,null);
				return new DfTime(new Date(0));
			}
			throw tex;
		}
	}

	protected void updateExDivId() throws DfException {
		if (!this.isDirty()) {
			return;
		}
		int exDivId = this.getInt(EXECUTING_DIVISION_ID);
		if (exDivId >= 0) {
			// set explicitly
			return;
		}
		int exBankId =  this.getInt("executing_bank_id") % 900;
		int exBranchId =  this.getInt("executing_branch_id");
		if (exBankId < 0 || exBranchId < 0) {
			// not enough attributes
			return;
		}
		exDivId = getExDivId(exBankId, exBranchId);
		if (exDivId >= 0) {
			this.setInt(EXECUTING_DIVISION_ID, exDivId);
		}
	}

	protected void updateAcDivId() throws DfException {
		if (!this.isDirty()) {
			return;
		}
		int divCnt = this.getValueCount(DIVISION_ID);
		//System.out.println("divCnt="+divCnt);
		for (int i = 0;  i < divCnt; i++) {
			//System.out.println("i="+i);
			if (i>=this.getValueCount(ACCOUNT_BANK_ID) ||
				i>=this.getValueCount(BRANCH_ID)) {
				break;
			}
			int divId = this.getRepeatingInt(DIVISION_ID, i);
			//System.out.println("divId="+divId);
			if (divId > 0) {
				// set explicitly
				continue;
			}
			int bankId   = this.getRepeatingInt(ACCOUNT_BANK_ID, i) % 900;
			int branchId = this.getRepeatingInt(BRANCH_ID, i);
			//System.out.println("bankId="+bankId+ " branchId="+branchId);
			if (bankId < 0 || branchId < 0) {
				// not enough attributes
				continue;
			}
			//System.out.println("retrieving divId");
			divId = getExDivId(bankId, branchId);
			//System.out.println("divId="+divId);
			if (divId >= 0) {
				//	System.out.println("divId="+divId);
				this.setRepeatingInt(DIVISION_ID,i, divId);
			}
		}
	}
	
	private static final String SHIYUCH_MANKALI = "shiyuch_mankali";

	protected int getExDivId(final int exBankId, final int exBranchId) throws DfException {
		StringBuilder b = new StringBuilder(128);
		b.append("select ").append(SHIYUCH_MANKALI).append(" AS ").append(SHIYUCH_MANKALI).append(" from DPT.DPT0018 WHERE MISPAR_SNIF=").
			append(exBranchId).append(" AND ").
			append("MISPAR_BANK=").append(exBankId);
		int result = -1;
		IDfQuery query = new DfQuery();
		query.setDQL(b.toString());
		IDfCollection col = null;
		try{
			col = query.execute(this.getSession(), 0);
			if(col != null && col.next()){
				result =  col.getInt(SHIYUCH_MANKALI);
			}
		} catch(DfException ex){
			DfLogger.error(this, "failed to read DPT0018 table: "+ex.getMessage(), null, ex);
		} finally {
			if (null!=col) {
				try {
					col.close();
				} catch (DfException dfex) {
					DfLogger.error(this, "failed to close collection: ", null, dfex);
				}
			}
		}
		return result;
	}

	
	@Override
	protected synchronized void doSave(boolean keepLock, String versionLabels,
			Object[] extendArgs) throws DfException {
		DfLogger.debug(this, "GeneralDocAspect:: entered doSave", null, null);
		boolean isNew = isNew();
		if (isNew) {
			setString(DCTM_DOCUMENT_ID, buildDctmDocumentId());
		}
		if (this.isDirty() && !this.isDeleted()) {
			doCheckBusinessConstraints();
		}
		DfLogger.debug(this, "GeneralDocAspect:: applying storage policy",
				null, null);
		applyStoragePolicy();
		// update version root flag if needed
		boolean oldRootFlag = this.getBoolean(VERSION_TREE_ROOT_IND);
		boolean rootFlag = this.getObjectId().equals(this.getChronicleId());
		if (oldRootFlag!=rootFlag) {
			this.setBoolean(VERSION_TREE_ROOT_IND, rootFlag);
		}
		updateExDivId();
		updateAcDivId();
		
		try {
			super.doSave(keepLock, versionLabels, extendArgs);
		} catch (DfException ex) {
			if (ex.getMessage().contains("ORA-00001") && 
					ex.getMessage().contains("DCTM.UNIQUE_LEGACY_ID_IDX")) {
				throw new DfException(
						"Code:600 Business constraint violated: Non-unique value '"
								+ this.getString(LEGACY_DOCUMENT_ID) + "' in field " + LEGACY_DOCUMENT_ID
								+ " of object " + this.getObjectId().getId()
								/*+ ". The existing id is: " + existingDocumentId*/);
			} else {
				throw ex;
			}
		}
	}

	@Override
	protected synchronized IDfId doCheckin(boolean arg0, String arg1,
			String arg2, String arg3, String arg4, String arg5, Object[] arg6)
			throws DfException {
		IDfId result = null; 
		boolean oldRootFlag = this.getBoolean(VERSION_TREE_ROOT_IND);
		try {
			if (this.isDirty()) {
				doCheckBusinessConstraints();
			}
			//updateExDivId();
			//updateAcDivId();
			applyStoragePolicy(true);
			// if we create version, new document should have root flag set to false
			if (oldRootFlag) {
				this.setBoolean(VERSION_TREE_ROOT_IND, false);
			}
			result = super.doCheckin(arg0, arg1, arg2, arg3, arg4, arg5, arg6);
			return result;
		} finally {
			if (null==result && oldRootFlag) {
				// checkin failed - return old value
				// do not think that really needed, 
				// but makes me feel better
				this.setBoolean(VERSION_TREE_ROOT_IND, true);
			}
		}
	}

	/**
	 * Checks business level constraints on object
	 * 
	 * must be called when saving object, that is from doSave, doCheckIn and
	 * like
	 * 
	 * throws exception in case if current attributes
	 * 
	 */
	protected void doCheckBusinessConstraints() throws DfException {
		/*
		 * check uniqueness of LEGACY_DOCUMENT_ID if it's set
		 * 
		 * check is performed for all documents (current and not) for all other
		 * version trees
		 */
		String legacyId = getString(LEGACY_DOCUMENT_ID).trim();
		//nullString is a default value we set for string field
		if (legacyId.length() > 0 && !legacyId.equals("nullString")) {
			StringBuffer buf = new StringBuffer(100);
			buf.append(this.getTypeName()).append(" (all) where ").append(
					LEGACY_DOCUMENT_ID + "='").append(
					DfUtil.escapeQuotedString(legacyId)).append("'");
			/*
			 * little performance optimization: if object is new and root of
			 * version tree there is no sence to check for it's children
			 */
			if (!(isNew() && this.getObjectId().equals(this.getChronicleId()))) {
				buf.append(" and i_chronicle_id<>ID('").append(
						this.getChronicleId().toString()).append("')")
						.toString();
			}
			IDfPersistentObject id = getSession().getObjectByQualification(
					buf.toString());
			if (null != id) {
				String existingDocumentId = id.getString(DCTM_DOCUMENT_ID);
				throw new DfException(
						"Code:600 Business constraint violated: Non-unique value '"
								+ legacyId + "' in field " + LEGACY_DOCUMENT_ID
								+ " of object " + this.getObjectId().getId()
								+ ". The existing id is: " + existingDocumentId);
			}
		} else if ("nullString".equals(legacyId)) {
			// KLUDGE ALERT: need to change data population in 
			// the app
			this.setString(LEGACY_DOCUMENT_ID, "");
		}
	}

	public void applyStoragePolicy() throws DfException {
			applyStoragePolicy(false);
	}
	
	public void applyStoragePolicy(boolean force) throws DfException {
	
	
		if (isDirty() && (!isDeleted())) {
			IDfClient client = DfClient.getLocalClient();
			IBnhpStorageSelectionService service = (IBnhpStorageSelectionService) client
					.newService(IBnhpStorageSelectionService.class.getName(),
							getSessionManager());
			String newStorage = service.getStorageDest(this);
			if (null != newStorage) {
				if (force || (!newStorage.equals(this.getStorageType()))) {
					setStorageType(newStorage);
				}
			}
		}
		DfLogger.debug(this, "GeneralDocAspect:: applied storage policy", null,
				null);
	}

	private static String buildDctmDocumentId() {
		return java.util.UUID.randomUUID().toString();
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\sbo\IMigrationConfigService.java
-----------------------------------------------------
package bnhp.infra.base.sbo;

import com.documentum.fc.client.IDfSession;

public interface IMigrationConfigService{

	public boolean isSyncNewToOldEnabled(IDfSession session);

	public boolean isSyncOldToNewEnabled(IDfSession session);

	public String getFolderID(IDfSession session);
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\sbo\MigrationConfigService.java
-----------------------------------------------------
package bnhp.infra.base.sbo;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.DfService;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;

public class MigrationConfigService extends DfService implements IMigrationConfigService {
	private long sUpdateIntervalMS = 1000 * 60;
	
	private class MigrationConfigData {
		private boolean syncNewToOldEnabled = false;
		private boolean syncOldToNewEnabled = false;
		private String folderId = null;
		private long updated = 0;
	}
	private static volatile MigrationConfigData sConfigData = null;
	
	
	public boolean isSyncNewToOldEnabled(IDfSession session) {
			MigrationConfigData data = getConfigData(session);
			return data.syncNewToOldEnabled;
	}
	
	public boolean isSyncOldToNewEnabled(IDfSession session) {
		MigrationConfigData data = getConfigData(session);
		return data.syncOldToNewEnabled;
	}
	
	public String getFolderID(IDfSession session){
		MigrationConfigData data = getConfigData(session);
		return data.folderId;
	}
	
	
	
	
	private MigrationConfigData getConfigData(IDfSession session) {
		MigrationConfigData configData = this.sConfigData;
		if (null!=configData && (System.currentTimeMillis() - configData.updated) <  sUpdateIntervalMS) {
			return configData;
		}
		return initializeConfigAttrs(session);
	}

	private synchronized MigrationConfigData  initializeConfigAttrs(IDfSession dfSession) {
		MigrationConfigData configData = this.sConfigData;
		if (null!=configData && (System.currentTimeMillis() - configData.updated) <  sUpdateIntervalMS) {
			return configData;
		}
		MigrationConfigData newConfigData = new MigrationConfigData();
		String str = "select folder_id, old_to_new, new_to_old from bnhp_migration_config" ;
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = null;
		try{
			col = query.execute(dfSession, 0);
		
			if(col != null && col.next()){
				newConfigData.folderId = col.getString("folder_id");
				newConfigData.syncNewToOldEnabled = col.getBoolean("new_to_old");
				newConfigData.syncOldToNewEnabled = col.getBoolean("old_to_new");
			}else{
				newConfigData.syncNewToOldEnabled = false;
				newConfigData.syncOldToNewEnabled = false;
			}
			newConfigData.updated = System.currentTimeMillis();
			sConfigData = newConfigData;
			return newConfigData;
		} catch(DfException ex){
			DfLogger.error(this, "failed to read migration config data: "+ex.getMessage(), null, ex);
			if (null==configData) {
				configData = new MigrationConfigData();
				configData.syncNewToOldEnabled = false;
				configData.syncOldToNewEnabled = false;
			}
			return configData;
		} finally {
			if (null!=col) {
				try {
					col.close();
				} catch (DfException dfex) {
					DfLogger.error(this, "failed to close collection: ", null, dfex);
				}
			}
		}
		
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\tbo\CustomerRelation.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import java.util.HashMap;
import java.util.Map;

import bnhp.infra.base.sbo.IMigrationConfigService;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.DfRelation;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfType;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.IDfId;

public class CustomerRelation extends DfRelation implements ICustomerRelation, ISyncObject {
	
	private static final String EVENT_CODE_HARIG = "201";
	private static final String EVENT_CODE_HEARA = "203";
	private static final String EVENT_CODE_SCAN = "301";
	private static final String ATTR_A_STATUS = "a_status";
	private static final String ATTR_DOC_KEY = "doc_key";
	private static final String ATTR_RELATION_NAME = "relation_name";
	private static final String ATTR_LEGACY_DOCUMENT_ID = "legacy_document_id";
	private static final String ATTR_EVENT_CATEGORY_CODE = "event_category_code";
	
		
	private static final String BNHP_DOC_EVENT = "bnhp_doc_event";
	private static final String DM_DOCUMENT = "dm_document"; 
	private static final String DM_RELATION = "dm_relation"; 
	private static final String BNHP_DOC_XML_FORM = "bnhp_doc_xml_form"; 
	private static final String BNHP_CUSTOMER_DOC = "bnhp_customer_doc"; 
	private static final String BNHP_YOMAN_ERUIM_TABLE = "bnhp_yoman_eruim_table";
	private static final String BNHP_TFS_PEULA_DOC = "bnhp_tfs_peula_doc"; 
	private static final String BNHP_TOFES_REPRINT_RELATION = "bnhp_tofes_reprint_relation"; 
	
	private static Map<String, TableObject> eventAttributesMap  = initializeEventAttrsMapping();
	private static Map<String, TableObject> harigDocAttributesMap  = initializeHarigDocAttrsOldMapping();
	
	private boolean isTransOpened = false;
	private String eventCode = "";
	//private static boolean syncNewToOldEnabled = false;
	
	//private static boolean isSyncUpdated = false;
	private boolean isSyncEnabled = true;
		
	@Override
	public boolean isSyncGloballyDisabled() throws DfException {
		String propVal = System.getProperty(GLOBAL_SYNC_CUSTOMER_DISABLED_PROP); 
		if ("true".equalsIgnoreCase(propVal)) {
			return true;
		}
		IDfClient client = DfClient.getLocalClient();
		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
				.newService(IMigrationConfigService.class.getName(),this.getSessionManager());
		return !migrationConfigService.isSyncNewToOldEnabled(this.getSession());
	}
	
	@Override
	protected void doSave(boolean arg0, String arg1, Object[] arg2)
			throws DfException {
		DfLogger.debug(this, "CustomerRelation:: entered doSave", null, null);
			
		boolean isNew = isNew();
		if (!isNew) {
			super.doSave(arg0, arg1, arg2);
			return;
		}
		
		/*if(!isSyncUpdated){
			IDfClient client = DfClient.getLocalClient();
			IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
					.newService(IMigrationConfigService.class.getName(),
							getSessionManager());
			syncNewToOldEnabled = migrationConfigService.isSyncNewToOldEnabled(getSession());
			
			isSyncUpdated = true;
			DfLogger.debug(this, "CustomerRelation:: syncNewToOldEnabled and syncOldToNewEnabled is updated from db", null, null);
		}*/
		
		IDfId childId = getChildId();
		
		IDfSession session = getSession();
		// get Category code from child
		IDfPersistentObject childObj = session.getObject(childId);
		
		String childTypeName = childObj.getType().getName();
		boolean isSyncDisabled = false;
		if (isSyncDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerRelation:: sync disabled locally", null,null);
		}
		if (this.isSyncGloballyDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerRelation:: sync disabled globally", null,null);
		}
		
		if(DM_DOCUMENT.equals(childTypeName) || BNHP_DOC_XML_FORM.equals(childTypeName)){
			//if((!this.isSyncDisabled()) && isSyncUpdated && syncNewToOldEnabled) {
			if(!isSyncDisabled) {
				doSyncSecondaryFile(this, session);
			}
			super.doSave(arg0, arg1, arg2);
			DfLogger.debug(this, "CustomerRelation:: type=" + childTypeName, null, null);
			return; 
		
		} else if(BNHP_DOC_EVENT.equals(childTypeName)){
			
			IDfId parentId = getParentId();
			eventCode = childObj.getString(ATTR_EVENT_CATEGORY_CODE);
			updateDmRelation(parentId, session);
			super.doSave(arg0, arg1, arg2);	
					
			if(!isSyncDisabled) {
				if(EVENT_CODE_SCAN.equals(eventCode) || EVENT_CODE_HARIG.equals(eventCode) || EVENT_CODE_HEARA.equals(eventCode)){
					doSync(childObj, parentId);
				}
			}
		} else {
			super.doSave(arg0, arg1, arg2);
			return; 
		}
		
	}

	private void doSyncSecondaryFile(CustomerRelation customerRelation, IDfSession session) throws DfException {
		DfLogger.debug(this, "CustomerRelation:: enter doSyncSecondaryFile", null, null);
		IDfCollection col = null;
		try{
			
			String parentId = customerRelation.getString(PARENT_ID);
			String str = "select a_status from " + BNHP_CUSTOMER_DOC  + " where r_object_id='" + parentId + "' and project_id in (1,2)";
			IDfQuery query = new DfQuery();
			query.setDQL(str);
			col = query.execute(session, 0);
			String oldObjId = null;
			if(!col.next()){
				DfLogger.debug(this, "CustomerRelation:: not found customer_doc with r_object_id=" + parentId , null, null);
				return;
			}
			oldObjId = col.getString(ATTR_A_STATUS);
			if (null==oldObjId || oldObjId.length()!=16) {
				DfLogger.debug(this, "CustomerRelation:: customer_doc with r_object_id=" + parentId +" has no a_status set", null, null);
				return;
			}
			IDfPersistentObject newRelation  = session.newObject(DM_RELATION);
			newRelation.setString(PARENT_ID, oldObjId);
			newRelation.setString(CHILD_ID, customerRelation.getString(CHILD_ID));
			newRelation.setString(RELATION_NAME, BNHP_TOFES_REPRINT_RELATION);
			newRelation.save();
		
			return;
		} finally{
			if(col!=null){
				col.close();
			}
			DfLogger.debug(this, "CustomerRelation:: exit doSyncSecondaryFile", null, null);
			
		}
	}

	private void updateDmRelation(IDfId parentId, IDfSession session2) throws DfException {
		StringBuffer buf = new StringBuffer();
		
		IDfQuery query = new DfQuery();
		buf.append("UPDATE dm_relation OBJECT SET order_no=0 where r_object_id in (" + "select rel.r_object_id from dm_relation rel, bnhp_doc_event ev where parent_id ='" + parentId.getId()
				+ "' and relation_name='" + getRelationName() + "' and ev.event_category_code = '" + eventCode
				+"' and ev.r_object_id=rel.child_id and order_no <>'0'" + ")");
		
		query.setDQL(buf.toString() );
		
		IDfCollection col = null;
		
		try {

			query.setDQL(buf.toString());
			
			IDfSession session = getSession();
			col = query.execute(session, IDfQuery.DF_EXEC_QUERY);

			this.setString("order_no", "1");
		}
		
		finally{
			if(col!=null){
				col.close();
			}
		}
	}

	protected void disableObjSync(IDfPersistentObject copy) {
		if (copy instanceof ISyncObject) {
			((ISyncObject) copy).disableSync();
		}
	}
	
	protected void enableObjSync(IDfPersistentObject copy) {
		if (copy instanceof ISyncObject) {
			((ISyncObject) copy).enableSync();
		}
	}
	
	
	private void doSync(IDfPersistentObject eventObj, IDfId docId) throws DfException {
		
		DfLogger.debug(this, "CustomerRelation:: entered doSync", null, null);
		
		IDfSessionManager mgr = getSessionManager();
		
		
		try{
			IDfDocument docObj = null; 
			try {
				docObj = (IDfDocument) getSession().getObject(docId);
			} catch (DfException ex) {
				DfLogger.warn(this, "CustomerRelation:: failed to get parent doc "+docId.getId(), null, null);
				return;
			}
			if (!needToSync(docObj)) {
				return;
			}
			
			String legacyID = docObj.getString(ATTR_LEGACY_DOCUMENT_ID);
			if (null == legacyID || legacyID.equals("") || legacyID.equals("nullString")) {
				DfLogger.warn(this, "CustomerRelation:: won't synchronize  parent doc "+docId.getId()+" with empty legacyId", null, null);
				return;
			}
			IDfDocument oldSystemDoc = findOldSystemDoc(legacyID, getSession());
			if (null == oldSystemDoc) {
				DfLogger.info(this, "CustomerRelation:: calling save on parent document, which should make parent tofes peula" + legacyID, null, null);
				((ISyncObject)docObj).setDirty(true);
				try {
					this.disableSync();
					docObj.save();
				} finally {
					this.enableSync();
				}
				DfLogger.debug(this, "CustomerRelation:: save on parent document finished", null, null);
				oldSystemDoc = findOldSystemDoc(legacyID, getSession());
				if (null == oldSystemDoc) {
					DfLogger.error(this, "CustomerRelation:: cannot find created tfs_peula with doc_key "+legacyID, null, null);
					throw new DfException("Failed to synchronize parent document for event: null document returned");
				}
			}
			if(eventCode.equals(EVENT_CODE_SCAN)){
					//copy scan details to doc in old system
					try {
						disableObjSync(oldSystemDoc);
						oldSystemDoc.setInt("scan_status", docObj.getInt("scan_status_code"));
						oldSystemDoc.setTime("scan_date", eventObj.getTime("legacy_event_entry_dttm")); // from event
						oldSystemDoc.setString("mpr_zihuy_pakid_sorek", eventObj.getString("executing_emp_id_code"));  // from event
						oldSystemDoc.save();
						
					} finally {
						enableObjSync(oldSystemDoc);
					}
					DfLogger.debug(this, "CustomerRelation:: copy scan details to bnhp_tfs_peula_doc with doc_key=" + legacyID, null, null);
					
			} else {
					if((!mgr.isTransactionActive()) && (!getSession().isTransactionActive())){
						getSession().beginTrans();
						isTransOpened = true;
					}
					//create new old-system event
					IDfPersistentObject newEventOldSystem  = getSession().newObject(BNHP_YOMAN_ERUIM_TABLE);
					((ISyncObject) newEventOldSystem).disableSync();
					eventObj.setString(ATTR_A_STATUS,newEventOldSystem.getObjectId().getId());
					eventObj.save();
					DfLogger.debug(this, "CustomerRelation:: create event in bnhp_yoman_eruim_table with id" + eventObj.getObjectId().getId(), null, null);

					copyEventAttributes((IDfDocument)eventObj, newEventOldSystem, getSession());
					newEventOldSystem.setString(ATTR_A_STATUS,eventObj.getObjectId().getId());

					String legacyDocId =  docObj.getString(ATTR_LEGACY_DOCUMENT_ID);
					newEventOldSystem.setString(ATTR_DOC_KEY, legacyDocId);

					if(eventCode.equals(EVENT_CODE_HARIG)){
						try {
							disableObjSync(oldSystemDoc);
							copyHarigAttributes((IDfDocument)eventObj, oldSystemDoc, getSession());
							oldSystemDoc.save();
						} finally {
							enableObjSync(oldSystemDoc);
						}
					}

					newEventOldSystem.save();
					((ISyncObject) newEventOldSystem).enableSync();
			}
		} catch (DfException e) {
			if (isTransOpened && getSession().isTransactionActive()) {
				getSession().abortTrans();
			} 
			throw e;
		} 	
		finally{
			if (isTransOpened && getSession().isTransactionActive()) {
				getSession().commitTrans();
			}
			DfLogger.debug(this, "CustomerRelation:: exit doSync", null, null);
			
		}
	}
	
	private boolean needToSync(IDfDocument docObj) throws DfException {
		if (null == docObj) {
			return false;
		}
		int projectId = docObj.getInt("project_id");
		return (1 == projectId) || (2 == projectId);
	}

	private IDfDocument findOldSystemDoc(String docId, IDfSession session) throws DfException {
		
		try{
		DfLogger.debug(this, "CustomerRelation:: enter findOldSystemDoc", null, null);
		String str = "select r_object_id from " + BNHP_TFS_PEULA_DOC  + " where doc_key='" + docId + "'";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		String objId = null;
		IDfCollection col = null;
		try {
			col = query.execute(session, 0);
			while (col.next()) {
				if (null == objId) {
					objId = col.getString("r_object_id");
				} else {
					throw new DfException("Relation Bof: duplicate tfs_peula object with doc_key " + docId );
				}
			}
			if (null == objId) {
				DfLogger.info(this,"CustomerRelation::findOldSystemDoc: not existing tfs_peula  document with doc_key " + docId, null, null);
				return null;
			} 
		} finally {
			if (null!=col) {
				try {col.close();} catch (DfException ex) {
					DfLogger.warn(this,"error closing collection: " + ex.getMessage(), null, null);
				}
			}
		}
		
		IDfDocument docObject = (IDfDocument) session.getObject(new DfId(objId));
		return docObject;
		
		} finally{
			DfLogger.debug(this, "CustomerRelation:: exit findOldSystemDoc", null, null);
		}
	}

	private void copyEventAttributes(IDfDocument eventObj, IDfPersistentObject newEvent,
			IDfSession session) throws DfException {
		String key = null;
		TableObject tableObject = null;
		try {
			DfLogger.debug(this,
					"CustomerRelation:: enter copyEventAttributes", null, null);

			for (Map.Entry<String, TableObject> entry : eventAttributesMap
					.entrySet()) {
				key = entry.getKey();
				if ("auto_event_ind".equals(key)) {
					newEvent.setInt("create_type_erua", eventObj
							.getBoolean(key) == true ? 1 : 2);

				} else {
					tableObject = eventAttributesMap.get(key);
					tableObject.getTransitionObject().execute(key,
							tableObject.getName(), newEvent, eventObj);
				}
			}
		} finally {
			DfLogger.debug(this, "CustomerRelation:: exit copyEventAttributes",
					null, null);

		}
	}
	
	private void copyHarigAttributes(IDfDocument eventObj, IDfPersistentObject newEvent,
			IDfSession session) throws DfException {
		String key = null;
		TableObject tableObject = null;
		try {
			DfLogger.debug(this,
					"CustomerRelation:: enter copyHarigAttributes", null, null);
			for (Map.Entry<String, TableObject> entry : harigDocAttributesMap
					.entrySet()) {
				key = entry.getKey();
				tableObject = harigDocAttributesMap.get(key);
				if ("auto_event_ind".equals(key)) {
					newEvent.setInt(tableObject.getName(), eventObj
							.getBoolean(key) == true ? 1 : 2);
				} else {
					tableObject.getTransitionObject().execute(key,
							tableObject.getName(), newEvent, eventObj);
				}
			}
		} finally {
			DfLogger.debug(this, "CustomerRelation:: exit copyHarigAttributes",
					null, null);

		}
	}

	private static Map<String, TableObject> initializeEventAttrsMapping() {
		Map<String, TableObject> map  = new HashMap<String, TableObject>();
		//map.put("dctm_document_id", new TableObject("doc_key",IDfType.DF_STRING, false ));
		map.put("auto_event_ind", new TableObject("create_type_erua",IDfType.DF_INTEGER, false ));
		map.put("event_category_code", new TableObject("kod_sug_erua",IDfType.DF_INTEGER, false ));
		map.put("event_category_type_code", new TableObject("ofen_tipul_erua",IDfType.DF_INTEGER, false ));
		map.put("legacy_event_entry_dttm", new TableObject("taarich_erua",IDfType.DF_TIME, false ));
		map.put("concatenated_event_id", new TableObject("kod_tavla_mekasheret",IDfType.DF_INTEGER, false ));
		map.put("executing_emp_id_code", new TableObject("mpr_zihuy_pakid_mevatzea",IDfType.DF_STRING, false ));
		map.put("terminal_channel_id", new TableObject("mispar_tachana",IDfType.DF_INTEGER, false ));
		map.put("bankol_id", new TableObject("mispar_bankol",IDfType.DF_INTEGER, false ));
		map.put("executing_emp_full_name", new TableObject("shem_pakid_mevatzea",IDfType.DF_STRING, false ));
		map.put("event_desc_text", new TableObject("heara",IDfType.DF_STRING, false ));
		//map.put("legacy_document_id", new TableObject("doc_key",IDfType.DF_STRING, false ));
		return map;
	}
	
	private static Map<String, TableObject> initializeHarigDocAttrsOldMapping() {
		Map<String, TableObject> map  = new HashMap<String, TableObject>();
		map.put("auto_event_ind", new TableObject("ymn_create_type_erua",IDfType.DF_INTEGER, false ));
		map.put("event_category_code", new TableObject("ymn_kod_sug_erua",IDfType.DF_INTEGER, false ));
		map.put("legacy_event_entry_dttm", new TableObject("ymn_taarich_erua",IDfType.DF_TIME, false ));
		map.put("executing_emp_id_code", new TableObject("ymn_mpr_zihuy_pakid",IDfType.DF_STRING, false ));
		map.put("event_category_type_code", new TableObject("ymn_ofen_tipul_erua",IDfType.DF_INTEGER, false ));
		map.put("executing_emp_id_code", new TableObject("ymn_mpr_zihuy_pakid",IDfType.DF_STRING, false ));
		map.put("terminal_channel_id", new TableObject("ymn_mispar_tachana",IDfType.DF_INTEGER, false ));
		map.put("bankol_id", new TableObject("ymn_mispar_bankol",IDfType.DF_INTEGER, false ));
		map.put("executing_emp_full_name", new TableObject("ymn_shem_pakid",IDfType.DF_STRING, false ));
		map.put("event_desc_text", new TableObject("ymn_heara",IDfType.DF_STRING, false ));
		return map;
	}
	
	@Override
	public void disableSync() {
		isSyncEnabled = false;
	}

	@Override
	public IDfId doSyncRecursive() throws DfException {
		// TODO Auto-generated method stub
		return null;
	}

	@Override
	public void enableSync() {
		isSyncEnabled = true;
	}

	@Override
	public boolean isSyncDisabled() {
		return !isSyncEnabled;
	}
	
	

	@Override
	public boolean hasBeenSynced() throws DfException {
		// TODO Auto-generated method stub
		return false;
	} 
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\tbo\ICustomerRelation.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import com.documentum.fc.client.IDfRelation;

public interface ICustomerRelation extends IDfRelation{

	public static final String GLOBAL_SYNC_CUSTOMER_DISABLED_PROP = "SYNC_DISABLED_CUSTOMER_DOC_TO_TFS_PEULA";
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\tbo\ISyncObject.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

public interface ISyncObject {
	public boolean isSyncDisabled() ;
	public void disableSync() ;
	public void enableSync() ;
	public boolean isSyncGloballyDisabled() throws DfException;
	public boolean hasBeenSynced() throws DfException;
	public IDfId doSyncRecursive() throws DfException ;
	public void setDirty(boolean value) throws DfException;
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\tbo\TableObject.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.IDfType;
import com.documentum.fc.common.DfException;

import bnhp.infra.base.utils.ITransition;
import bnhp.infra.base.utils.TransitionBoolean;
import bnhp.infra.base.utils.TransitionDouble;
import bnhp.infra.base.utils.TransitionInt;
import bnhp.infra.base.utils.TransitionString;
import bnhp.infra.base.utils.TransitionTime;

public class TableObject{
	private String name;
	private int type;
	private boolean isRepeating;
	private ITransition transitionObject;
		
	public TableObject(String name, int type, boolean isRepeating) {
		this.isRepeating = isRepeating;
		this.name = name;
		this.type = type;
		
		if(type == IDfType.DF_BOOLEAN){
			transitionObject = new TransitionBoolean(isRepeating);
		}else if(type == IDfType.DF_INTEGER){
			transitionObject = new TransitionInt(isRepeating);
		}else if(type == IDfType.DF_DOUBLE){
			transitionObject = new TransitionDouble(isRepeating);
		}else if(type == IDfType.DF_TIME){
			transitionObject = new TransitionTime();
		}else {
			transitionObject = new TransitionString(isRepeating);
		}
	}
	
	public ITransition getTransitionObject() {
		return transitionObject;
	}

	public String getName() {
		return name;
	}
	public void setName(String name) {
		this.name = name;
	}
	public int getType() {
		return type;
	}
	public void setType(int type) {
		this.type = type;
	}
	public void setRepeating(boolean isRepeating) {
		this.isRepeating = isRepeating;
	}
	public boolean isRepeating() {
		return isRepeating;
	}
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\ITransition.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public interface ITransition {

	public void execute(String oldName, String newName, IDfPersistentObject newDoc,
			IDfDocument document) throws DfException;

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\TransitionBoolean.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.common.DfException;

public class TransitionBoolean implements ITransition{

	private boolean isRepeating = false;
	
	public TransitionBoolean(boolean isRepeating) {
		this.isRepeating = isRepeating;
	}

	public TransitionBoolean() {	}
	
	@Override
	public void execute(String oldName, String newName, IDfPersistentObject newDoc, IDfDocument document) throws DfException {
		if(isRepeating){
    		newDoc.setBoolean(newName, document.getRepeatingBoolean(oldName,0));
    	
    	}else{
       		newDoc.setBoolean(newName, document.getBoolean(oldName));
    	}
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\TransitionDouble.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public class TransitionDouble implements ITransition{

	private boolean isRepeating = false;
	
	public TransitionDouble(boolean isRepeating) {
		
		this.isRepeating = isRepeating;
	}
	
	public TransitionDouble() {	}
	
	@Override
	public void execute(String oldName, String newName, IDfPersistentObject newDoc,
			IDfDocument document) throws DfException {
		if(isRepeating){
    		newDoc.setDouble(newName, document.getRepeatingDouble(oldName,0));
    	
    	}else{
       		newDoc.setDouble(newName, document.getDouble(oldName));
    	}
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\TransitionInt.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public class TransitionInt implements ITransition{

	private boolean isRepeating = false;
	
	public TransitionInt(boolean isRepeating) {
		
		this.isRepeating = isRepeating;
	}
	
	public TransitionInt() {	}
	@Override
	public void execute(String oldName, String newName,IDfPersistentObject newDoc,
			IDfDocument document) throws DfException {
		if("special_handling_code".equals(oldName)){
	    	newDoc.setInt("kod_cheshbon_mutzpan", document.getBoolean(oldName) == false ? 0 : 1);
	    	return;
	    } else if("concatenated_event_id".equals(oldName)){
	    	String val = document.getString(oldName);
	    	if (null == val || val.length()==0 || "nullString".equals(val)) {
	    		newDoc.setInt("kod_tavla_mekasheret", Integer.valueOf(0));
	    	} else {
	    		try{
	    			newDoc.setInt("kod_tavla_mekasheret",Integer.parseInt(val));
	    		} catch(Exception e){
	    			throw new DfException("Event bof - concatenated_event_id must be a number, but got '"+val+"'");
	    		}
	    	}
	    	return;
	    }
		
		if(isRepeating){
    		newDoc.setInt(newName, document.getRepeatingInt(oldName,0));
    	
    	}else{
    		newDoc.setInt(newName, document.getInt(oldName));
    	}
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\TransitionString.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public class TransitionString implements ITransition{

	private boolean isRepeating = false;
	
	public TransitionString(boolean isRepeating) {
		
		this.isRepeating = isRepeating;
	}
	
	public TransitionString() {	}
	
	@Override
	public void execute(String oldName, String newName, 
			IDfPersistentObject newDoc, IDfDocument document) throws DfException {
		String value; 
		if(isRepeating){
			value = document.getRepeatingString(oldName,0);
    	}else{
			value = document.getString(oldName);
       		
    	}
		if ("nullString".equals(value)) {
			value = "";
		}
		newDoc.setString(newName, value);
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\bnhp\infra\base\utils\TransitionTime.java
-----------------------------------------------------
package bnhp.infra.base.utils;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public class TransitionTime implements ITransition{

	@Override
	public void execute(String oldName, String newName, 
			IDfPersistentObject newDoc, IDfDocument document) throws DfException {
			
		newDoc.setTime(newName, document.getTime(oldName));
    	
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\com\documentum\fc\client\customerdoc\tbo\CustomerDoc.java
-----------------------------------------------------
package com.documentum.fc.client.customerdoc.tbo;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import bnhp.infra.base.sbo.IMigrationConfigService;
import bnhp.infra.base.tbo.ISyncObject;
import bnhp.infra.base.tbo.TableObject;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfDocument;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.IDfType;
import com.documentum.fc.client.IDfVersionLabels;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.DfList;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.IDfId;
import com.documentum.fc.common.IDfList;

public class CustomerDoc extends DfDocument implements ICustomerDoc, ISyncObject  {
	

	private static final String PROJECT_ID = "project_id";
	private static final String LEGACY_DOCUMENT_ID = "legacy_document_id";
	
	private static final String BNHP_DOC_EVENT = "bnhp_doc_event";
	private static final String BNHP_TFS_PEULA_DOC = "bnhp_tfs_peula_doc";
		
	//private static boolean syncNewToOldEnabled = false;
	//private static boolean syncOldToNewEnabled = false;
	//private static boolean isSyncUpdated = false;
	private boolean isSyncEnabled = true;
	private static volatile String folderId = null;
	
	
	private boolean isTransOpened = false;
	
	private static Map<String, TableObject> tfsPeulaAttributesMap  = initializeTfsPeulaMapping();
	private static Map<String, TableObject> customerDocAspectAttributesMap  = initializeCustomerDocAspectMapping();
	
	
	@Override
	public boolean isSyncGloballyDisabled() throws DfException {
		String propVal = System.getProperty(GLOBAL_SYNC_CUSTOMER_DISABLED_PROP); 
		if ("true".equalsIgnoreCase(propVal)) {
			return true;
		}
		IDfClient client = DfClient.getLocalClient();
		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
				.newService(IMigrationConfigService.class.getName(),this.getSessionManager());
		// ugly side effect
		String aFolderId = migrationConfigService.getFolderID(getSession());
		if (null==folderId || (!folderId.equals(aFolderId))) {
			folderId = aFolderId;
		}
		return !migrationConfigService.isSyncNewToOldEnabled(this.getSession());
	}
	
	@Override
	protected void doSave(boolean arg0, String arg1, Object[] arg2)
			throws DfException {
		
		try {
			DfLogger.debug(this, "CustomerDoc:: entered doSave"
					+ this.getString("legacy_document_id"), null, null);
			
			boolean isSyncDisabled = false;
			if (isSyncDisabled()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled locally", null,null);
			}
			if (this.isSyncGloballyDisabled()) {
				isSyncDisabled = true;
				DfLogger.debug(this,  "CustomerDoc:: sync disabled globally", null,null);
			}
			if (!isDirty()){
				isSyncDisabled = true;
				DfLogger.debug(this,  "CustomerDoc:: sync disabled since is not dirty", null,null);
			}
			if (!isProject()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled because of project_id", null,null);
			}
			if (!isLegacy()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled because it lacks legacy id", null,null);
			}
			
			if (isSyncDisabled) {
				DfLogger.debug(this, "CustomerDoc:: sync isn't enabled or isn't needed", null, null);
				super.doSave(arg0, arg1, arg2);
				return;
			} else {
				doSync(this, arg0, arg1, arg2);
			}
		} finally {
			DfLogger.debug(this, "CustomerDoc:: exit doSave", null, null);
		}
	}
	
	/*private void updateSync() throws DfException {
		try {
			DfLogger.debug(this, "CustomerDoc:: entered updateSync"
					+ this.getString("legacy_document_id"), null, null);

			IDfClient client = DfClient.getLocalClient();
			IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
					.newService(IMigrationConfigService.class.getName(),getSessionManager());
			syncNewToOldEnabled = migrationConfigService.isSyncNewToOldEnabled(getSession());
			//syncOldToNewEnabled = migrationConfigService.isSyncOldToNewEnabled(getSession());
			folderId = migrationConfigService.getFolderID(getSession());
			if (folderId == null || !validateFolderID(folderId, getSession())) {
				throw new DfException(
						"Code:611 Business constraint violated: Replication Bof "
								+ "Folder id " + folderId
								+ " to sync in tofes peula doesn't exist");
			}
			isSyncUpdated = true;
		} finally {

			DfLogger.debug(	this, "CustomerDoc:: exit updateSync", null, null);
		}
		
	}*/

	@Override
	protected synchronized IDfId doCheckin(boolean arg0, String arg1,
			String arg2, String arg3, String arg4, String arg5, Object[] arg6)
			throws DfException {
		try {
			DfLogger.debug(this, "CustomerDoc:: entered doCheckin", null, null);

			IDfId checkinId = super.doCheckin(arg0, arg1, arg2, arg3, arg4,
					arg5, arg6);

			ICustomerDoc obj = (ICustomerDoc) getSession().getObject(checkinId);

			boolean isSyncDisabled = false;
			if (isSyncDisabled()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled locally", null,null);
			}
			if (this.isSyncGloballyDisabled()) {
				isSyncDisabled = true;
				DfLogger.debug(this,  "CustomerDoc:: sync disabled globally", null,null);
			}
			
			if (!isProject()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled because of project_id", null,null);
			}
			if (!isLegacy()) {
				isSyncDisabled = true;
				DfLogger.debug(this, "CustomerDoc:: sync disabled because it lacks legacy id", null,null);
			}
			

			//if (isSyncEnabled && syncNewToOldEnabled && syncOldToNewEnabled
			//		&& isProject() && isLegacy() && !isSyncGloballyDisabled()) {
			if (!isSyncDisabled) {
				DfLogger.debug(this,
						"CustomerDoc:: sync is enabled from doCheckin", null,
						null);
				obj.setDirty(true);
				obj.save();
			} 
			return checkinId;
		} finally {
			DfLogger.debug(this, "CustomerDoc:: exit doCheckin", null, null);
		}
	}

	@Override
	protected IDfId doCheckout(String arg0, String arg1, String arg2,
			Object[] arg3) throws DfException {
		try {
			DfLogger
					.debug(this, "CustomerDoc:: entered doCheckout", null, null);

			IDfId id = super.doCheckout(arg0, arg1, arg2, arg3);

			ICustomerDoc obj = (ICustomerDoc) getSession().getObject(id);
			obj.setString("a_status", "");

			return id;
		} finally {
			DfLogger.debug(this, "CustomerDoc:: exit doCheckin", null, null);
		}
	}

	private boolean validateFolderID(String folderId2, IDfSession session) throws DfException {
		String str = "select r_object_id from dm_folder where r_object_id='" + folderId2 + "'";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = query.execute(session, 0);
		
		if(col != null && col.next()){
			return true;
		
		}else{
			return false;
		}
		
	}

	public void doSync(CustomerDoc customerDoc, boolean arg0, String arg1, Object[] arg2) throws DfException {
		
		DfLogger.debug(this, "CustomerDoc:: entered doSync. legacy_document_id=" + customerDoc.getString("legacy_document_id"), null, null);
		
		
		IDfSessionManager mgr = getSessionManager();
		if((!mgr.isTransactionActive()) && (! this.getSession().isTransactionActive())){
			getSession().beginTrans();
			isTransOpened = true;
		}
		
		
		try{
			
			//find doc in old system
			StringBuffer buf = new StringBuffer("");
			//buf.append("bnhp_tfs_peula_doc where doc_key='").append(customerDoc.getString("legacy_document_id")).append("'");
			buf.append("bnhp_tfs_peula_doc (all) where a_status='").append(customerDoc.getObjectId().getId()).append("'");
				
			IDfSysObject docPeula = (IDfSysObject)getSession().getObjectByQualification(buf.toString());
				
			
			if (null == docPeula) {
				if (this.getObjectId().equals(this.getChronicleId())) {
					DfLogger.debug(this, "CustomerDoc:: doSync. peula_doc does not exist for doc_key " + customerDoc.getString("legacy_document_id"), null, null);
					DfLogger.debug(this, "CustomerDoc:: doSync. root of tree, creating new copy object", null, null);
					docPeula  = (IDfSysObject)getSession().newObject(BNHP_TFS_PEULA_DOC);
					DfLogger.debug(this, "CustomerDoc:: doSync. New tfs_peula_doc created with doc_key " + customerDoc.getString("legacy_document_id")+" id="+docPeula.getObjectId().getId(), null, null);
					customerDoc.setString("a_status",docPeula.getObjectId().getId());
					// existing document may be updated intrnally, we need it saved to
					// be sure that it's consistent, p.e. that happens in DQL UPDATE
					DfLogger.debug(this, "CustomerDoc:: doSync. doing super.doSave()", null, null);
					super.doSave(arg0, arg1, arg2);
					DfLogger.debug(this, "CustomerDoc:: doSync. super.doSave() done", null, null);
					copyData(docPeula);
					docPeula.setString("a_status", customerDoc.getObjectId().getId());
					docPeula.link(folderId);
					((ISyncObject)docPeula).disableSync();
					docPeula.save();	
				} else {
					 // have to do full sync
					DfLogger.debug(this, "CustomerDoc:: doSync. peula_doc does not exist for doc_key " + customerDoc.getString("legacy_document_id") + " doing full sync", null, null);
					DfLogger.debug(this, "will do recursive sync()", null, null);
					DfLogger.debug(this, "CustomerDoc:: doSync. doing super.doSave() before sync recursive", null, null);
					super.doSave(arg0, arg1, arg2);
					DfLogger.debug(this, "CustomerDoc:: doSync. super.doSave() done", null, null);
					this.doSyncRecursive();
					DfLogger.debug(this, "CustomerDoc:: doSync. doing super.doSave() after sync recursive", null, null);
					super.doSave(arg0, arg1, arg2);
					DfLogger.debug(this, "CustomerDoc:: doSync. super.doSave() done", null, null);
				}
			}  else {
				IDfId statusObj = getStatus(docPeula);
				
				// the obj in old system already synced. have to just copy data
				
				DfLogger.debug(this, "CustomerDoc:: doSync. peula_doc exists for doc_key " + customerDoc.getString("legacy_document_id") + " just copy data", null, null);
				DfLogger.debug(this, "CustomerDoc:: doSync. doing super.doSave()", null, null);
				// existing document may be updated intrnally, we need it saved to
				// be sure that it's consistent, p.e. that happens in DQL UPDATE
				super.doSave(arg0, arg1, arg2);
				DfLogger.debug(this, "CustomerDoc:: doSync. super.doSave() done", null, null);
				DfLogger.debug(this, "CustomerDoc:: doSync. copyData", null, null);
				copyData(docPeula);
				DfLogger.debug(this, "CustomerDoc:: doSync. copyData done", null, null);
				((ISyncObject)docPeula).disableSync();
				DfLogger.debug(this, "CustomerDoc:: doSync. saving copy object", null, null);
				docPeula.save();
				DfLogger.debug(this, "CustomerDoc:: doSync. copy object saved", null, null);
			}		
			
		} catch (DfException e) {
			if (isTransOpened && getSession().isTransactionActive()) {
				getSession().abortTrans();
			}
			throw e;
		} finally{
			if (isTransOpened && getSession().isTransactionActive()) {
				getSession().commitTrans();
			}
			DfLogger.debug(this, "CustomerDoc:: exit doSync. legacy_document_id=" + customerDoc.getString("legacy_document_id"), null, null);
		}
	}

	public IDfId doSyncRecursive() throws DfException {
		try{
		DfLogger.debug(this, "CustomerDoc:: entered doSyncRecursive. legacy_document_id=" + this.getString("legacy_document_id"), null, null);
		// ugly!
		if (null == folderId) {
			this.isSyncGloballyDisabled();
		}
		IDfId  copyId = getStatus(this);
		if (null!= copyId) {
			//tfspeuladoc
			IDfSysObject copyObj = (IDfSysObject) this.getSession().getObject(copyId);
			copyData(copyObj);
			setVersionLabel(copyObj);
			((ISyncObject)copyObj).disableSync();
			copyObj.save();
			DfLogger.debug(this, "CustomerDoc:: doSyncRecursive. Copy data for tfs_peula_doc with doc_key " + this.getString("legacy_document_id"), null, null);
			
		} else {
			if (this.getObjectId().equals(getChronicleId())) {
				// we're root of the chain
				IDfSysObject obj = (IDfSysObject) this.getSession().newObject(BNHP_TFS_PEULA_DOC);
				obj.link(folderId);
				copyData(obj);
				obj.setString("a_status", this.getObjectId().getId());
				setVersionLabel(obj);
				((ISyncObject)obj).disableSync();
				obj.save();
				copyId = obj.getObjectId();
				DfLogger.debug(this, "CustomerDoc:: doSyncRecursive. New tfs_peula_doc created with doc_key " + this.getString("legacy_document_id"), null, null);
			
			} else  {
				ICustomerDoc parent = (ICustomerDoc)this.getSession().getObject(this.getAntecedentId());
				boolean isImmutable= parent.isImmutable();
				if (isImmutable) {
					parent.setBoolean("r_immutable_flag", false);
				}
				IDfId parentCopyId;
				try {
					parent.setSync(false);
					parentCopyId = parent.doSyncRecursive();
					parent.save();
					if (isImmutable) {
						parent.setBoolean("r_immutable_flag", true);
					}
					parent.save();
				} finally {
					parent.setSync(true);
				}
				IDfSysObject parentCopy = (IDfSysObject) this.getSession().getObject(parentCopyId);
				parentCopy.checkout();
				copyData(parentCopy);
				parentCopy.setInt("girsa", parentCopy.getInt("girsa")+1);
				parentCopy.setString("a_status", this.getObjectId().getId());
				String versionCopy = makeCopyVersionLabels(this.getVersionLabels());
				try {
					((ISyncObject)parentCopy).disableSync();
					if(versionCopy == null){
						copyId = parentCopy.checkin(false, "CURRENT");
						
					} else{
						copyId = parentCopy.checkin(false, "CURRENT," + makeCopyVersionLabels(this.getVersionLabels()));
					}
				} finally {
					((ISyncObject)parentCopy).enableSync();
				}
			}
			this.setString("a_status", copyId.getId());
		}
		
		return copyId;
		
		} finally{
			
			DfLogger.debug(this, "CustomerDoc:: exit doSyncRecursive. legacy_document_id= " + this.getString("legacy_document_id"), null, null);	
		}
	}
	
	private void setVersionLabel(IDfSysObject obj) throws DfException {
		int availableIndex = obj.getValueCount("r_version_label");
		 
		String versions =  makeCopyVersionLabels(this.getVersionLabels());
		List<String> versionList = getVersions(obj);
		if(versions != null){
			String[] arr = versions.split(",");
			int j = availableIndex;
			for(int i = 0; i < arr.length; i++){
				if(!versionList.contains(arr[i])){
					obj.setRepeatingString("r_version_label",j++,arr[i]);
				}
			}
		}
	}
	
	private List<String> getVersions(IDfSysObject obj) throws DfException{
		
		List<String> list = new ArrayList<String>();
		
		String implicit=obj.getImplicitVersionLabel();
		IDfVersionLabels objVersionLabels = obj.getVersionLabels();
		for (int i = 0; i <  objVersionLabels.getVersionLabelCount(); i++) {
			String label = objVersionLabels.getVersionLabel(i);
			if (label.equals(implicit) || "CURRENT".equals(label)) {
				continue;
			}
			else{
				list.add(label);
			}
		}
		
		return list;
	}

	protected String makeCopyVersionLabels(IDfVersionLabels versionLabels) throws DfException {
		StringBuilder result = new StringBuilder();
		if (versionLabels.hasSymbolicVersionLabel()) {
			String implicit=versionLabels.getImplicitVersionLabel();
			for (int i = 0; i < versionLabels.getVersionLabelCount(); i++) {
				String label = versionLabels.getVersionLabel(i);
				if (label.equals(implicit) || "CURRENT".equals(label)) {
					continue;
				}
				if (result.length()>0) {
					result.append(" ");
				}
				result.append(label);
			}
		}
		return result.length() > 0 ? result.toString():null;
	}
	
	private IDfId getStatus(IDfSysObject doc) throws DfException {
		IDfId result = null;
		if(doc == null){
			return null;
		}
		String idStr = doc.getString("a_status");
		if (null!=idStr && idStr.length()==16) {
			result = new DfId(idStr);
		} 
		return result;
	}
	
	private void copyData(IDfSysObject doc) throws DfException {
		copyAttributes(doc);
		copyCustomString(doc);
		copyAspectAttrs(doc);
		if (this.getContentSize() > 0 /*|| hasContentButNotUpdated()*/) {
			DfLogger.debug(this, "binding content", null,null);
			//ensureContentSaved();
			if (doc.getContentSize()>0 && !doc.getContentType().equals(this.getContentType())) {
				doc.setContentType(this.getContentType());
			}
			doc.bindFile(0, this.getObjectId(), 0);
		} else {
			DfLogger.debug(this, "no content to bind", null,null);
		}
		
		
	}

	/* no longer needed, super.doSave() is always called before copyData() 
	private boolean hasContentButNotUpdated() throws DfException {
		DfLogger.debug(this, "hasContentButNotUpdated(): checking if has content, but instance is not up-to-date", null, null);
		IDfCollection col = null;
		boolean result = false;
		try {
			IDfList args = new DfList();
			args.append("PAGE");
			IDfList types = new DfList();
			types.append("I");
			IDfList values = new DfList();
			values.append("0");
			col = this.getSession().apply(this.getObjectId().getId(),
					"ORIGINAL_CONTENT",
					args, types, values);
			while (col.next()) {
				IDfId id = col.getId("result");
				DfLogger.debug(this, "content id is: "+id.getId(), null, null);
				if (!id.isNull()) {
					result=true;
				}
			}
			DfLogger.debug(this, "hasContentButNotUpdated() result: "+result, null, null);
		} finally {
			if (null!=col) {
				try {
					col.close();
				} catch(DfException dfex) {
					DfLogger.error(this, "failed to close collection: "+dfex.getMessage(), null, dfex);
				}
			}
		}
		return result;
	}

	protected void ensureContentSaved() throws DfException {
		DfLogger.debug(this, "checking if need to fix pending changes in content", null, null);
		IDfCollection col = null;
		Boolean needSave = null;
		try {
			IDfList args = new DfList();
			args.append("PAGE");
			IDfList types = new DfList();
			types.append("I");
			IDfList values = new DfList();
			values.append("0");
			col = this.getSession().apply(this.getObjectId().getId(),
					"ORIGINAL_CONTENT",
					args, types, values);
			while (col.next()) {
				IDfId id = col.getId("result");
				if (id.isNull()) {
					needSave = true;
				} else {
					if (null==needSave) {
						needSave=false;
					}
				}
			}
			if (null==needSave || needSave) {
				DfLogger.debug(this, "calling super.doSave() to fix pending changes in content", null, null);
				super.doSave(false, null, null);
			} else {
				DfLogger.debug(this, "skipping super.doSave()", null, null);
			}
		} finally {
			if (null!=col) {
				try {
					col.close();
				} catch(DfException dfex) {
					DfLogger.error(this, "failed to close collection: "+dfex.getMessage(), null, dfex);
				}
			}
		}
		
	}*/
	
	private void copyCustomString(IDfSysObject doc) throws DfException {
		String customText = this.getString("custom_text");
		
		if(customText != null && customText.contains("kod_cheshbon_mushee")){
			String[] arr = customText.split(",");
			for(String str : arr){
				if(str != null && str.contains("kod_cheshbon_mushee")){
					String[] kodAr = str.split("=");
					if(kodAr.length == 2){
						int kod = new Integer(kodAr[1]);
						doc.setInt("kod_cheshbon_mushee", kod);
					}
				}
			}
		}
	}

	private void copyAttributes(IDfSysObject newDoc) throws DfException {
		try {
			DfLogger.debug(this,
					"CustomerDoc:: enter copyAttributes", null, null);

			String key = null;
			TableObject tableObject = null;

			for (Map.Entry<String, TableObject> entry : tfsPeulaAttributesMap
					.entrySet()) {
				key = entry.getKey();
				tableObject = tfsPeulaAttributesMap.get(key);
				tableObject.getTransitionObject().execute(key,
						tableObject.getName(), newDoc, this);
			}
		} finally {
			DfLogger.debug(this, "CustomerDoc:: exit copyAttributes",
					null, null);
		}
	}
	
	private void copyAspectAttrs(IDfSysObject newDoc) throws DfException {
		try {
			DfLogger.debug(this, "CustomerDoc:: enter copyAspectAttrs", null,
					null);

			String key = null;
			TableObject tableObject = null;

			IDfList aspectList = this.getAspects();
			for (int i = 0; i < aspectList.getCount(); i++) {
				if (((String) aspectList.get(i))
						.equalsIgnoreCase("bnhp_paper_doc")) {
					for (Map.Entry<String, TableObject> entry : customerDocAspectAttributesMap
							.entrySet()) {
						key = entry.getKey();
						tableObject = entry.getValue();
						tableObject.getTransitionObject().execute(
								"bnhp_paper_doc." + key, tableObject.getName(),
								newDoc, this);
					}
				}
			}
		} finally {
			DfLogger.debug(this, "CustomerDoc:: exit copyAspectAttrs", null,
					null);
		}
	}

	private boolean isProject() throws DfException {
		String projId = this.getString(PROJECT_ID);
		if("1".equals(projId) ||("2".equals(projId))){
			return true;
		} else{
			return false;
		}
	}

	private boolean isLegacy() throws DfException {
		String legacyid = this.getString(LEGACY_DOCUMENT_ID);
		if(null == legacyid || legacyid.length() == 0 || "nullString".equals(legacyid)){
			return false;
		} else{
			return true;
		}
	}

	@Override
	public void disableSync() {
		isSyncEnabled = false;
	}

	@Override
	public void enableSync() {
		isSyncEnabled = true;
	}

	@Override
	public boolean isSyncDisabled() {
		return !isSyncEnabled;
	}
	
	
	
	private static Map<String, TableObject> initializeCustomerDocAspectMapping() {
		Map<String, TableObject> map  = new HashMap<String, TableObject>();
		map.put("archive_date", new TableObject("sent_date",IDfType.DF_TIME, false ));
		map.put("bank_archive_id", new TableObject("original_box",IDfType.DF_INTEGER, false ));
		map.put("archive_box_nbr", new TableObject("box",IDfType.DF_INTEGER, false ));
		map.put("box_batch_nbr", new TableObject("portion",IDfType.DF_INTEGER, false ));
		map.put("box_doc_serial_nbr", new TableObject("portion_inner_num",IDfType.DF_INTEGER, false ));
		map.put("document_page_cnt", new TableObject("page_count",IDfType.DF_INTEGER, false ));
		map.put("doc_location_code", new TableObject("physical_location",IDfType.DF_INTEGER, false ));
		map.put("paper_destruction_dttm", new TableObject("expiration_date",IDfType.DF_TIME, false ));
		map.put("bar_code", new TableObject("barcode",IDfType.DF_STRING, false ));
		return map;
	}

	private static Map<String, TableObject> initializeTfsPeulaMapping() {
		Map<String, TableObject> map  = new HashMap<String, TableObject>();
		map.put("object_name",new TableObject("object_name",IDfType.DF_STRING, false ));
		map.put("customer_id",new TableObject("sifrur_lakoach", IDfType.DF_DOUBLE, true));
		map.put("customer_id_doc_type_code",new TableObject("sug_mismach_mezahe_lak",IDfType.DF_INTEGER, true ));
		map.put("complete_customer_id_code",new TableObject("mpr_zihuy_lakoach",IDfType.DF_STRING, true ));
		map.put("customer_serial_nbr",new TableObject("mpr_siduri_lakoach",IDfType.DF_INTEGER, true ));
		map.put("customer_full_name",new TableObject("shem_lakoach",IDfType.DF_STRING, true ));
		map.put("account_bank_id",new TableObject("mispar_bank",IDfType.DF_INTEGER, true ));
		map.put("branch_id",new TableObject("mispar_snif",IDfType.DF_INTEGER, true ));
		map.put("account_nbr",new TableObject("mispar_cheshbon",IDfType.DF_INTEGER, true ));
		map.put("special_handling_code", new TableObject("kod_cheshbon_mutzpan",IDfType.DF_INTEGER, true ));
		map.put("division_id",new TableObject("mispar_hativa",IDfType.DF_INTEGER, true ));
		map.put("pension_fund_nbr", new TableObject("mispar_kupat_gemel",IDfType.DF_INTEGER, false ));
		map.put("planholder_number", new TableObject("mispar_amit",IDfType.DF_INTEGER, false ));
		map.put("legacy_document_id", new TableObject("doc_key",IDfType.DF_STRING, false ));
		map.put("system_code", new TableObject("app_id",IDfType.DF_INTEGER, false ));
		map.put("business_area_code", new TableObject("doc_type",IDfType.DF_INTEGER, false ));
 		map.put("business_sub_area_code", new TableObject("doc_sub_type",IDfType.DF_INTEGER, false ));
		map.put("project_id", new TableObject("project_id",IDfType.DF_INTEGER, false ));
		map.put("ongoing_or_history_code", new TableObject("shotef_or_history",IDfType.DF_INTEGER, false ));
		map.put("document_form_id", new TableObject("mispar_tofes",IDfType.DF_STRING, false ));
		map.put("document_edition_nbr", new TableObject("mispar_mahadura",IDfType.DF_INTEGER, false ));
		map.put("template_data_exists_ind", new TableObject("reprint_exists",IDfType.DF_BOOLEAN, false ));
		map.put("legacy_document_entry_dttm", new TableObject("co_date",IDfType.DF_TIME, false ));
		map.put("document_group_id", new TableObject("process_id",IDfType.DF_STRING, true ));
		map.put("doc_completeness_code", new TableObject("incomplete",IDfType.DF_INTEGER, false ));
		map.put("channel_id", new TableObject("kod_arutz",IDfType.DF_INTEGER, false ));
		map.put("transaction_amt",new TableObject("schum_iska",IDfType.DF_DOUBLE, false ) );
		map.put("currency_code", new TableObject("sug_matbea",IDfType.DF_INTEGER, false ) );
		map.put("scan_status_code",new TableObject("scan_status",IDfType.DF_INTEGER, false ) );
		map.put("executing_bank_id", new TableObject("bank_mevatzea",IDfType.DF_INTEGER, false ));
		map.put("executing_branch_id", new TableObject("snif_mevatzea",IDfType.DF_INTEGER, false ));
		map.put("emp_id_document_type_code", new TableObject("sug_mismach_mezahe_pak",IDfType.DF_INTEGER, false ));
		map.put("executing_emp_id_code", new TableObject("mpr_zihuy_pakid_mevatzea",IDfType.DF_STRING, false ));
		map.put("ip_address", new TableObject("tachana_ip_addrs",IDfType.DF_STRING, false ));
		map.put("terminal_channel_id", new TableObject("mispar_tachana",IDfType.DF_INTEGER, false ));
		map.put("bankol_id",new TableObject("mispar_bankol",IDfType.DF_INTEGER, false ) );
		map.put("executing_emp_full_name", new TableObject("shem_pakid_mevatzea",IDfType.DF_STRING, false ));
		map.put("doc_delivery_num",new TableObject("release_num",IDfType.DF_INTEGER, false ) );
		map.put("instruction_rcv_type_code",new TableObject("kod_ofen_kabalat_horaa",IDfType.DF_INTEGER, false ) );
		map.put("signature_status_code",new TableObject("doc_status",IDfType.DF_INTEGER, false ) );
		return map;
	}

	@Override
	public void setDirty(boolean flag) throws DfException{
		super.setDirty(flag);
	}

	@Override
	public void setSync(boolean b) {
		isSyncEnabled = b;
	}

	/*@Override
	public IDfId getLinkToCopy() throws DfException {
		// TODO Auto-generated method stub
		return null;
	}*/

	@Override
	public boolean hasBeenSynced() throws DfException {
		// TODO Auto-generated method stub
		return false;
	}

	
	@Override
	public void doDestroy(boolean arg0, Object [] arg1) throws DfException {
		DfLogger.debug(this, "CustomerDoc::"+ ": entered doDestroy("+arg0+","+arg1+") on bnhp_tfs_peula_doc "+this.getObjectId().getId(), null,null);
		
		boolean openedTransaction = false;
		boolean success = false;
		boolean isSyncDisabled = false;
		if (isSyncDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled locally", null,null);
		}
		if (this.isSyncGloballyDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled globally", null,null);
		}
		int projectId = getInt("project_id");
		if ((1 != projectId) && (2!=projectId)) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled since project_id="+projectId, null,null);
		}
		if (isSyncDisabled) {
			DfLogger.debug(this, "CustomerDoc::"+ ": doing super.doDestroy() ", null,null);
			super.doDestroy(arg0, arg1);
			DfLogger.debug(this, "CustomerDoc::"+ ": super.doDestroy() finished, all done", null,null);
			return;
		}
		try {
			if (!(this.getSessionManager().isTransactionActive() ||
					this.getSession().isTransactionActive())) {
				openedTransaction = true;
				DfLogger.debug(this, "CustomerDoc::"+ ": opening transaction", null,null);
				this.getSession().beginTrans();
			}
			IDfId copyId = this.getStatus(this);
			if (null!= copyId) {
				DfLogger.debug(this, "CustomerDoc::"+": fetching copy object "+copyId.getId(), null,null);
				IDfSysObject copy = (IDfSysObject) this.getSession().getObject(copyId);
				DfLogger.debug(this, "CustomerDoc::"+": destroying copy object "+copyId.getId(), null,null);
				((ISyncObject)copy).disableSync();
				copy.destroy();
				DfLogger.debug(this, "CustomerDoc::"+": copy object "+copyId.getId()+" destroyed", null,null);
			}
			DfLogger.debug(this, "CustomerDoc::"+": destroying current object", null,null);			
			super.doDestroy(arg0, arg1);
			success = true;
		}  finally {
			if (success) {
				DfLogger.debug(this, "CustomerDoc::"+ ":  destroy syncronization succeeded", null,null);
			} else {
				DfLogger.debug(this, "CustomerDoc::"+ ": destroy syncronization failed, look for exceptions in log", null,null);
			}
			if (openedTransaction && this.getSession().isTransactionActive()) {
				if (success) {
					DfLogger.debug(this, "CustomerDoc::"+ ": commiting transaction", null,null);
					this.getSession().commitTrans();
					DfLogger.debug(this, "CustomerDoc::"+ ": transaction commited", null,null);
				} else {
					DfLogger.debug(this, "CustomerDoc::"+ ": aborting transaction transaction", null,null);
					this.getSession().abortTrans();
					DfLogger.debug(this, "CustomerDoc::"+ ": transaction aborted", null,null);
				}
			}
		}
	}
	
	@Override
	public void doDestroyAllVersions(Object [] arg0) throws DfException {
		DfLogger.debug(this, "CustomerDoc::"+ ": entered doDestroyAllVersions("+arg0+") on bnhp_tfs_peula_doc "+this.getObjectId().getId(), null,null);
		
		boolean openedTransaction = false;
		boolean success = false;
		boolean isSyncDisabled = false;
		if (isSyncDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled locally", null,null);
		}
		if (this.isSyncGloballyDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled globally", null,null);
		}
		int projectId = getInt("project_id");
		if ((1 != projectId) && (2!=projectId)) {
			isSyncDisabled = true;
			DfLogger.debug(this, "CustomerDoc::"+ ": sync disabled since project_id="+projectId, null,null);
		}
		if (isSyncDisabled) {
			DfLogger.debug(this, "CustomerDoc::"+ ": doing super.doDestroyAllVersions() ", null,null);
			super.doDestroyAllVersions(arg0);
			DfLogger.debug(this, "CustomerDoc::"+ ": super.doDestroy() finished, all done", null,null);
			return;
		}
		try {
			if (!(this.getSessionManager().isTransactionActive() ||
					this.getSession().isTransactionActive())) {
				openedTransaction = true;
				DfLogger.debug(this, "CustomerDoc::"+ ": opening transaction", null,null);
				this.getSession().beginTrans();
			}
			IDfId copyId = this.getStatus(this);
			if (null!= copyId) {
				DfLogger.debug(this, "CustomerDoc::"+": fetching copy object "+copyId.getId(), null,null);
				IDfSysObject copy = (IDfSysObject) this.getSession().getObject(copyId);
				DfLogger.debug(this, "CustomerDoc::"+": destroying copy object "+copyId.getId()+" versions", null,null);
				((ISyncObject)copy).disableSync();
				copy.destroyAllVersions();
				DfLogger.debug(this, "CustomerDoc::"+": copy object "+copyId.getId()+" all versions destroyed", null,null);
			}
			DfLogger.debug(this, "CustomerDoc::"+": destroying current object versions", null,null);			
			super.doDestroyAllVersions( arg0);
			success = true;
		}  finally {
			if (success) {
				DfLogger.debug(this, "CustomerDoc::"+ ":  destroyAllVersions syncronization succeeded", null,null);
			} else {
				DfLogger.debug(this, "CustomerDoc::"+ ": destroyAllVersions syncronization failed, look for exceptions in log", null,null);
			}
			if (openedTransaction && this.getSession().isTransactionActive()) {
				if (success) {
					DfLogger.debug(this, "CustomerDoc::"+ ": commiting transaction", null,null);
					this.getSession().commitTrans();
					DfLogger.debug(this, "CustomerDoc::"+ ": transaction commited", null,null);
				} else {
					DfLogger.debug(this, "CustomerDoc::"+ ": aborting transaction transaction", null,null);
					this.getSession().abortTrans();
					DfLogger.debug(this, "CustomerDoc::"+ ": transaction aborted", null,null);
				}
			}
		}
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\src\com\documentum\fc\client\customerdoc\tbo\ICustomerDoc.java
-----------------------------------------------------
package com.documentum.fc.client.customerdoc.tbo;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

public interface ICustomerDoc extends IDfDocument{

	public static final String GLOBAL_SYNC_CUSTOMER_DISABLED_PROP = "SYNC_DISABLED_CUSTOMER_DOC_TO_TFS_PEULA";

	IDfId doSyncRecursive() throws DfException;

	public void setDirty(boolean b) throws DfException;

	public void setSync(boolean b);
	
}




file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\antrun\build-main.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8" ?>
<project name="maven-antrun-" default="main"  >
<target name="main">
  <property name="project" value="BnhpInfraDFArtifacts"/>
  <property name="headlesscomposer" value="C:/Users/AP068/dctm/HeadlessComposer//7.1"/>
  <ant antfile="${maven.dependency.com.poalim.documentum.build-dar.xml.path}" target="run-build-dar"/>
</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\classes\META-INF\maven\com.poalim.documentum\BnhpInfraDFArtifacts\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Wed Jun 09 16:48:58 IDT 2021
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\BnhpInfraDFArtifacts
m2e.projectName=BnhpInfraDFArtifacts
groupId=com.poalim.documentum
artifactId=BnhpInfraDFArtifacts
version=1.0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\classes\META-INF\maven\com.poalim.documentum\BnhpInfraDFArtifacts\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>

	<artifactId>BnhpInfraDFArtifacts</artifactId>
	<packaging>jar</packaging>
	<name>BnhpInfraDFArtifacts</name>
	<description>BnhpInfraDFArtifacts</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency>

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\maven-archiver\pom.properties
-----------------------------------------------------
#Generated by Maven
#Thu Mar 11 15:42:06 IST 2021
version=1.0
groupId=com.poalim.documentum
artifactId=BnhpInfraDFArtifacts


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\test-classes\dfc.properties
-----------------------------------------------------
dfc.data.dir=C:/Documentum
dfc.registry.mode=file
dfc.search.ecis.enable=false
dfc.search.ecis.host=
dfc.search.ecis.port=
dfc.tokenstorage.dir=C:/Documentum/apptoken
dfc.tokenstorage.enable=false
dfc.docbroker.host[0]=172.31.55.212	
dfc.docbroker.port[0]=1489
dfc.globalregistry.password=AAAAEC/2hx7DpcHjndR7Dt5iEvPtxvLuQevIyewwAkjxg+M1
dfc.globalregistry.repository=banhap_dev1
dfc.globalregistry.username=dm_bof_registry


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\target\test-classes\log4j.properties
-----------------------------------------------------
log4j.rootCategory=WARN, A1, F1
log4j.category.MUTE=OFF
log4j.additivity.tracing=false
log4j.category.tracing=DEBUG, FILE_TRACE

#------------------- CONSOLE --------------------------
log4j.appender.A1=org.apache.log4j.ConsoleAppender

log4j.appender.A1.threshold=ERROR
log4j.appender.A1.layout=org.apache.log4j.PatternLayout
log4j.appender.A1.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c - %m%n

#------------------- FILE --------------------------
log4j.appender.F1=org.apache.log4j.RollingFileAppender

log4j.appender.F1.File=C\:/Documentum/logs/log4j.log
log4j.appender.F1.MaxFileSize=10MB
log4j.appender.F1.layout=org.apache.log4j.PatternLayout
log4j.appender.F1.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c - %m%n

#------------------- FILE_TRACE --------------------------
log4j.appender.FILE_TRACE=org.apache.log4j.RollingFileAppender

log4j.appender.FILE_TRACE.File=C\:/Documentum/logs/trace.log
log4j.appender.FILE_TRACE.MaxFileSize=100MB
log4j.appender.FILE_TRACE.layout=org.apache.log4j.PatternLayout
log4j.appender.FILE_TRACE.layout.ConversionPattern=%d{ABSOLUTE} [%t] %m%n


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\tests\resources\dfc.properties
-----------------------------------------------------
dfc.data.dir=C:/Documentum
dfc.registry.mode=file
dfc.search.ecis.enable=false
dfc.search.ecis.host=
dfc.search.ecis.port=
dfc.tokenstorage.dir=C:/Documentum/apptoken
dfc.tokenstorage.enable=false
dfc.docbroker.host[0]=172.31.55.212	
dfc.docbroker.port[0]=1489
dfc.globalregistry.password=AAAAEC/2hx7DpcHjndR7Dt5iEvPtxvLuQevIyewwAkjxg+M1
dfc.globalregistry.repository=banhap_dev1
dfc.globalregistry.username=dm_bof_registry


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\tests\resources\log4j.properties
-----------------------------------------------------
log4j.rootCategory=WARN, A1, F1
log4j.category.MUTE=OFF
log4j.additivity.tracing=false
log4j.category.tracing=DEBUG, FILE_TRACE

#------------------- CONSOLE --------------------------
log4j.appender.A1=org.apache.log4j.ConsoleAppender

log4j.appender.A1.threshold=ERROR
log4j.appender.A1.layout=org.apache.log4j.PatternLayout
log4j.appender.A1.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c - %m%n

#------------------- FILE --------------------------
log4j.appender.F1=org.apache.log4j.RollingFileAppender

log4j.appender.F1.File=C\:/Documentum/logs/log4j.log
log4j.appender.F1.MaxFileSize=10MB
log4j.appender.F1.layout=org.apache.log4j.PatternLayout
log4j.appender.F1.layout.ConversionPattern=%d{ABSOLUTE} %5p [%t] %c - %m%n

#------------------- FILE_TRACE --------------------------
log4j.appender.FILE_TRACE=org.apache.log4j.RollingFileAppender

log4j.appender.FILE_TRACE.File=C\:/Documentum/logs/trace.log
log4j.appender.FILE_TRACE.MaxFileSize=100MB
log4j.appender.FILE_TRACE.layout=org.apache.log4j.PatternLayout
log4j.appender.FILE_TRACE.layout.ConversionPattern=%d{ABSOLUTE} [%t] %m%n


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\tests\src\bnhp\infra\base\aspect\BnhpInfraDFArtifactsAbstractTest.java
-----------------------------------------------------
package bnhp.infra.base.aspect;

import org.junit.BeforeClass;

import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfLoginInfo;

/**
 * FIXME: make connection properties configurable using properties
 * 
 * @author FW0
 *
 */

public abstract class BnhpInfraDFArtifactsAbstractTest {
	public static String DOCBASE = "banhap_dev1";
	public static String USER = "dctm";
	public static String PASSWORD = "dctm1";
	public boolean isDFCinitialized = false;
	public static IDfSessionManager s_sm = null;
	public static IDfClientX s_clientx = null;
	public static IDfClient s_client = null;
	public static IDfLoginInfo s_identity = null;
	
	@BeforeClass
	public static void setUp() throws DfException {
		if (null == s_clientx) {
			s_clientx = new DfClientX();
		}
		if (null == s_client) {
			s_client = s_clientx.getLocalClient();
		}
		if (null == s_sm) {
			s_sm = s_client.newSessionManager();
		}
		if (null == s_identity) {
			s_identity = s_clientx.getLoginInfo();
			s_identity.setUser(USER);
			s_identity.setPassword(PASSWORD);
			s_sm.setIdentity(DOCBASE, s_identity);
		}
	}
	

	public IDfSession getDocbaseSession() throws DfException {
		return s_sm.getSession(DOCBASE);
	}
	
	public void releaseDocbaseSession(IDfSession pSess) throws DfException {
		s_sm.release(pSess);
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\tests\src\bnhp\infra\base\aspect\GeneralDocAspectTests.java
-----------------------------------------------------
package bnhp.infra.base.aspect;

import org.junit.Assert;
import org.junit.Test;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;


public class GeneralDocAspectTests extends BnhpInfraDFArtifactsAbstractTest {
	
	protected String getExistingLegacyId(IDfSession session) throws DfException {
		String result = null;
		IDfSysObject obj = (IDfSysObject)
		session.getObjectByQualification("bnhp_customer_doc  where legacy_document_id<>' '");
		result = obj.getString("legacy_document_id");
		return result;
	}
	
	
	@Test
	public void testBusinessConstraints() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;
		
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			String existingLegId = getExistingLegacyId(session);
			Assert.assertNotNull(existingLegId);
			Assert.assertFalse("".equals(existingLegId.trim()));

			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			obj.setString("legacy_document_id",existingLegId);
			obj.setString("object_name","TEST_OBJECT_CONTSRAINTS");
			obj.save();
			
			Assert.fail("Object should not have been created");
		} catch (DfException ex) {
			Assert.assertTrue(ex.getMessage().matches(".*Business constraint violated.*"));
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=col) {
				try { col.close();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}
	
	@Test
	public void testBusinessConstraintsViaDQLUpdate() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;
		IDfId createdId = null;
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			String existingLegId = getExistingLegacyId(session);
			Assert.assertNotNull(existingLegId);
			Assert.assertFalse("".equals(existingLegId.trim()));

			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			obj.setString("object_name","TEST_OBJECT_CONTSTRAINTS_UPDATE");
			obj.save();
			
			
			IDfQuery query = new DfQuery();
			query.setDQL("UPDATE bnhp_customer_doc OBJECT "
					+ "set legacy_document_id='"+existingLegId+"'"+
					" WHERE r_object_id='"+obj.getObjectId().getId()+"'");
			col = query.execute(session, IDfQuery.DF_EXEC_QUERY);
			while (col.next()) {
				Assert.assertNull(createdId);
				createdId = col.getId("object_updated");
				Assert.assertNotNull(createdId);
			}
			Assert.fail("Object should not have been updated");
			
		} catch (DfException ex) {
			Assert.assertTrue(ex.getMessage().matches(".*Business constraint violated.*"));
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=session && null!=createdId) {
				try {
					session.apiExec("destroy", createdId.getId());
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=col) {
				try { col.close();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}
	
	// this one fails, probably because we use aspect and not regular TBOS for class
	@Test
	public void testBusinessConstraintsViaDQLCREATE() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;
		IDfId createdId = null;
		try {
			session = this.getDocbaseSession();
			
			String existingLegId = getExistingLegacyId(session);
			Assert.assertNotNull(existingLegId);
			Assert.assertFalse("".equals(existingLegId.trim()));
			IDfQuery query = new DfQuery();
			query.setDQL("CREATE bnhp_customer_doc OBJECT "
					+ "set OBJECT_NAME='TEST_OBJECT', set legacy_document_id='"+existingLegId+"'");
			col = query.execute(session, IDfQuery.DF_EXEC_QUERY);
			while (col.next()) {
				Assert.assertNull(createdId);
				createdId = col.getId("object_created");
				Assert.assertNotNull(createdId);
			}
			Assert.fail("Object should not have been created");
		} catch (DfException ex) {
			Assert.assertTrue(ex.getMessage().matches(".*Business constraint violated.*"));
		} finally {
			if (null!=session && null!=createdId) {
				try {
					session.apiExec("destroy", createdId.getId());
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=col) {
				try { col.close();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}
	
	@Test
	public void testSelectStorageExDivId() throws DfException {
		IDfSession session = null;
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(obj);
			obj.setInt("executing_branch_id",817);
			obj.setInt("executing_bank_id",12);
			obj.setObjectName("TEST_EXDIVID_STORAGE");
			obj.setTitle("Lost in action");
			obj.save();
			
			Assert.assertEquals(5, obj.getInt("executing_division_id"));
			// no change!
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}
	}
	
	 
	
	
	@Test
	public void testSelectStorageTBOCustomerDoc() throws DfException {
		IDfSession session = null;
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(obj);
			obj.setObjectName("TEST_OBJECT_STORAGE");
			obj.setContentType("pdf");
			obj.setInt("project_id",2);
			obj.setTitle("TEST_OBJECT_STORAGE");
			obj.save();
			
			Assert.assertEquals("Tofes-Peula", obj.getStorageType());
			
			obj.setInt("project_id",3);
			obj.save();
			Assert.assertEquals("filestore_01", obj.getStorageType());
			
			
			obj.setInt("project_id",1);
			obj.save();
			Assert.assertEquals("Tofes-Peula", obj.getStorageType());
			
			obj.setTitle("Lost in action");
			obj.save();
			// no change!
			Assert.assertEquals("Tofes-Peula", obj.getStorageType());
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
		
	}
	@Test
	public void testRootFlag() throws DfException {
		final String VERSION_TREE_ROOT_IND = "version_tree_root_ind";
		IDfSession session = null;
		IDfCollection col = null;
		
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			obj.setString("object_name","TEST_ROOT_FLAG");
			obj.save();
			Assert.assertTrue(obj.getBoolean(VERSION_TREE_ROOT_IND));
			obj.save();
			Assert.assertTrue(obj.getBoolean(VERSION_TREE_ROOT_IND));
			obj.setBoolean(VERSION_TREE_ROOT_IND, false);
			obj.save();
			Assert.assertTrue(obj.getBoolean(VERSION_TREE_ROOT_IND));
			obj.checkout();
			IDfId newId = obj.checkin(false, null);
			Assert.assertTrue(obj.getBoolean(VERSION_TREE_ROOT_IND));
			IDfSysObject newObj = (IDfSysObject)session.getObject(newId);
			Assert.assertFalse(newObj.getBoolean(VERSION_TREE_ROOT_IND));
			newObj.setBoolean(VERSION_TREE_ROOT_IND, true);
			newObj.save();
			Assert.assertFalse(newObj.getBoolean(VERSION_TREE_ROOT_IND));
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				//try { obj.destroyAllVersions();} catch (Exception ex) {
				//	ex.printStackTrace();
				//}
			}
			if (null!=col) {
				try { col.close();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\tests\src\CheckStorage.java
-----------------------------------------------------
import com.documentum.fc.client.*;
import com.documentum.fc.client.acs.IDfAcsRequest;
import com.documentum.fc.client.acs.IDfAcsTransferPreferences;
import com.documentum.fc.client.acs.impl.DfAcsTransferPreferences;
import com.documentum.fc.common.*;
import java.io.PrintStream;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.Date;

public class CheckStorage
{

    private static void log(String string)
    {
        System.err.println((new StringBuilder(String.valueOf(ts()))).append(": ").append(string).toString());
    }

    private static String ts()
    {
        return dateFormat.format(new Date());
    }

    private static IDfSession getSession(IDfSessionManager sm, String docbase, String user, String password)
        throws DfException
    {
        log((new StringBuilder("using dfc version ")).append(DfClient.getDFCVersion()).toString());
        log((new StringBuilder("connecting to docbase '")).append(docbase).append("' as user '").append(user).append("'").toString());
        IDfLoginInfo identity = new DfLoginInfo();
        identity.setUser(user);
        identity.setPassword(password);
        sm.setIdentity(docbase, identity);
        IDfSession session = sm.getSession(docbase);
        log((new StringBuilder("connected to server ")).append(session.getServerVersion()).toString());
        return session;
    }

    public static void main(String argv[])
        throws Exception
    {
        if(argv.length < 4)
        {
            System.err.println("Arguments: docbase user password project");
            System.exit(1);
        }
        String docbase = argv[0];
        String user = argv[1];
        String password = argv[2];
        String projectIdStr = argv[3];
        int projectId = 0;
        try {
           projectId = Integer.parseInt(projectIdStr);
        } catch(NumberFormatException nex) {
           log("ERROR: Wrong number format for page num");
           System.exit(1);
        }

        DfClient clnt = new DfClient();
        IDfSessionManager sm = clnt.newSessionManager();
        IDfSession session = getSession(sm, docbase, user, password);
        IDfSysObject obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
	obj.setInt("project_id",projectId);
	obj.save();
	System.err.println("saved object "+obj.getObjectId().getId()+" projectId="+projectId);
	obj.checkout();
	obj.setObjectName("orig obj - "+obj.getObjectId().getId()); 
	obj.setContentType("text");
	obj.setFile("junk.txt");
	IDfId newId = obj.checkin(false,"");
	System.err.println("checked in as "+ newId.getId());
	IDfSysObject newObj = (IDfSysObject)session.getObject(newId);
	System.err.println("Storage type: "+newObj.getString("a_storage_type"));
        sm.release(session);
    }

    private static DateFormat dateFormat = new SimpleDateFormat("HH:mm:ss.SSS");

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraDFArtifacts\version.properties
-----------------------------------------------------
#Wed Apr 15 14:35:16 GMT+02:00 2015
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\bin\META-INF\maven\com.poalim.documentum\BnhpInfraRequests\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Thu May 20 19:17:40 IDT 2021
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\BnhpInfraRequests
m2e.projectName=BnhpInfraRequests
groupId=com.poalim.documentum
artifactId=BnhpInfraRequests
version=1.0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\bin\META-INF\maven\com.poalim.documentum\BnhpInfraRequests\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	
	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>
	
	<artifactId>BnhpInfraRequests</artifactId>
	<packaging>jar</packaging>
	<name>BnhpInfraRequests</name>
	<description>BnhpInfraRequests</description>		
	
	<dependencies>
		<dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>build-dar</artifactId>
        	<version>1.0-SNAPSHOT</version>
        	<type>xml</type>
        	<scope>assembly</scope>
        </dependency>
	</dependencies>
	<build>
	
		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>

		<plugins>
		 <plugin>
    			<artifactId>maven-clean-plugin</artifactId>
			    <configuration>
			      <filesets>
			        <fileset>
			          <directory>bin-dar</directory>
			          <includes>
			            <include>*.dar</include>
			          </includes>
			          <followSymlinks>false</followSymlinks>
			        </fileset>
			      </filesets>
			    </configuration>
			</plugin>
		    <plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals><goal>run</goal></goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>build-helper-maven-plugin</artifactId>
               <executions>
                  <execution>
                    <id>attach-dar</id>
                    <phase>package</phase>
                    <goals>
							<goal>attach-artifact</goal>
						</goals>
              	  </execution>
               </executions>
            </plugin>
            <plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
		</plugins>
		
	</build>
	
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\bin-dar\keepme.txt
-----------------------------------------------------
keep me


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	
	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>
	
	<artifactId>BnhpInfraRequests</artifactId>
	<packaging>jar</packaging>
	<name>BnhpInfraRequests</name>
	<description>BnhpInfraRequests</description>		
	
	<dependencies>
		<dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>build-dar</artifactId>
        	<version>1.0-SNAPSHOT</version>
        	<type>xml</type>
        	<scope>assembly</scope>
        </dependency>
	</dependencies>
	<build>
	
		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>

		<plugins>
		 <plugin>
    			<artifactId>maven-clean-plugin</artifactId>
			    <configuration>
			      <filesets>
			        <fileset>
			          <directory>bin-dar</directory>
			          <includes>
			            <include>*.dar</include>
			          </includes>
			          <followSymlinks>false</followSymlinks>
			        </fileset>
			      </filesets>
			    </configuration>
			</plugin>
		    <plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals><goal>run</goal></goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>build-helper-maven-plugin</artifactId>
               <executions>
                  <execution>
                    <id>attach-dar</id>
                    <phase>package</phase>
                    <goals>
							<goal>attach-artifact</goal>
						</goals>
              	  </execution>
               </executions>
            </plugin>
            <plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
		</plugins>
		
	</build>
	
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\target\antrun\build-main.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8" ?>
<project name="maven-antrun-" default="main"  >
<target name="main">
  <property name="project" value="BnhpInfraRequests"/>
  <property name="headlesscomposer" value="C:/Users/AP068/dctm/HeadlessComposer//7.1"/>
  <ant antfile="${maven.dependency.com.poalim.documentum.build-dar.xml.path}" target="run-build-dar"/>
</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\target\classes\META-INF\maven\com.poalim.documentum\BnhpInfraRequests\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Mon May 24 13:42:03 IDT 2021
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\BnhpInfraRequests
m2e.projectName=BnhpInfraRequests
groupId=com.poalim.documentum
artifactId=BnhpInfraRequests
version=1.0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\target\classes\META-INF\maven\com.poalim.documentum\BnhpInfraRequests\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	
	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>
	
	<artifactId>BnhpInfraRequests</artifactId>
	<packaging>jar</packaging>
	<name>BnhpInfraRequests</name>
	<description>BnhpInfraRequests</description>		
	
	<dependencies>
		<dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>build-dar</artifactId>
        	<version>1.0-SNAPSHOT</version>
        	<type>xml</type>
        	<scope>assembly</scope>
        </dependency>
	</dependencies>
	<build>
	
		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>

		<plugins>
		 <plugin>
    			<artifactId>maven-clean-plugin</artifactId>
			    <configuration>
			      <filesets>
			        <fileset>
			          <directory>bin-dar</directory>
			          <includes>
			            <include>*.dar</include>
			          </includes>
			          <followSymlinks>false</followSymlinks>
			        </fileset>
			      </filesets>
			    </configuration>
			</plugin>
		    <plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals><goal>run</goal></goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>build-helper-maven-plugin</artifactId>
               <executions>
                  <execution>
                    <id>attach-dar</id>
                    <phase>package</phase>
                    <goals>
							<goal>attach-artifact</goal>
						</goals>
              	  </execution>
               </executions>
            </plugin>
            <plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
		</plugins>
		
	</build>
	
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\target\maven-archiver\pom.properties
-----------------------------------------------------
#Generated by Maven
#Wed Mar 10 17:16:41 IST 2021
version=1.0
groupId=com.poalim.documentum
artifactId=BnhpInfraRequests


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpInfraRequests\version.properties
-----------------------------------------------------
#Wed Apr 15 14:21:57 GMT+02:00 2015
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\bof_package.xml
-----------------------------------------------------
<project name="bof_package" default="buildAndRefresh">
	<!--This scripts rebuild jar files of BOF and copies them into 
		proper place for composer to see updated files. It's 
		supposed to be run from within eclipse ant builder
		
		For DAR file to be properly built this script must be envoked 
		BEFORE building of dar, that is ant builder must be envoked before 
		Documentum Builder in the eclipse build configuration
    -->
	
	<!-- this defines task for building/copiying individual jar -->
	<macrodef name="build_jar_artifact">
	 		<!-- filename of jar to be built -->
	 		<attribute name="name"/>
	 		<!-- files to put into jar (used by jar task) -->
	 		<attribute name="classfilter"/>
	 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
	 		<attribute name="jardef"/>
	 		<sequential>
	 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
				<echo message="** rebuilding ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
				<echo message="   class filter: @{classfilter}"/>
				<!--  delete jar file in project root -->
				<delete verbose="false" file="@{name}" failonerror="false"/> 
				 
				<!--  make jar file in project root -->			 
				<jar destfile="@{name}" 
					basedir="target/classes" includes="@{classfilter}">
				</jar>
	 			<!--  copy jar to proper location (content/...) that is known to composer -->
	 				 			<!-- force tries to override read-only protection (those files must not be stored in VCS) --> 
	 							<move file="@{name}" 
	 								tofile="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"
	 								overwrite="true" 
	 								force="true" 
	 								failonerror="true"
	 				 				verbose="true"/>
	 		</sequential>
 	</macrodef>
	 		
	<!-- this defines task for deleting individual jar -->
	<macrodef name="clean_jar_artifact">
		 		<!-- filename of jar to be built -->
		 		<attribute name="name"/>
		 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
		 		<attribute name="jardef"/>
		 		<sequential>
		 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
					<echo message="** cleaning ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
								<!--  delete jar file in project root -->
					<delete file="@{name}" failonerror="true"/>
		 			<delete verbose="true" file="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}" failonerror="true"/> 
		 		</sequential>
 	</macrodef>
 		
	<target name="cleanAndRefresh">
			<antcall target="clean" />
			<eclipse.refreshLocal resource="/" depth="infinite"/>
	</target>
	
	<target name="clean">
		<clean_jar_artifact 
			name="bnhp_storage_selection_service_impl.jar" 
			jardef="Artifacts/JAR Definitions/bnhp_storage_selection_service_impl.jardef"/>
		<clean_jar_artifact 
			name="bnhp_storage_selection_service_iface.jar" 
			jardef="Artifacts/JAR Definitions/bnhp_storage_selection_service_iface.jardef"/>
		<clean_jar_artifact 
			name="t_storage_change_impl.jar" 
			jardef="Artifacts/JAR Definitions/t_storage_change_impl.jardef"/>
		<clean_jar_artifact 
			name="t_storage_change_iface.jar" 
			jardef="Artifacts/JAR Definitions/t_storage_change_iface.jardef"/>
	</target>

	
	<target name="build">
		<build_jar_artifact 
			name="bnhp_storage_selection_service_impl.jar" 
			classfilter="bnhp/infra/base/storage/impl/BnhpStorageSelectionService*,bnhp/infra/base/storage/impl/BnhpStorageSelector*,bnhp/infra/base/storage/impl/DfStorageRuleMatcher*"
			jardef="Artifacts/JAR Definitions/bnhp_storage_selection_service_impl.jardef"/>
		<build_jar_artifact 
			name="bnhp_storage_selection_service_iface.jar" 
			classfilter="bnhp/infra/base/storage/iface/IBnhpStorageSelectionService*,bnhp/infra/base/storage/iface/IBnhpStorageSelector*"
			jardef="Artifacts/JAR Definitions/bnhp_storage_selection_service_iface.jardef"/>
		<build_jar_artifact 
			name="t_storage_change_impl.jar" 
			classfilter="bnhp/infra/base/storage/impl/DfTSTorageChange*"
			jardef="Artifacts/JAR Definitions/t_storage_change_impl.jardef"/>
		<build_jar_artifact 
			name="t_storage_change_iface.jar" 
			classfilter="bnhp/infra/base/storage/iface/IDfTStorageChange*"
			jardef="Artifacts/JAR Definitions/t_storage_change_iface.jardef"/>
	</target>
	<target name="buildAndRefresh">
		<antcall target="build" />
		<eclipse.refreshLocal resource="/" depth="infinite"/>
	</target>	
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\build-dar.properties
-----------------------------------------------------

headlesscomposer=/home/FW0/java/HeadlessComposer


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\content\08\-702791908\testRulesCreation[1].xml
-----------------------------------------------------
<?xml version="1.0" ?>
	<StorageRules>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[tT].*$</Value>
			</RuleFactor>
			<StorageType>Tofes Peula</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[fF].*$</Value>
			</RuleFactor>
			<StorageType>fax project</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[cC].*$</Value>
			</RuleFactor>
			<StorageType>Customer Doc</StorageType>
		</RuleLine>
	</StorageRules>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>

	<artifactId>BnhpStoragePolicy</artifactId>
	<packaging>jar</packaging>
	<name>BnhpStoragePolicy</name>
	<description>BnhpStoragePolicy</description>


	<dependencies>

		<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
		</dependency>
		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>

		<plugins>

			<!-- The JAR needs to contain ONLY IBnhpStorageSelectionService class -->
			<plugin>
				<artifactId>maven-jar-plugin</artifactId>
				<configuration>
					<includes>
						<include>**/*IBnhpStorageSelectionService.class</include>
					</includes>
				</configuration>
			</plugin>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>

		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\iface\IBnhpStorageSelectionService.java
-----------------------------------------------------
package bnhp.infra.base.storage.iface;


import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

/**
 * This service determines proper destination for storage
 * of given sysobject.  Service is assumed to be used from
 * TBO or DFC applications that want to determine preferred
 * storage location for object contents. 
 * 
 * Service is configured by content of per-type configuration object
 * (dm_sysobject), which is located in /System/Applications/BnhpStoragePolicy
 * and named 'bnhp_storage_selector_{Type of Object}' There must be
 * only one file with that name in thst folder.
 * 
 * Unlike EMC storage policies configuration object is searched by location
 * and not by relations and does not try to read config of supertypes.
 * 
 * Service uses same config format as EMC storage policies, but 
 * supports hand editing of content of configuration objects.
 *  
 * Examples of configuration format see in the tests
 * /bnhp/infra/base/storage/testRulesCreation.xml
 * 
 *  * 
 * @author FW0
 *
 */
public interface IBnhpStorageSelectionService {
	/**
	 * Returns storage name for sys object  
	 * 
	 * @param  obj
	 * @return Name of storage to be used for file
	 * @throws DfException
	 */
	String getStorageDest(IDfSysObject obj) throws DfException;
	
	/**
	 * Returns storage name for sys object or rendition
	 * 
	 * @param obj
	 * @param formatName  format of rendition, may be null. 
	 * Value of this parameter is checked against "format" parameter
	 * in service configuration file
	 * @param pageModifier page modifier of rendition. 
	 * Value of this parameter is checked against "pageModifier" parameter
	 * in service configuration file 
	 * @return Name of storage to be used for file 
	 * @throws DfException
	 */
	String getStorageDest(IDfSysObject obj, 
			String formatName,
			String pageModifier) throws DfException;
	
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\iface\IDfTStorageChange.java
-----------------------------------------------------
package bnhp.infra.base.storage.iface;

import  com.documentum.fc.client.IDfDocument;
import com.documentum.fc.common.DfException;

public interface IDfTStorageChange extends IDfDocument {
	public void applyStoragePolicy() throws DfException;
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\impl\BnhpStorageSelectionService.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;

import java.util.HashMap;

import bnhp.infra.base.storage.iface.IBnhpStorageSelectionService;

import com.documentum.fc.client.DfService;
import com.documentum.fc.client.IDfFormat;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;

/**
 * 
 * Implementation of storage selection service. Please see
 * documentation in the interface declaration
 * bnhp.infra.base.storage.iface.BnhpStorageSelectionService
 * 
 * @author FW0
 *
 */
public class BnhpStorageSelectionService extends DfService implements IBnhpStorageSelectionService{
	
	/* hash of selectors per combinations of docbase+object type, note
	 * that hash is one for all service instances */
	private static HashMap<String, BnhpStorageSelector> sSelectors = 
		new HashMap<String,BnhpStorageSelector>();
	
	/* create new selector */
	protected BnhpStorageSelector makeNewSelector(String p_docbaseName, String p_type) {
		return new BnhpStorageSelector(p_docbaseName, p_type);
	}
	
	/**
	 * finds or creates selector for given docbase+type pair 
	 * 
	 */
	protected BnhpStorageSelector getSelector(
			IDfSession p_session, String p_type) {
		String docbaseName = "";
		try {
			DfLogger.debug(this, 
					"getSelector: called for type {0} in docbase {1}", 
					new String[] {p_type, docbaseName},null);
			BnhpStorageSelector selector = null;
			docbaseName = p_session.getDocbaseName();
			String key = docbaseName+"/"+p_type;
			
			DfLogger.debug(this, 
					"getSelector: will try to get precreated selector from hash", 
					new String[] {p_type, docbaseName},null);
			/* get selector if it exists ore create new */
			synchronized (sSelectors) {
				DfLogger.debug(this, 
						"getSelector: in critical section", 
						null,null);
				selector = sSelectors.get(key);
				if (null == selector) {
					DfLogger.debug(this, 
							"getSelector: no precreated selector, creating new one for type {0} in docbase {1}", 
					new String[] {p_type, docbaseName},null);
					selector = makeNewSelector(docbaseName, p_type); 
					sSelectors.put(key, selector);
					DfLogger.debug(this, 
							"getSelector: selector created",null,null);

				} else {
					DfLogger.debug(this,"getSelector: got precreated selector  for type {0} in docbase {1}",
							new String[] {p_type, docbaseName},null);
				}
			}
			// here we have selector that may or may be not configured
			if (!selector.isUpToDate()) {
				// configure configures both uninitialized selectors
				// and initialized, which are not up to date
				DfLogger.debug(this, 
						"will try to update selector for type {0} in docbase {1}", 
						new String[] {p_type, docbaseName},null);
				// configure is synchronized
				selector.configure(p_session);
			} else {
				DfLogger.debug(this,"getSelector: selector for type {0} in docbase {1} is up-to-date",
						new String[] {p_type, docbaseName},null);
			}
			return selector;
		} catch (DfException ex) {
			DfLogger.error(this, 
					"Failed to create/initialize storage selector for type {0} in docbase {1}", 
					new String[] {p_type, docbaseName}, ex);
			return null;
		}
	}
	
	/* standard service methods, see DFC dev. guide */
	@Override
	public String getVendorString() {
		return "Bank HaPoalim";
	}

	@Override
	public String getVersion() {
		return "1.0";
	}

	@Override
	public boolean isCompatible(String arg0) {
		return true;
	}

	/* Below methods of service user API
	/**
	 * returns storage name for sys object  
	 * 
	 * @param obj
	 * @return
	 * @throws DfException
	 */
	public String getStorageDest(IDfSysObject p_obj) throws DfException {
		DfLogger.debug(this,"getStorageDest(1): called for object with id={0} of type {1}",
				new String[] {p_obj.getObjectId().getId(),p_obj.getTypeName()},null);
		/* format if not given - use it from sysobject */ 
		String formatName = null;
		try {
			IDfFormat format = p_obj.getFormat();
			if (null!=format) {
				formatName = format.getName();
				DfLogger.debug(this,"getStorageDest 1: using given format {0}",new String[] {formatName},null);
			}
		} catch (Exception ex) {
			DfLogger.error(this, 
					"Failed to get format of document '{0}'", 
					new String[] {p_obj.getObjectId().toString()}, ex);
			
		}
		return getStorageDest(p_obj, formatName, null);
	}
	
	/**
	 * Returns storage name for rendition
	 * 
	 * @param obj sysobject
	 * @param formatName rendition format
	 * @param pageModifier rendition page modifier string
	 * @return storage name or null if no matching rule has been found
	 * @throws DfException
	 */
	public String getStorageDest(IDfSysObject obj, 
			String formatName,
			String pageModifier) throws DfException {
		DfLogger.debug(this,"getStorageDest(2): called for object with id={0} of type {1}, format={2} modifier={3}",
				new String[] {obj.getObjectId().getId(),obj.getTypeName(),formatName,pageModifier},null);
	
		BnhpStorageSelector selector = getSelector(obj.getSession(), obj.getTypeName());
		/* if no config or it's invalid - use default policy */ 
		if (null == selector) {
			DfLogger.debug(this,"no selector defined for type {0}, using default policy ",
					new String[] {obj.getTypeName()},null);
			return null;
		}
		return selector.getStorageDest(obj, formatName, pageModifier);
	}


	
	
	
}


	


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\impl\BnhpStorageSelector.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;


import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.util.Date;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;

import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.w3c.dom.Document;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.DfServiceException;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfStorageRuleLine;
import com.documentum.fc.client.IDfStorageRules;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.storagepolicy.DfStoragePolicyUtils;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.internal.xml.IXMLOutputFormat;
import com.documentum.fc.internal.xml.IXMLSerializer;
import com.documentum.fc.internal.xml.XMLUtilsFactory;
import com.documentum.xml.common.DfErrorHandler;
import com.documentum.xml.jaxp.DfDocumentBuilderFactory;


/**
 * Performs storage selection for type 
 * 
 * Note: Code of this class is used reentrantly by different threads
 * 
 * @author FW0
 *
 */
public class BnhpStorageSelector /*implements IBnhpStorageSelectionService.IBnhpStorageSelector*/ {
	// default length of interval between policy updates, if intervall passed
	// update will be performied before handling of query
	static final long POLICY_UPDATE_INTERVAL_MSEC = 120000;

	// definitions for configuration object
	static final String CONFIG_OBJECT_TYPE = "dm_sysobject";
	static final String STORAGE_SELECTOR_FOLDER="/System/Applications/BnhpStoragePolicy";
	static final String STORAGE_SELECTOR_NAME_PREFIX="bnhp_storage_selector_";
	
	
	// datetime of last update of selector
	// null means it's not yet initialized
	protected Date m_updated = null; 
	// version id - combination of object id and vstamp of
	// configuration object from which selector was configured.
	protected String m_versionId = null;
	// name of docbase - used to prevent erroneous queries for another docbase
	protected String m_docbaseName = null;
	// object type for which storage is selected
	protected String m_type;
	
	// length of interval between policy updates (msec), if intervall passed
	// currently it's changed from default value only by tests 
	protected long m_interval = POLICY_UPDATE_INTERVAL_MSEC;
	// rules - may be null if object not initialized, if there 
	// is no config object or there was problem loading configuration
	IDfStorageRules m_rules = null;


	protected BnhpStorageSelector(String p_docbaseName, String p_type) {
		m_docbaseName = p_docbaseName;
		m_type = p_type;
	}


    protected long getPolicyUpdateInterval() {
        return m_interval;
    }
    
    protected long setPolicyUpdateInterval(long p_interval) {
        return m_interval = p_interval;
    }

    /* check if object is up to date */
    protected boolean isUpToDate() {
    	// it's important to read value once
    	// and not to use m_update after that since
    	// it may be updated while we're in process of checking
    	Date updated = m_updated;
    	long current = System.currentTimeMillis();
    	return (null == updated ||  
    			(current - updated.getTime())>getPolicyUpdateInterval()) ? false:true;  
    }

    /* get storage rules, IDfStorageRule is format that documentum
     * storage services use to keep it's storage policies. 
     * We use it only for reading/storage of rules. Interpretation of rules
     * is does not depend on EMC code
     */
    protected IDfStorageRules getRules() {
        return m_rules;
    }
	
	
    /* building of path for config object */
	protected String getConfigName() {
		return (new StringBuffer()).append(STORAGE_SELECTOR_NAME_PREFIX).append(m_type).toString();
	}
	
	protected String getConfigPath() {
		String confPath = (new StringBuffer()).append(STORAGE_SELECTOR_FOLDER).
		append("/").append(getConfigName()).toString();
		return confPath;
	}
	
	

	 /**
	  * Parser of DfStorageRules stupidly fails to parse
	  * xml files that contain whitespace between tags.
	  * This method fixes XML input before feeding it to parser.
	  * 
	  * FIXME: Performancewise probably there is sence 
	  * to write our own xml reading routine instead of using EMC's
	  * 
	  */
	 protected ByteArrayInputStream prepareRuleConfigStream(ByteArrayInputStream is)  throws DfException {
		 DfLogger.debug(this,"prepareRuleConfigStream: started",null,null);
         ByteArrayInputStream result;
         try {
        	 // create dom document
             DocumentBuilderFactory builderFactory; 
             builderFactory = DfDocumentBuilderFactory.newInstance();
             builderFactory.setNamespaceAware(true);
             builderFactory.setIgnoringElementContentWhitespace(true);
             builderFactory.setIgnoringComments(true);
             builderFactory.setCoalescing(true);
             builderFactory.setValidating(false);
             
             XMLUtilsFactory factory = new XMLUtilsFactory();
             
             DocumentBuilder builder = builderFactory.newDocumentBuilder();
             
             builder.setErrorHandler(new DfErrorHandler());
             Document document = builder.parse(is);
             // here whitespace is cleaned
             cleanWhitespace(document);
             
             // write it to byteoutput stream 
             ByteArrayOutputStream bos = new ByteArrayOutputStream();
                             
             IXMLSerializer serializer = factory.getXMLSerializer();
             IXMLOutputFormat format = factory.getXMLOutputFormat();
             format.setIndenting(false);
             format.setPreserveSpace(false);
             format.setPreserveEmptyAttributes(false);
             serializer.setOutputFormat(format);
             serializer.setOutputStream(bos);
             serializer.serialize(document.getDocumentElement());
             //and convert it back to input stream
             result = new ByteArrayInputStream(bos.toByteArray());
         } catch (Exception ex) {
             throw new DfException(ex);
         } finally {
        	 DfLogger.debug(this,"prepareRuleConfigStream: finished",null,null);
         }
         return result;
     }

	 
	 /* Cleans whitespace from XML document */
	 protected void cleanWhitespace(Node pnode) {
		 NodeList nlist=pnode.getChildNodes();  
		 for (int i=nlist.getLength()-1; i>-1; i--) {  
			 Node node=nlist.item(i);  
			 if (node.getNodeType()==Node.ELEMENT_NODE) {  
				 cleanWhitespace(node);
			 }  
			 if (node.getNodeType()==Node.TEXT_NODE) {
				 // nuke text nodes containing only whitespace
				 if (node.getTextContent().trim().length()==0) {
					 node.getParentNode().removeChild(node);    
				 }
			 }  
		 } 
	 }

	
	/* get content of configuration object */
	protected  ByteArrayInputStream getRuleConfigStream(IDfSession pSess) 
	throws DfException {
		DfLogger.debug(this,"getRuleConfigStream: started",null,null);
		ByteArrayInputStream result = null;
		String confPath = getConfigPath();
		DfLogger.debug(this,"getRuleConfigStream: getting BNHP storage selector config object {0}",
				new String[] {confPath},null);
		IDfSysObject confObj = (IDfSysObject)pSess.getObjectByPath(confPath);
		if (null!=confObj) {
			DfLogger.info(this, 
					"reading BNHP storage selector config from {0}", 
					new String[] {confPath}, null);
			result = confObj.getContent();
		} else {
			DfLogger.error(this, 
					"Failed to get BNHP storage selector config for type {0} on docbase {1} at {2}", 
					new String[] {m_type, m_docbaseName, confPath}, null);
		}
		return result;
	}
		
	/*
	 *  Gets version id of current config object in docbase,
	 *  that consists of r_object_id and vstamp of the config object.
	 *  if it changes, then config object was replaced or updated 
	 *  
	 *  Returns null if there is no config object
	 */
	protected String getConfigVersionId(IDfSession pSess) throws DfException {
		String queryStr = (new StringBuffer(60)).append("SELECT r_object_id, i_vstamp FROM ")
			.append(CONFIG_OBJECT_TYPE).append(" where object_name='").append(getConfigName())
			.append("' and FOLDER('").append(STORAGE_SELECTOR_FOLDER).append("')").toString();
		IDfQuery query = new DfQuery();
		query.setDQL(queryStr);
		IDfCollection col = query.execute(pSess, IDfQuery.DF_READ_QUERY);
		String versionId = null;
		try {
			for (int line = 0; col.next(); line++) {
				if (0 == line) {
					versionId = (new StringBuffer(25)).append(col.getId("r_object_id").toString())
							.append(",").append(col.getInt("i_vstamp")).toString();
				} else {
					/* multiple configs found, log warning */
					DfLogger.warn(this, 
							"duplicate config object for Storage Selection Service  type {0} on docbase {1} at path {2}", 
							new String[] {this.m_type, this.m_docbaseName, getConfigPath()}, null);
				}
			}
		} finally {
			if (null!=col) col.close();
		}
		return versionId;
	}
	
	/* parse config using EMC written routines */
	protected IDfStorageRules parseRules(ByteArrayInputStream pBis) throws DfException {
		return  DfStoragePolicyUtils.parseXML(pBis);
	}
	
	/* update configuration using session of given sysobject */
	protected void updateConfigForObject(IDfSysObject pObj) {
		try {
			IDfSession session = pObj.getSession();
			if (null!=session && session.isConnected()) {
				configure(session);
			} else {
				DfLogger.error(this, 
						"Failed to update BNHP storage selector config for type {0} on docbase {1} because has been called for object with non-existing or disconnected session", 
						new String[] {m_type, m_docbaseName}, null);

			}
		} catch (Exception ex) {
			DfLogger.error(this, 
					"Failed to update BNHP storage selector config for type {0} on docbase {1} because of exception", 
					new String[] {m_type, m_docbaseName}, null);
		}
	}
	
	/* Finds matching rule and returns storage for given document,
	 * If no matching rules exists returns  null
	 */
	protected String getStorageDest(IDfSysObject pObj, 
			String formatName, 
			String pageModifier) {
		DfLogger.debug(this,"getStorageDest: called",null,null);
	
		// it's important to not use m_rules after that - 
		// it may be updated while we're in process of checking
		if (!this.isUpToDate()) {
			DfLogger.debug(this,"getStorageDest: selector is not up-to-date, will update",null,null);
			updateConfigForObject(pObj);
		} else {
			DfLogger.debug(this,"getStorageDest: selector is up-to-date",null,null);
		}
		// need to read rules once before use,
		// since the reference may be replaced while we run
		DfLogger.debug(this,"getStorageDest: got selector, will process rules",null,null);
		IDfStorageRules rules = m_rules;
		if (null != rules) {
			DfLogger.debug(this,"getStorageDest: there are rules",new String[] {Integer.toString(rules.getRuleLineCount())},null);
			for ( int i = 0; i < rules.getRuleLineCount(); i++) {
				IDfStorageRuleLine rule = rules.getRuleLine(i);
				if (DfStorageRuleMatcher.matches(rule, pObj, formatName, pageModifier)) {
					// return result on first match
					DfLogger.debug(this,"getStorageDest: match found: {0}",new String[] {rule.getStorageType()},null);
					return rule.getStorageType();
				}
			}
			DfLogger.debug(this,"getStorageDest: finished processing rules, no match found",null,null);
		} else {
			DfLogger.debug(this,"getStorageDest: there are no rules, no match found",null,null);
		}
		return null;
	}
	
	protected void setRules(IDfStorageRules p_rules) {
		m_rules = p_rules;
	}
	
	/* tries to reconfigure selector, if configuration fails
	 * it keeps old rules (if any)
	 */
	protected synchronized boolean configure(IDfSession pSess) throws DfException{
		boolean result = false;
		DfLogger.debug(this,"configure: started",null,null);
		if (!pSess.getDocbaseName().equals(m_docbaseName)) {
			throw new DfServiceException("configure: called with session of wrong docbase "
					+pSess.getDocbaseName()+", expected: "+m_docbaseName);
		}
		// extra check to prevent extra reconfiguration 
		//if other thread was waiting on the entry lock
		if (!isUpToDate()) { 
			DfLogger.debug(this,"configure: object expired, need recheck",null,null);
			try {
				//DfStoragePolicyUtils utils = new DfStoragePolicyUtils();
				String newVersionId = getConfigVersionId(pSess);
				if (null== newVersionId) {
					// there is no configuration - disable the service
					if (null !=m_rules || null == this.m_updated) {
						// do not log every time about that 
						DfLogger.info(this, 
								"no configuration for BNHP storage selector config for type {0} on docbase {1}, disabling selector", 
								new String[] {this.m_type, this.m_docbaseName}, null);
						setRules(null);
					}
					m_versionId = null;
				} else if (null==m_versionId || (!newVersionId.equals(m_versionId))) {
					DfLogger.info(this, 
							" updating BNHP storage selector config for type {0} on docbase {1} from {2}", 
							new String[] {this.m_type, this.m_docbaseName, getConfigPath()}, null);
					// configuration updated or object not yet configured - read rules
					ByteArrayInputStream is = getRuleConfigStream(pSess);
					is = this.prepareRuleConfigStream(is);
					DfLogger.debug(this,"parsing rules",null,null);
					setRules(parseRules(is));
					DfLogger.debug(this,"parsing done",null,null);
					m_versionId = newVersionId;
				} else {
					DfLogger.debug(this,"configure: object was not changed, nothing to do",null,null);
					// configuration object not updated - nothing to do
				}
				result =  true;
			} catch (DfException dfex) {
				DfLogger.error(this, 
						"failed to read  BNHP storage selector config for type {0} on docbase {1}", 
						new String[] {this.m_type, this.m_docbaseName}, null);
			}
		} else {
			DfLogger.debug(this,"configure: object happened to be checked/updated by another thread",null,null);
		}
		m_updated = new Date();
		return result;
	}
};


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\impl\DfStorageRuleMatcher.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;

import com.documentum.fc.client.DfServiceException;
import com.documentum.fc.client.IDfStorageRuleFactor;
import com.documentum.fc.client.IDfStorageRuleLine;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.DfValue;
import com.documentum.fc.common.IDfValue;

public class DfStorageRuleMatcher {
	// theses attributes are special - they can be set by arguments
	// for the getStorage() call (used for renditions), argument values
	// override those from the sysobject
	public static String FORMAT_ATTR = "format";
	public static String PAGE_MODIFIER_ATTR = "pageModifier";
	
	public static boolean matches(IDfStorageRuleLine pRule,
			IDfSysObject pObj,
			String pFormat,
			String pPageModifier){
		for (int i = 0; i < pRule.getRuleFactorCount(); i++) {
			IDfStorageRuleFactor factor = pRule.getRuleFactor(i);
			// FIXME: currently only AND between factors is supported, inf fact
			// logical operation is not checked
			try { 
				if (! factorMatches(factor, pObj, pFormat, pPageModifier)) {
					return false;
				}
			} catch (DfException ex) {
				// FIXME: check if message is meaningful
				DfLogger.error(BnhpStorageSelectionService.class.getName(), 
						"error evaluating factor {0} in rule {1}", 
						new String[] {factor.toString(), pRule.toString()},null);
				return false;
			}
		}
		return true;
	}
	
	public static IDfValue getAttrValue(String pAttr, 
			IDfSysObject pObj, String pFormat, String pPageModifier) throws DfException {
		if (pAttr.equals(FORMAT_ATTR)) {
			return new DfValue(null == pFormat ? "" : pFormat);
		} else if (pAttr.equals(PAGE_MODIFIER_ATTR)) {
			return new DfValue(null == pPageModifier ? "" : pPageModifier);
		} else if (pObj.hasAttr(pAttr) || pAttr.equals("r_object_id")) {
			// strangely, hasAttr not always work for r_object_id
			return pObj.getValue(pAttr);
		} else {
			throw new DfServiceException("Invalid attribute "+pAttr+ " " +
					"specified for object type "+pObj.getTypeName()+"  in storage rule");
		}
	}
	
	public static IDfValue getCompareValue(IDfStorageRuleFactor pFactor) {
		return new DfValue(pFactor.getValue(), pFactor.getDataType());
	}
	
	
	
	public static boolean compare(String pOp, IDfValue pAttrValue, IDfValue pCompValue) 
	throws DfException{
		if (null == pAttrValue || null==pCompValue) {
			return false;
		} else if ("Equals".equals(pOp)) {
			return pAttrValue.equals(pCompValue);
		} else if ("NotEqual".equals(pOp)) {
			return !pAttrValue.equals(pCompValue);
		} else if ("GreaterThan".equals(pOp)) {
			return pAttrValue.compareTo(pCompValue) > 0;
		} else if ("LessThan".equals(pOp)) {
			return pAttrValue.compareTo(pCompValue) < 0;
		} else if ("GreaterEqual".equals(pOp)) {
			return pAttrValue.compareTo(pCompValue) >= 0;
		} else if ("LessEqual".equals(pOp)) {
			return pAttrValue.compareTo(pCompValue) <= 0;
		} else if ("Matches".equals(pOp)) {
			return pAttrValue.asString().matches(pCompValue.asString());
		} else if ("NotMatches".equals(pOp)) {
			return !pAttrValue.asString().matches(pCompValue.asString());
		} else {
			throw new DfServiceException("invalid relational operator "+pOp+" in BNHP storage rule");
		}
	}
	
	public static boolean factorMatches(IDfStorageRuleFactor pFactor, IDfSysObject pObj, 
			String pFormat, String pPageModifier)  throws DfException {
		String attr = pFactor.getAttrName();
		IDfValue attrValue = getAttrValue(attr, pObj, pFormat, pPageModifier);
		IDfValue compValue = getCompareValue(pFactor);
		return compare(pFactor.getRelationalOp(), attrValue, compValue);
	}
			
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\src\bnhp\infra\base\storage\impl\DfTSTorageChange.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;

import bnhp.infra.base.storage.iface.IBnhpStorageSelectionService;
import bnhp.infra.base.storage.iface.IDfTStorageChange;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfDocument;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

 

public class DfTSTorageChange extends DfDocument implements IDfTStorageChange {
	@Override
	protected synchronized void doSaveEx(boolean arg0, String arg1,
			Object[] arg2) throws DfException {
		applyStoragePolicy();
		super.doSaveEx(arg0, arg1, arg2);
	}
	
	@Override
	protected synchronized void doSave(boolean arg0, String arg1, Object[] arg2)
			throws DfException {
		applyStoragePolicy();
		super.doSave(arg0, arg1, arg2);
	}

	@Override
	protected synchronized IDfId doCheckin(boolean arg0, String arg1,
			String arg2, String arg3, String arg4, String arg5, Object[] arg6)
			throws DfException {
		applyStoragePolicy();
		return super.doCheckin(arg0, arg1, arg2, arg3, arg4, arg5, arg6);
	}
	
	@Override
	public void applyStoragePolicy() throws DfException {
		System.err.println("applyStoragePolicy");
		IDfClient client = DfClient.getLocalClient();
		IBnhpStorageSelectionService service =
			(IBnhpStorageSelectionService)client.newService(
					IBnhpStorageSelectionService.class.getName(),
					getSessionManager());
		if (isDirty() && (!isDeleted()) ) {
			String newStorage = service.getStorageDest(this);
			if (null!=newStorage) {
				System.err.println("new storage dest is " +newStorage);
				String oldStorage = this.getStorageType();
				System.err.println("old storage dest is " +newStorage);
				if (!newStorage.equals(oldStorage)) {
					System.err.println("changing storage dest from "+
							oldStorage+" to "+newStorage);
					this.setStorageType(newStorage);
				}
			}		
		}
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\resources\bnhp\infra\base\storage\testOps.xml
-----------------------------------------------------
<?xml version="1.0" ?>
	<StorageRules>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_GreaterThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>GreaterThan</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_GreaterThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_LessThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>LessThan</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_LessThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_GreaterEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>GreaterEqual</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_GreaterEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_LessEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>LessEqual</RelationalOp>
				<Value>stringvalue</Value>
			</RuleFactor>
			<StorageType>String_LessEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_Matches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^stringvalue$</Value>
			</RuleFactor>
			<StorageType>String_Matches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>String_NotMatches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>NotMatches</RelationalOp>
				<Value>^stringvalue$</Value>
			</RuleFactor>
			<StorageType>String_NotMatches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_GreaterThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>GreaterThan</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_GreaterThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_LessThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>LessThan</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_LessThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_GreaterEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>GreaterEqual</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_GreaterEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_LessEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>LessEqual</RelationalOp>
				<Value>3</Value>
			</RuleFactor>
			<StorageType>Int_LessEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_Matches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^3*$</Value>
			</RuleFactor>
			<StorageType>Int_Matches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Int_NotMatches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>n_test</AttrName>
				<DataType>1</DataType>
				<RelationalOp>NotMatches</RelationalOp>
				<Value>^3*$</Value>
			</RuleFactor>
			<StorageType>Int_NotMatches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_Matches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^.*5.*$</Value>
			</RuleFactor>
			<StorageType>Double_Matches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_NotMatches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>NotMatches</RelationalOp>
				<Value>^.*5.*$</Value>
			</RuleFactor>
			<StorageType>Double_NotMatches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Boolean_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>b_test</AttrName>
				<DataType>0</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>true</Value>
			</RuleFactor>
			<StorageType>Boolean_Equals</StorageType>
		</RuleLine>
		<RuleLine>
		<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Boolean_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>b_test</AttrName>
				<DataType>0</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>true</Value>
			</RuleFactor>
			<StorageType>Boolean_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_GreaterThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>GreaterThan</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_GreaterThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_LessThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>LessThan</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_LessThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_LessEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>LessEqual</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_LessEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Date_GreaterEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>ts_test</AttrName>
				<DataType>4</DataType>
				<RelationalOp>GreaterEqual</RelationalOp>
				<Value>30/05/2008 08:09:00</Value>
			</RuleFactor>
			<StorageType>Date_GreaterEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Id_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>id_test</AttrName>
				<DataType>3</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>0300000a80000299</Value>
			</RuleFactor>
			<StorageType>Id_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Id_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>id_test</AttrName>
				<DataType>3</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>0300000a80000299</Value>
			</RuleFactor>
			<StorageType>Id_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Id_Matches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>id_test</AttrName>
				<DataType>3</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^0300000a.*$</Value>
			</RuleFactor>
			<StorageType>Id_Matches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Id_NotMatches</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>id_test</AttrName>
				<DataType>3</DataType>
				<RelationalOp>NotMatches</RelationalOp>
				<Value>^0300000a.*$</Value>
			</RuleFactor>
			<StorageType>Id_NotMatches</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>2.71</Value>
			</RuleFactor>
			<StorageType>Double_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_NotEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>NotEqual</RelationalOp>
				<Value>2.71</Value>
			</RuleFactor>
			<StorageType>Double_NotEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_GreaterThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>GreaterThan</RelationalOp>
				<Value>2.71</Value>
			</RuleFactor>
			<StorageType>Double_GreaterThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_LessThan</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>LessThan</RelationalOp>
				<Value>2.71</Value>
			</RuleFactor>
			<StorageType>Double_LessThan</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_LessEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>LessEqual</RelationalOp>
				<Value>2.71</Value>
			</RuleFactor>
			<StorageType>Double_LessEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Double_GreaterEqual</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>d_test</AttrName>
				<DataType>5</DataType>
				<RelationalOp>GreaterEqual</RelationalOp>
				<Value>5</Value>
			</RuleFactor>
			<StorageType>Double_GreaterEqual</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>Format_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>format</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>acad</Value>
			</RuleFactor>
			<StorageType>Format_Equals</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>object_name</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pageModifier_Equals</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>pageModifier</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>foo</Value>
			</RuleFactor>
			<StorageType>pageModifier_Equals</StorageType>
		</RuleLine>
	</StorageRules>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\resources\bnhp\infra\base\storage\testRulesCreation.xml
-----------------------------------------------------
<?xml version="1.0" ?>
	<StorageRules>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[tT].*$</Value>
			</RuleFactor>
			<StorageType>Tofes Peula</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[fF].*$</Value>
			</RuleFactor>
			<StorageType>fax project</StorageType>
		</RuleLine>
		<RuleLine>
			<RuleFactor>
				<AttrName>a_content_type</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Equals</RelationalOp>
				<Value>pdf</Value>
			</RuleFactor>
			<LogicOp>AND</LogicOp>
			<RuleFactor>
				<AttrName>title</AttrName>
				<DataType>2</DataType>
				<RelationalOp>Matches</RelationalOp>
				<Value>^[cC].*$</Value>
			</RuleFactor>
			<StorageType>Customer Doc</StorageType>
		</RuleLine>
	</StorageRules>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\resources\dfc.properties
-----------------------------------------------------
dfc.data.dir=C:/Documentum
dfc.registry.mode=windows
dfc.search.ecis.enable=false
dfc.search.ecis.host=
dfc.search.ecis.port=
dfc.tokenstorage.dir=C:/Documentum/apptoken
dfc.tokenstorage.enable=false
dfc.docbroker.host[0]=172.31.55.212	
dfc.docbroker.port[0]=1489
dfc.globalregistry.password=AAAAEC/2hx7DpcHjndR7Dt5iEvPtxvLuQevIyewwAkjxg+M1
dfc.globalregistry.repository=banhap_dev1
dfc.globalregistry.username=dm_bof_registry


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\resources\log4j.properties
-----------------------------------------------------
log4j.rootLogger=INFO,BNHPDFS,A1

log4j.logger.bnhp.infra=DEBUG, A1, F1
log4j.logger.com.emc.documentum.fs.rt = WARN, A1, F1
log4j.logger.com.emc.documentum.fs.datamodel = WARN, A1, F1
log4j.logger.com.emc.documentum.fs.services = WARN, A1, F1
log4j.logger.com.emc.documentum.fs.tools = WARN, A1, F1
log4j.logger.com.emc.documentum.fs.tracing = DEBUG, ASPECT_TRACE

#------------------- CONSOLE --------------------------
log4j.appender.A1=org.apache.log4j.ConsoleAppender
log4j.appender.A1.layout=org.apache.log4j.PatternLayout
log4j.appender.A1.layout.ConversionPattern=%d{ABSOLUTE} %5p [DFS] %c - %m%n

#------------------- FILE --------------------------
log4j.appender.F1=org.apache.log4j.RollingFileAppender
log4j.appender.F1.File=C\:/Documentum/logs/dfs-runtime.log
log4j.appender.F1.MaxFileSize=10MB
log4j.appender.F1.layout=org.apache.log4j.PatternLayout
log4j.appender.F1.layout.ConversionPattern=%d{ABSOLUTE} %5p [DFS] %c - %m%n

#------------------- ASPECT_TRACE --------------------------
log4j.appender.ASPECT_TRACE=org.apache.log4j.RollingFileAppender
log4j.appender.ASPECT_TRACE.File=C\:/Documentum/logs/dfs-runtime-trace.log
log4j.appender.ASPECT_TRACE.MaxFileSize=100MB
log4j.appender.ASPECT_TRACE.layout=org.apache.log4j.PatternLayout
log4j.appender.ASPECT_TRACE.layout.ConversionPattern=%d{ABSOLUTE} [ASPECTS] %m%n


#------------------- BNHP LOG --------------------------
log4j.logger.bnhp.infra.dfs=DEBUG, BNHPDFS, A1

log4j.appender.BNHPDFS=org.apache.log4j.RollingFileAppender
log4j.appender.BNHPDFS.File=./logs/bnhpdfs.log
log4j.appender.BNHPDFS.MaxFileSize=100MB
log4j.appender.BNHPDFS.layout=org.apache.log4j.PatternLayout
log4j.appender.BNHPDFS.layout.ConversionPattern=%d{ABSOLUTE} [BNHP] %m%n


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\src\bnhp\infra\base\storage\impl\BnhpAbstractDfcTest.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;

import org.junit.BeforeClass;

import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfLoginInfo;


/**
 * FIXME: make connection properties configurable using properties
 * 
 * @author FW0
 *
 */
public abstract class BnhpAbstractDfcTest {
	public static String DOCBASE = "banhap_dev1";
	public static String USER = "dctm";
	public static String PASSWORD = "dctm1";
	public boolean isDFCinitialized = false;
	public static IDfSessionManager s_sm = null;
	public static IDfClientX s_clientx = null;
	public static IDfClient s_client = null;
	public static IDfLoginInfo s_identity = null;
	
	@BeforeClass
	public static void setUp() throws DfException {
		if (null == s_clientx) {
			s_clientx = new DfClientX();
		}
		if (null == s_client) {
			s_client = s_clientx.getLocalClient();
		}
		if (null == s_sm) {
			s_sm = s_client.newSessionManager();
		}
		if (null == s_identity) {
			s_identity = s_clientx.getLoginInfo();
			s_identity.setUser(USER);
			s_identity.setPassword(PASSWORD);
			s_sm.setIdentity(DOCBASE, s_identity);
		}
	}
	

	public IDfSession getDocbaseSession() throws DfException {
		return s_sm.getSession(DOCBASE);
	}
	
	public void releaseDocbaseSession(IDfSession pSess) throws DfException {
		s_sm.release(pSess);
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\src\bnhp\infra\base\storage\impl\BnhpStorageSelectorServiceTest.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;


import bnhp.infra.base.storage.iface.IBnhpStorageSelectionService;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfService;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

import org.junit.Assert;
import org.junit.Test;

public class BnhpStorageSelectorServiceTest extends BnhpAbstractDfcTest {
	public static String TEST_OBJECT_TYPE = "t_storage_change";
	 
	public IDfService getStorageSelectionService() throws DfException {
		return s_client.newService(IBnhpStorageSelectionService.class.getName(), s_sm);
	}
	
	@Test
	public void testGetStorageSelectorService() throws DfException {
		IBnhpStorageSelectionService svc = (IBnhpStorageSelectionService)getStorageSelectionService();
		Assert.assertNotNull(svc);
	}
	
	@Test
	public void testSelectStorage() throws DfException {
		IBnhpStorageSelectionService svc = (IBnhpStorageSelectionService)getStorageSelectionService();
		Assert.assertNotNull(svc);
		IDfSession session = null;
		try {
			session = this.getDocbaseSession();
			IDfSysObject obj = (IDfSysObject)session.newObject(TEST_OBJECT_TYPE);
			Assert.assertNotNull(obj);
			obj.setObjectName("TEST_OBJECT_");
			obj.setContentType("pdf");
			
			obj.setTitle("Tutti Frutti");
			String storage = svc.getStorageDest(obj);
			Assert.assertEquals("Tofes Peula", storage);
			
			obj.setTitle("foo");
			storage = svc.getStorageDest(obj);
			Assert.assertEquals("fax project", storage);
			
			
			obj.setTitle("Cool kid");
			storage = svc.getStorageDest(obj);
			Assert.assertEquals("Customer Doc", storage);
			
			obj.setTitle("Lost in action");
			storage = svc.getStorageDest(obj);
			Assert.assertNull(storage);
			
		} finally {
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
		
	}
	
	@Test
	public void testSelectStorageTBO() throws DfException {
		IDfSession session = null;
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			obj = (IDfSysObject)session.newObject(TEST_OBJECT_TYPE);
			Assert.assertNotNull(obj);
			obj.setObjectName("TEST_OBJECT_");
			obj.setContentType("pdf");
			
			obj.setTitle("Tutti Frutti");
			obj.save();
			
			Assert.assertEquals("Tofes Peula", obj.getStorageType());
			
			obj.setTitle("foo");
			obj.save();
			Assert.assertEquals("fax project", obj.getStorageType());
			
			
			obj.setTitle("Cool kid");
			obj.save();
			Assert.assertEquals("Customer Doc", obj.getStorageType());
			
			obj.setTitle("Lost in action");
			obj.save();
			// no change!
			Assert.assertEquals("Customer Doc", obj.getStorageType());
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
		
	}
	
	@Test
	public void testSelectStorageTBOviaDQLCREATE() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;
		IDfId createdId = null;
		try {
			
			session = this.getDocbaseSession();
			IDfQuery query = new DfQuery();
			query.setDQL("CREATE "+TEST_OBJECT_TYPE+" OBJECT "
					+ "set OBJECT_NAME='TEST_OBJECT', set title='Tobo Frobo', set a_content_type='pdf'");
			col = query.execute(session, IDfQuery.DF_EXEC_QUERY);
			while (col.next()) {
				Assert.assertNull(createdId);
				createdId = col.getId("object_created");
				Assert.assertNotNull(createdId);
			}
			
			IDfSysObject obj = (IDfSysObject)session.getObject(createdId);
			Assert.assertNotNull(obj);
			Assert.assertEquals("Tofes Peula", obj.getStorageType());
			
		
			
		} finally {
			if (null!=session && null!=createdId) {
				try {
					session.apiExec("destroy", createdId.getId());
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=col) {
				try { col.close();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
		
	}
	
	/*@Test
	public void testSelectStorageTBOCustomerDoc() throws DfException {
		IDfSession session = null;
		IDfSysObject obj = null;
		try {
			session = this.getDocbaseSession();
			obj = (IDfSysObject)session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(obj);
			obj.setObjectName("TEST_OBJECT_STORAGE");
			obj.setContentType("pdf");
			obj.setInt("project_id",3);
			obj.setTitle("TEST_OBJECT_STORAGE");
			obj.save();
			
			Assert.assertEquals("Tofes Peula", obj.getStorageType());
			
			obj.setInt("project_id",4);
			obj.save();
			Assert.assertEquals("fax project", obj.getStorageType());
			
			
			obj.setInt("project_id",0);
			obj.save();
			Assert.assertEquals("Customer Doc", obj.getStorageType());
			
			obj.setTitle("Lost in action");
			obj.save();
			// no change!
			Assert.assertEquals("Customer Doc", obj.getStorageType());
			
		} finally {
			if (null!=obj && !obj.isNew()) {
				try { obj.destroy();} catch (Exception ex) {
					ex.printStackTrace();
				}
			}
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
		
	}*/
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\tests\src\bnhp\infra\base\storage\impl\BnhpStorageSelectorTests.java
-----------------------------------------------------
package bnhp.infra.base.storage.impl;


import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Date;


import org.junit.Test;
import org.junit.Assert;

import bnhp.infra.base.storage.impl.BnhpStorageSelector;

import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfStorageRules;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.DfTime;


/**
 * 
 * @author FW0
 *
 */
public class BnhpStorageSelectorTests extends BnhpAbstractDfcTest {
	
	public static String RES_FOLDER = "/bnhp/infra/base/storage/";
    public static String TEST_OBJECT_TYPE = "t_storage_change";


	public ByteArrayInputStream getByteArrayInputStream(String resource)  {
		try {
			byte[] buff = new byte[1024];
			InputStream is = this.getClass().getResourceAsStream(resource);
			if (null == is) {
				throw new RuntimeException("can't open '"+resource+"' as resource");
			}
			ByteArrayOutputStream bos = new ByteArrayOutputStream();
			int bytesRead;
			while ((bytesRead = is.read(buff)) > -1) {
				bos.write(buff, 0, bytesRead);
			}
			return new ByteArrayInputStream(bos.toByteArray());
		} catch (IOException ioex) {
			throw new RuntimeException(ioex);
		}
	}
		
	
	
	
	public IDfSysObject getTestObject(IDfSession session) throws DfException {
		return (IDfSysObject)session.newObject(TEST_OBJECT_TYPE);
	}
	
	

	public IDfStorageRules getRulesFromResource(String pRes) throws DfException {
		BnhpStorageSelector selector = new BnhpStorageSelector(DOCBASE, "t_storage_change");
		ByteArrayInputStream is = getByteArrayInputStream(pRes);
		is = selector.prepareRuleConfigStream(is);
		IDfStorageRules rules = selector.parseRules(is);
		return rules;
	}
	
	public BnhpStorageSelector getSelectorFromResource(String pRes) throws DfException {
		BnhpStorageSelector selector = new BnhpStorageSelector(DOCBASE,  TEST_OBJECT_TYPE );
		ByteArrayInputStream is = getByteArrayInputStream(pRes);
		is = selector.prepareRuleConfigStream(is);
		IDfStorageRules rules = selector.parseRules(is);
		selector.setRules(rules);
		selector.m_updated= new Date();
		return selector;
	}
	
	
	@Test 
	public void testConfigObject() throws DfException {
		IDfSession session = null;
		try {
			BnhpStorageSelector selector    = new BnhpStorageSelector(DOCBASE, TEST_OBJECT_TYPE);
			String path = selector.getConfigPath();
			Assert.assertNotNull(path);
			Assert.assertEquals(BnhpStorageSelector.STORAGE_SELECTOR_FOLDER+"/"+
					BnhpStorageSelector.STORAGE_SELECTOR_NAME_PREFIX+TEST_OBJECT_TYPE, path);
			session = this.getDocbaseSession();
			IDfSysObject obj = (IDfSysObject)session.getObjectByPath(path);
			Assert.assertNotNull(obj);
			Assert.assertTrue(obj.getContentSize() > 0);
			ByteArrayInputStream is = obj.getContent();
			Assert.assertNotNull(is);
			is = selector.prepareRuleConfigStream(is);
			
			IDfStorageRules rules = selector.parseRules(is);
			Assert.assertNotNull(rules);
			Assert.assertTrue(rules.getRuleLineCount() > 0);
		} finally {
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}

	@Test
	public void testReadConfig() throws DfException {
		IDfSession session = null;
		try {
			session = this.getDocbaseSession();
			final long interval = 2000;
			BnhpStorageSelector selector    = new BnhpStorageSelector(DOCBASE, TEST_OBJECT_TYPE);
			selector.setPolicyUpdateInterval(interval);
			selector.configure(session);
			Assert.assertNotNull(selector.m_versionId);
			Assert.assertTrue(selector.isUpToDate());

			IDfStorageRules rules = selector.getRules();
			Assert.assertNotNull(rules);
			Assert.assertTrue(rules.getRuleLineCount() > 0);

			try {Thread.sleep(interval+100);} catch (InterruptedException iex) {Thread.currentThread().interrupt();};
			Assert.assertFalse(selector.isUpToDate());

			IDfSysObject obj = (IDfSysObject) session.newObject(TEST_OBJECT_TYPE);
			selector.getStorageDest(obj, null, null);
			Assert.assertTrue(selector.isUpToDate());
		} finally {
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    

	}


	@Test
	public void testReadUnknownConfig() throws DfException {
		IDfSession session = null;
		try {
			session = this.getDocbaseSession();

			BnhpStorageSelector selector    = new BnhpStorageSelector(DOCBASE, TEST_OBJECT_TYPE+"_junk");
			selector.configure(session);
			Assert.assertNull(selector.m_versionId);
			Assert.assertTrue(selector.isUpToDate());

			IDfStorageRules rules = selector.getRules();
			Assert.assertNull(rules);
		} finally {
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    

	}


	@Test
	public void testReadUpdatedConfig() throws DfException {
		IDfSession session = null;
		try {
			session = this.getDocbaseSession();
			final long interval = 4000;

			BnhpStorageSelector selector    = new BnhpStorageSelector(DOCBASE, TEST_OBJECT_TYPE);
			String path = selector.getConfigPath();
			IDfSysObject obj = (IDfSysObject) session.newObject(TEST_OBJECT_TYPE);

			IDfSysObject confObj = (IDfSysObject) session.getObjectByPath(path);
			Assert.assertNotNull(confObj);
			String versionId1 = confObj.getObjectId().getId()+","+confObj.getVStamp(); 

			selector.setPolicyUpdateInterval(interval);
			selector.configure(session);
			Assert.assertNotNull(selector.m_versionId);
			Assert.assertEquals(versionId1, selector.m_versionId);    
			Assert.assertTrue(selector.isUpToDate());

			confObj.save();
			String versionId2 = confObj.getObjectId().getId()+","+confObj.getVStamp();

			selector.getStorageDest(obj, null, null);
			Assert.assertTrue(selector.isUpToDate());
			Assert.assertFalse(versionId2.equals(selector.m_versionId));

			try {Thread.sleep(interval+100);} catch (InterruptedException iex) {Thread.currentThread().interrupt();};
			Assert.assertFalse(selector.isUpToDate());
			selector.getStorageDest(obj, null, null);
			Assert.assertTrue(selector.isUpToDate());
			Assert.assertEquals(versionId2, selector.m_versionId);    
		} finally {
			if (null!=session) {
				releaseDocbaseSession(session);
			}
		}    
	}

	@Test 
	public void testRulesCreationFromFile() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 IDfStorageRules rules = getRulesFromResource(RES_FOLDER+"testRulesCreation.xml");
		 Assert.assertNotNull(rules);
		 Assert.assertTrue(rules.getRuleLineCount() == 3);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	
	
	@Test
	public void testOpEqualsString()  throws DfException { 
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_Equals");
		 obj.setTitle("stringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_Equals", storage);
		 
		 obj.setTitle("stringvalue_");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpEqualsInt()  throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("Int_Equals");
		 obj.setInt("n_test", 3);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("Int_Equals", storage);
		
		 obj.setInt("n_test", 4);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		 obj.setInt("n_test", 0);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpEqualsDate()  throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 
		 final String testName = "Date_Equals";
		 
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:01"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		 obj.setTime("ts_test", new DfTime("30/05/2010 08:09:01"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpEqualsBoolean()  throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Boolean_Equals";
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setBoolean("b_test",true);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setBoolean("b_test", false);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		} finally {
			releaseDocbaseSession(session);
		}

	}
	
	@Test
	public void testOpEqualsId()  throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Id_Equals";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setId("id_test", new DfId("0300000a80000299"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals("Id_Equals", storage);
			
			obj.setId("id_test", new DfId("0300000a80000300"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setId("id_test", new DfId("0000000000000000"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpEqualsDouble()  throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_Equals";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.71);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 2.70);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}


	@Test
	public void testOpNotEqualString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_NotEqual");
		 obj.setTitle("stringvalue_");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_NotEqual", storage);
		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
		
		
	}
	
	@Test
	public void testOpNotEqualInt() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("Int_NotEqual");
		 obj.setInt("n_test", 4);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("Int_NotEqual", storage);
		
		 obj.setInt("n_test", 3);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpNotEqualDate() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 
		 final String testName = "Date_NotEqual";
		 
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:01"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		 

		 obj.setTime("ts_test", new DfTime("30/05/2010 08:09:01"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		} finally {
			releaseDocbaseSession(session);
		}
	
	}
	
	@Test
	public void testOpNotEqualBoolean() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Boolean_NotEqual";
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setBoolean("b_test",false);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setBoolean("b_test", true);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpNotEqualId() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Id_NotEqual";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setId("id_test", new DfId("0300000a80000299"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setId("id_test", new DfId("0300000a80000300"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			
			obj.setId("id_test", new DfId("0000000000000000"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpNotEqualDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_NotEqual";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.71);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setDouble("d_test", 2.70);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);

			obj.setDouble("d_test", 2.72);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);

			
		} finally {
			releaseDocbaseSession(session);
		}
		
	}

	
	@Test
	public void testOpGreaterThanString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_GreaterThan");
		 obj.setTitle("tstringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_GreaterThan", storage);
		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 obj.setTitle("rstringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpGreaterThanInt() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("Int_GreaterThan");
		 obj.setInt("n_test", 4);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("Int_GreaterThan", storage);
		
		 obj.setInt("n_test", 3);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		 obj.setInt("n_test", 2);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpGreaterThanDate() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 
		 final String testName = "Date_GreaterThan";
		 
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:10"));
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		 
		
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 

		 obj.setTime("ts_test", new DfTime("30/05/2008 07:08:59"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	/* these comparisons do not seem to be useful */
	/*
	@Test
	public void testOpGreaterThanBoolean() {
		Assert.assertTrue(false);
	}
	
	@Test
	public void testOpGreaterThanId() {
		Assert.assertTrue(false);
	}
	*/
	
	@Test
	public void testOpGreaterThanDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_GreaterThan";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.72);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 2.71);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			

			obj.setDouble("d_test", 2.70);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);


			
		} finally {
			releaseDocbaseSession(session);
		}
	}
	

	@Test
	public void testOpGreaterEqualString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_GreaterEqual");
		 obj.setTitle("tstringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_GreaterEqual", storage);
		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_GreaterEqual", storage);
		 
		 obj.setTitle("rstringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpGreaterEqualInt() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("Int_GreaterEqual");
		 obj.setInt("n_test", 4);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("Int_GreaterEqual", storage);
		
		 obj.setInt("n_test", 3);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("Int_GreaterEqual", storage);
		 

		 obj.setInt("n_test", 2);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpGreaterEqualDate() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 
		 final String testName = "Date_GreaterEqual";
		 
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:10"));
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		 
		
		 obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);

		 obj.setTime("ts_test", new DfTime("30/05/2008 08:08:59"));
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	
	/* these comparisons do not seem to be useful */
	/*
	@Test
	public void testOpGreaterEqualBoolean() {
		Assert.assertTrue(false);
	}
	
	@Test
	public void testOpGreaterEqualId() {
		Assert.assertTrue(false);
	}	
	 */
	
	@Test
	public void testOpGreaterEqualDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_GreaterEqual";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 5.1);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 5);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 4.9);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	
	
	@Test
	public void testOpLessThanString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_LessThan");
		 obj.setTitle("rstringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_LessThan", storage);
		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 obj.setTitle("tstringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpLessThanInt() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Int_LessThan"; 
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setInt("n_test", 2);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setInt("n_test", 3);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 

		 obj.setInt("n_test", 4);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpLessThanDate() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);

			final String testName = "Date_LessThan";

			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setTime("ts_test", new DfTime("30/05/2008 08:09:10"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);

			obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);


			obj.setTime("ts_test", new DfTime("30/05/2008 08:08:59"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
		} finally {
			releaseDocbaseSession(session);
		}

	}
	
	
	/* these comparisons do not seem to be useful */
	/*
	@Test
	public void testOpLessThanBoolean() {
		Assert.assertTrue(false);
	}
	
	@Test
	public void testOpLessThanId() {
		Assert.assertTrue(false);
	}*/
	
	@Test
	public void testOpLessThanDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_LessThan";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.72);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setDouble("d_test", 2.71);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setDouble("d_test", 2.70);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
		} finally {
			releaseDocbaseSession(session);
		}

	}
	

	@Test
	public void testOpLessEqualString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_LessEqual");
		 obj.setTitle("rstringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_LessEqual", storage);
		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_LessEqual", storage);
		 
		 obj.setTitle("tstringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpLessEqualInt() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Int_LessEqual"; 
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setInt("n_test", 2);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setInt("n_test", 3);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);

		 obj.setInt("n_test", 4);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpLessEqualDate() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);

			final String testName = "Date_LessEqual";

			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setTime("ts_test", new DfTime("30/05/2008 08:09:10"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);

			obj.setTime("ts_test", new DfTime("30/05/2008 08:09:00"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setTime("ts_test", new DfTime("30/05/2008 08:08:59"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}

	/* these comparisons do not seem to be useful */
	/*
	@Test
	public void testOpLessEqualBoolean() {
		Assert.assertTrue(false);
	}
	
	@Test
	public void testOpLessEqualId() {
		Assert.assertTrue(false);
	}	
	 */
	
	
	@Test
	public void testOpLessEqualDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_LessEqual";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.72);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setDouble("d_test", 2.71);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 2.70);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpMatchesString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_Matches");
		 obj.setTitle("stringvalue");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_Matches", storage);
		 
		 obj.setTitle("stringvaluea");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 
		 obj.setTitle("rstringvalu");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpMatchesInt() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Int_Matches"; 
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setInt("n_test", 3);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		
		 obj.setInt("n_test", 33);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);

		 obj.setInt("n_test", 333);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);

		 obj.setInt("n_test", 343);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	/* these comparisons do not seem to be useful */
	/*
	@Test
	public void testOpMatchesDate() {
		Assert.assertTrue(false);
	}

	@Test
	public void testOpMatchesBoolean() {
		Assert.assertTrue(false);
	}
	 */
	
	
	@Test
	public void testOpMatchesId() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Id_Matches";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setId("id_test", new DfId("0300000a80000299"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setId("id_test", new DfId("0300000a80000300"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
						
			obj.setId("id_test", new DfId("0000000000000000"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpMatchesDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_Matches";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.5);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			obj.setDouble("d_test", 3.5);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			
			obj.setDouble("d_test", 2.6);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
		} finally {
			releaseDocbaseSession(session);
		}
	}	

	
	@Test
	public void testOpNotMatchesString() throws DfException {
		IDfSession session = getDocbaseSession();
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName("String_NotMatches");
		 obj.setTitle("stringvaluea");
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_NotMatches", storage);

		 obj.setTitle("stringvalu");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals("String_NotMatches", storage);

		 
		 obj.setTitle("stringvalue");
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		 
		} finally {
			releaseDocbaseSession(session);
		}
	}

	
	@Test
	public void testOpNotMatchesInt() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Int_NotMatches"; 
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);
		 obj.setInt("n_test", 3);
		 String storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);
		
		 obj.setInt("n_test", 33);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		 obj.setInt("n_test", 333);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertNull(storage);

		 obj.setInt("n_test", 343);
		 storage = selector.getStorageDest(obj, null, null);
		 Assert.assertEquals(testName, storage);
		 
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	/* these comparisons do not seem to be useful */
	/*
	
	@Test
	public void testOpNotMatchesDate() {
		Assert.assertTrue(false);
	}
	
	@Test
	public void testOpNotMatchesBoolean() {
		Assert.assertTrue(false);
	}
	*/
	
	@Test
	public void testOpNotMatchesId() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Id_NotMatches";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setId("id_test", new DfId("0300000a80000299"));
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setId("id_test", new DfId("0300000a80000300"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
						
			obj.setId("id_test", new DfId("0000000000000000"));
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
		} finally {
			releaseDocbaseSession(session);
		}
		
	}
	
	@Test
	public void testOpNotMatchesDouble() throws DfException {
		IDfSession session = getDocbaseSession();
		final String testName = "Double_NotMatches";
		try {
			BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
			Assert.assertNotNull(selector);
			IDfSysObject obj = this.getTestObject(session);
			obj.setObjectName(testName);
			obj.setDouble("d_test", 2.5);
			String storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			obj.setDouble("d_test", 3.5);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertNull(storage);
			
			
			
			obj.setDouble("d_test", 2.6);
			storage = selector.getStorageDest(obj, null, null);
			Assert.assertEquals(testName, storage);
			
			
		} finally {
			releaseDocbaseSession(session);
		}
	}	
	
	@Test
	public void testOpEqualsFormat()  throws DfException { 
		IDfSession session = getDocbaseSession();
		final String testName = "Format_Equals";
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);

		 String storage = selector.getStorageDest(obj, "acad", null);
		 Assert.assertEquals(testName, storage);
		 
		 storage = selector.getStorageDest(obj, "pdf", null);
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
	
	@Test
	public void testOpEqualsPageModifier()  throws DfException { 
		IDfSession session = getDocbaseSession();
		final String testName = "pageModifier_Equals";
		try {
		 BnhpStorageSelector selector = getSelectorFromResource(RES_FOLDER+"testOps.xml");
		 Assert.assertNotNull(selector);
		 IDfSysObject obj = this.getTestObject(session);
		 obj.setObjectName(testName);

		 String storage = selector.getStorageDest(obj, null, "foo");
		 Assert.assertEquals(testName, storage);
		 
		 storage = selector.getStorageDest(obj, null, "bar");
		 Assert.assertNull(storage);
		} finally {
			releaseDocbaseSession(session);
		}
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\bnhpstoragepolicy\version.properties
-----------------------------------------------------
#Wed Apr 15 14:33:38 GMT+02:00 2015
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\bof_package.xml
-----------------------------------------------------
<project name="bof_package" default="build">
	<!--This scripts rebuild jar files of BOF and copies them into 
		proper place for composer to see updated files. It's 
		supposed to be run from within eclipse ant builder
		
		For DAR file to be properly built this script must be envoked 
		BEFORE building of dar, that is ant builder must be envoked before 
		Documentum Builder in the eclipse build configuration
    -->

	
	<!-- this defines task for building/copiying individual jar -->
	 	<macrodef name="build_jar_artifact">
	 		<!-- filename of jar to be built -->
	 		<attribute name="name"/>
	 		<!-- files to put into jar (used by jar task) -->
	 		<attribute name="classfilter"/>
	 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
	 		<attribute name="jardef"/>
	 		<sequential>
	 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
				<echo message="** rebuilding ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
				<echo message="   class filter: @{classfilter}"/>
				<!--  delete jar file in project root -->
				<delete verbose="false" file="@{name}" failonerror="false"/> 
				 
				<!--  make jar file in project root -->			 
				<jar destfile="@{name}" 
					basedir="target/classes" includes="@{classfilter}">
				</jar>
	 			<!--  copy jar to proper location (content/...) that is known to composer -->
	 				 			<!-- force tries to override read-only protection (those files must not be stored in VCS) --> 
	 							<move file="@{name}" 
	 								tofile="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"
	 								overwrite="true" 
	 								force="true" 
	 								failonerror="true"
	 				 				verbose="true"/>
	 		</sequential>
	 	</macrodef>
	 		
		<!-- this defines task for deleting individual jar -->
		<macrodef name="clean_jar_artifact">
		 		<!-- filename of jar to be built -->
		 		<attribute name="name"/>
		 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
		 		<attribute name="jardef"/>
		 		<sequential>
		 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
					<echo message="** cleaning ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
								<!--  delete jar file in project root -->
					<delete file="@{name}" failonerror="true"/>
		 			<delete verbose="true" file="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}" failonerror="true"/> 
		 		</sequential>
		 	</macrodef>
 	
 		
 
	<target name="clean">
			<clean_jar_artifact 
				name="tfs_peula_doc_iface.jar" 
				jardef="Artifacts/JAR Definitions/tfs_peula_doc_iface.jardef"/>
			<clean_jar_artifact 
				name="tfs_peula_doc_impl.jar" 
				jardef="Artifacts/JAR Definitions/tfs_peula_doc_impl.jardef"/>
			<clean_jar_artifact 
				name="tfs_peula_erua_iface.jar" 
				jardef="Artifacts/JAR Definitions/tfs_peula_erua_iface.jardef"/>
			<clean_jar_artifact 
				name="tfs_peula_erua_impl.jar" 
				jardef="Artifacts/JAR Definitions/tfs_peula_erua_impl.jardef"/>
			<!--eclipse.refreshLocal resource="/" depth="infinite"/-->
	</target>
	
	
	<target name="build">
		<build_jar_artifact 
			name="tfs_peula_doc_iface.jar" 
			classfilter="com/documentum/fc/client/tfspeula/tbo/ITfsPeulaDoc*"
			jardef="Artifacts/JAR Definitions/tfs_peula_doc_iface.jardef"/>
		<build_jar_artifact 
			name="tfs_peula_doc_impl.jar" 
			classfilter="com/documentum/fc/client/tfspeula/tbo/TfsPeulaDoc.class"
			jardef="Artifacts/JAR Definitions/tfs_peula_doc_impl.jardef"/>
		<build_jar_artifact 
			name="tfs_peula_erua_iface.jar" 
			classfilter="com/documentum/fc/client/tfspeula/tbo/ITfsPeulaErua*"
			jardef="Artifacts/JAR Definitions/tfs_peula_erua_iface.jardef"/>
		<build_jar_artifact 
			name="tfs_peula_erua_impl.jar" 
			classfilter="com/documentum/fc/client/tfspeula/tbo/TfsPeulaErua.class"
			jardef="Artifacts/JAR Definitions/tfs_peula_erua_impl.jardef"/>
		<!--eclipse.refreshLocal resource="/" depth="infinite"/-->
	</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\migration\MigrationTest.java
-----------------------------------------------------
package bnhp.infra.base.migration;

import java.io.IOException;
import java.text.DateFormat;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.junit.Assert;
import org.junit.Test;

import bnhp.infra.base.tbo.AbstractLegacyTest;
import bnhp.infra.dfs.exceptions.DFSDocumentumException;
import bnhp.infra.dfs.legacy_support.CreateObjectServicesWrapper;
import bnhp.infra.dfs.legacy_support.ILegacyConstants;
import bnhp.infra.dfs.legacy_support.LegacyDataInitializer;
import bnhp.infra.dfs.legacy_support.LegacyWrapperConstants;
import bnhp.infra.dfs.model.business.BankAccount;
import bnhp.infra.dfs.model.business.DocData;
import bnhp.infra.dfs.model.business.DocEventData;
import bnhp.infra.dfs.model.business.DocFile;
import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.model.service.DocDataForCreate;
import bnhp.infra.dfs.model.service.DocIdData;
import bnhp.infra.dfs.services.DataInitializer;
import bnhp.infra.dfs.utils.dfc.DfcUtils;

import com.documentum.fc.client.DfIdNotFoundException;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.customerdoc.tbo.ICustomerDoc;
import com.documentum.fc.client.tfspeula.tbo.ITfsPeulaDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.IDfId;
import com.emc.documentum.fs.rt.ServiceException;

public class MigrationTest extends AbstractLegacyTest implements ILegacyConstants{

	public final int SCANCODE_WAIT = 1;
	
	private String TEST_MIGRATION_DOC_OBJECT_NAME = "test_migration_doc" + todayDate;
	
	private String peulaQuery = "select r_object_id from bnhp_tfs_peula_doc where object_name='" + TEST_MIGRATION_DOC_OBJECT_NAME + "'";
	
	private String customerQuery = "select r_object_id from bnhp_customer_doc where object_name='" + TEST_MIGRATION_DOC_OBJECT_NAME + "'";
	
	private static String todayDate = getDate(); 
	
	private void setGlobalTfsPeulaSyncDisabled(boolean value) {
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(value));
	}
	
	private static String getDate() {
		DateFormat dateFormat = new SimpleDateFormat("dd/MM/yyyy");
		Calendar cal = Calendar.getInstance();
		return "_" + (dateFormat.format(cal.getTime())); //2014/08/06 16:00:22
	}

	//@Test
	public void testOneDocMigrated() throws DfException {
		
		testDocsMigrated(1, "10", "5", 0, 0);
	}
	
	//@Test
	public void test3DocMigrated() throws DfException {
		
		testDocsMigrated(3, "10", "5", 0, 0);
	}
	
	@Test
	public void test20DocMigrated() throws DfException {
		
		testDocsMigrated(20, "10", "5", 0, 0);
	}
	
	//@Test
	public void test100DocMigrated() throws DfException {
		
		testDocsMigrated(100, "10", "10", 0, 0);
	}
	
	//@Test
	public void test1000DocMigrated() throws DfException {
		
		testDocsMigrated(1000, "10", "5", 0, 0);
	}
	
	//@Test
	public void test1000DocMigrated1() throws DfException {
		
		testDocsMigrated(1000, "10", "100", 0, 0);
	}
	
	@Test
	public void testMissingMandatoryArgumentsDql() throws DfException {
		
		testMissingArguments(10, "10", "10", 0);
	}
	
	@Test
	public void testMissingMandatoryArgumentsDocbase() throws DfException {
		
		testMissingArguments(10, "10", "10", 2);
	}
	
	@Test
	public void testMissingMandatoryArgumentsUser() throws DfException {
		
		testMissingArguments(10, "10", "10", 4);
	}
	
	@Test
	public void testMissingMandatoryArgumentsPwd() throws DfException {
		
		testMissingArguments(10, "10", "10", 6);
	}
	
	//@Test
	public void testDeleteCreatedDocsHearaEvent3() throws DfException {
		
		testDeleteCreatedDocs(3, "1", "1", 1, 0);
	}
	
	//@Test
	public void testDeleteCreatedDocsScanEvent3() throws DfException {
		
		testDeleteCreatedDocs(3, "1", "1", 0, 0);
	}
	
	@Test
	public void testDeleteCreatedDocsHearaEvent10() throws DfException {
		
		testDeleteCreatedDocs(10, "1", "1", 1, 0);
	}
	
	@Test
	public void testDeleteCreatedDocsScanEvent10() throws DfException {
		
		testDeleteCreatedDocs(10, "1", "1", 0, 0);
	}
	
	@Test
	public void testDeleteCreatedDocsHarigEvent10() throws DfException {
		
		testDeleteCreatedDocs(10, "1", "1", 0, 1);
	}
	
   // @Test
	public void testDeleteCreatedDocsHearaEvent100() throws DfException {
		
		testDeleteCreatedDocs(100, "10", "10", 1, 2);
	}
	
	//@Test
	public void testDeleteCreatedDocsScanEvent100() throws DfException {
		
		testDeleteCreatedDocs(100, "10", "10", 0, 0);
	}
	
	//@Test
	public void testDeleteCreatedDocsHarigEvent100() throws DfException {
		
		testDeleteCreatedDocs(100, "10", "10", 2, 1);
	}
	
	//@Test
	public void testDeleteCreatedDocsHearaEvent1000() throws DfException {
		
		testDeleteCreatedDocs(1000, "10", "10", 0, 1);
	}
	
	//@Test
	public void testDeleteCreatedDocsScanEvent1000() throws DfException {
		
		testDeleteCreatedDocs(1000, "10", "10", 0, 0);
	}
	
	//@Test
	public void testDeleteCreatedDocsHarigEvent1000() throws DfException {
		
		testDeleteCreatedDocs(1000, "10", "10", 0, 1);
	}
	
	private void testDeleteCreatedDocs(int num, String max_threads, String queueSize, int hearot, int harigim) throws DfException {
		List<IDfId> objList = null;
		
		if(!isEmptyTable()){
			Assert.fail();
		}
		try {
			SecurityContext sctx = this.getLegacyTestSecurityContext();
			
			IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
			System.out.println("docbase is "+sctx.getRepositoryName());
			IDfSession session = sm.newSession(sctx.getRepositoryName());
			
			System.out.println("Creating " + num + " docs");
			this.setGlobalTfsPeulaSyncDisabled(true);
			objList = createDocs(sctx, session, true, num, hearot, harigim);
						
			for(IDfId obj : objList){
				assertObjectCreated(obj); 
			}
			
			System.out.println("Created " + objList.size() + " docs");
			String[] args = createArguments(max_threads, queueSize, -1, false);
			this.setGlobalTfsPeulaSyncDisabled(false);
			Migration.resetAllArgs();
			Migration.run(args);
			
			Thread.sleep(1000);
			for(IDfId obj : objList){
				String legacyId = getDocKey(obj.getId());
				IDfSysObject custDoc = getCustomerDocByLegacyId(legacyId);
				Assert.assertNotNull(custDoc);
				assertStatus(legacyId);
			}
			
			Assert.assertTrue(objList.size() == Migration.getTotalMigrated() + Migration.getTotalFailed());
			
			//getting events objId for test
			List<String> eventsList = getEvents(objList, session);
			//Assert.assertSame(eventsList.size(), (hearot + harigim) * objList.size() );
			args = createArguments(max_threads, queueSize, -1, true);
			Migration.resetAllArgs();
			Migration.resetCounters();
			Migration.run(args);
			
			Thread.sleep(1000);
			for(IDfId obj : objList){
				String legacyId = getDocKey(obj.getId());
				IDfSysObject custDoc = getCustomerDocByLegacyId(legacyId);
				Assert.assertNull(custDoc);
				assertEmptyStatus(legacyId);
			}
			assertNoEventsLeft(eventsList, session);
			Assert.assertTrue(objList.size() == Migration.getTotalMigrated() + Migration.getTotalFailed());
			
		} catch (Exception e) {
			e.printStackTrace();
			Assert.fail();
		
		} finally{
			System.out.println("Deleting " + num + " docs");
			deleteCreatedDocs(objList);
			Migration.resetCounters();
		}
	}

	private void testMissingArguments(int num, String max_threads, String queueSize, int i) throws DfException {
		List<IDfId> objList = null;
		
		if(!isEmptyTable()){
			Assert.fail();
		}
		try {
			SecurityContext sctx = this.getLegacyTestSecurityContext();
			
			IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
			System.out.println("docbase is "+sctx.getRepositoryName());
			IDfSession session = sm.newSession(sctx.getRepositoryName());
			
			System.out.println("Creating " + num + " docs");
			objList = createDocs(sctx, session, true, num, 0, 0);
			this.setGlobalTfsPeulaSyncDisabled(false);
			
			for(IDfId obj : objList){
				assertObjectCreated(obj); 
			}
			
			System.out.println("Created " + objList.size() + " docs");
			String[] args = createArguments(max_threads, queueSize, i, false);
			Migration.resetAllArgs();
			Migration.run(args);
			
			Thread.sleep(10000);
			for(IDfId obj : objList){
				String legacyId = getDocKey(obj.getId());
				IDfSysObject custDoc = getCustomerDocByLegacyId(legacyId);
				Assert.assertNull(custDoc);
			}
		} catch (Exception e) {
			e.printStackTrace();
			Assert.fail();
		
		} finally{
			System.out.println("Deleting " + num + " docs");
			deleteCreatedDocs(objList);
			Migration.resetCounters();
		}
	}

	public void testDocsMigrated(int num, String max_threads, String queueSize, int hearot, int harigim) throws DfException {
		List<IDfId> objList = null;
		
		if(!isEmptyTable()){
			Assert.fail();
		}
		try {
			SecurityContext sctx = this.getLegacyTestSecurityContext();
			
			IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
			System.out.println("docbase is "+sctx.getRepositoryName());
			IDfSession session = sm.newSession(sctx.getRepositoryName());
			
			System.out.println("Creating " + num + " docs");
			this.setGlobalTfsPeulaSyncDisabled(true);
			objList = createDocs(sctx, session, true, num, hearot, harigim);
			//creating docs without sync
						
			for(IDfId obj : objList){
				assertObjectCreated(obj); 
			}
			
			System.out.println("Created " + objList.size() + " docs");
			String[] args = createArguments(max_threads, queueSize, -1, false);
			Migration.resetAllArgs();
			//now sync with new system
			this.setGlobalTfsPeulaSyncDisabled(false);
			Migration.run(args);
			
			Thread.sleep(1000);
			for(IDfId obj : objList){
				String legacyId = getDocKey(obj.getId());
				IDfSysObject custDoc = getCustomerDocByLegacyId(legacyId);
				Assert.assertNotNull(custDoc);
				assertStatus(legacyId);
			}
			
			Assert.assertTrue(objList.size() == Migration.getTotalMigrated() + Migration.getTotalFailed());
			
		} catch (Exception e) {
			e.printStackTrace();
			Assert.fail();
		
		} finally{
			System.out.println("Deleting " + num + " docs");
			deleteCreatedDocs(objList);
			Migration.resetCounters();
		}
	}
	
	private String[] createArguments(String max_threads, String queueSize, int ind, boolean isRollback) {
		
		String[] args = new String[8];
		args[0] = "-dql";
		args[1] = peulaQuery;
		args[2] = "-docbase";
		args[3] = "banhap_dev1";
		args[4] = "-user";
		args[5] = "dctm";
		args[6] = "-pwd";
		args[7] = "dctm1";
		
		List<String> list = new ArrayList<String>();
		for(int i = 0; i < args.length; i++){
			if(i == ind){
				i++;
			}
			else{
				list.add(args[i]);
			}
		}
		
		if(max_threads != null && max_threads.length() != 0){
			list.add("-t");
			list.add(max_threads);
		}
		
		if(queueSize != null && queueSize.length() != 0){
			list.add("-q");
			list.add(queueSize);
		}
		
		if(isRollback){
			list.add("-rollback");
		}
		String[] arr =  list.toArray(new String[list.size()]);
		return arr;
	}
	
	private boolean isEmptyTable() throws DfException { 
		
		IDfSession session = null;
		boolean res = false;
		try {
			Session.setUp();
			session = Session.getDocbaseSession();
			
			String str = "delete bnhp_tfs_peula_doc object where object_name='" + TEST_MIGRATION_DOC_OBJECT_NAME +"'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);
			query.execute(session, 0);
			
			str = "select count(*) as num from  bnhp_tfs_peula_doc where object_name='" + TEST_MIGRATION_DOC_OBJECT_NAME +"'";
			query.setDQL(str);
			
			IDfCollection col = query.execute(session, 0);
			if(col.next()){
				if("0".equals(col.getString("num"))){
					res = true;
				}
			}
			
		} catch (DfIdNotFoundException ex) {
			ex.printStackTrace();
		}
		
		finally{
			Session.releaseDocbaseSession(session);
		}
		return res;
	}
	
	private void deleteCreatedDocs(List<IDfId> objList) throws DfException {
		
		IDfSession session = null;
		
		System.out.println("Deleting created docs");
		try {
			Session.setUp();
			session = Session.getDocbaseSession();
			String dql = null;
			
			dql = "delete bnhp_tfs_peula_doc object where r_object_id='";
			
			for(IDfId id : objList){
				String str = dql + id.getId() +"'";
				IDfQuery query = new DfQuery();
				query.setDQL(str);
				query.execute(session, 0);
			}
		} catch (DfIdNotFoundException ex) {
			ex.printStackTrace();
		}
		
		finally{
			Session.releaseDocbaseSession(session);
		}
	}

	private void assertStatus(String legacyId) throws DfException {
		
		IDfSession session = null;
		String str = null;
		try{
			Session.setUp();
			session = Session.getDocbaseSession();
			
			str = "select r_object_id, a_status from bnhp_customer_doc (all) where legacy_document_id='" + legacyId + "'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);
			IDfCollection col = query.execute(session, 0);
			
			HashMap<String, String> custMap = new HashMap<String, String>();
			
			String status1 = "", status2 = "";
			String objId1 = "", objId2 = "";
			
			while(col != null && col.next()){
				custMap.put(col.getString("r_object_id"), col.getString("a_status"));
			}
			
			HashMap<String, String> peulaMap = new HashMap<String, String>();
			
			str = "select r_object_id, a_status from bnhp_tfs_peula_doc (all) where doc_key='" + legacyId + "'";
			query.setDQL(str);
			
			col = query.execute(session, 0);
			while(col != null && col.next()){
				peulaMap.put(col.getString("r_object_id"), col.getString("a_status"));
			}
			
			Assert.assertTrue(custMap.size() == peulaMap.size());
			
			for (Map.Entry<String, String> entry :custMap.entrySet()) {
			    objId1 = entry.getKey();
			    status1 = entry.getValue();
			    
			    status2 = peulaMap.get(status1);
			    if(status2 == null){
			    	Assert.fail();
			    }else{
			    	Assert.assertTrue(status2.equals(objId1));
			    }
			}
			
			
		}
		finally{
			Session.releaseDocbaseSession(session);
		}
	}

	private void assertEmptyStatus(String legacyId) throws DfException {
		
		IDfSession session = null;
		String str = null;
		try{
			Session.setUp();
			session = Session.getDocbaseSession();
			
			IDfQuery query = new DfQuery();
			IDfCollection col = null;
			
			str = "select a_status from bnhp_tfs_peula_doc (all) where doc_key='" + legacyId + "'";
			
			query.setDQL(str);
			
			col = query.execute(session, 0);
			while(col != null && col.next()){
				Assert.assertEquals(col.getString("a_status").length(),0);
			}		
		}
		finally{
			Session.releaseDocbaseSession(session);
		}
	}

	private void assertNoEventsLeft(List<String> eventsList, IDfSession session) throws DfException {
		
		for(String id : eventsList){
			IDfSysObject sysObj = (IDfSysObject)session.getObjectByQualification("bnhp_doc_event where r_object_id='" + id +  "'");
			Assert.assertNull(sysObj);
		}
	}

	private List<String> getEvents(List<IDfId> objList, IDfSession session) throws DfException {
		List<String> list = new ArrayList<String>();
		for(IDfId objId : objList){
			IDfSysObject sysObj = (IDfSysObject)session.getObject(objId);
			list.addAll(getEventsById(sysObj.getString("a_status")));
		}
		return list;
	}

	private List<IDfId> createDocs(SecurityContext sctx, IDfSession session, boolean isDisabled , int num, int hearot, int harigim) throws Exception  {
		
		List<IDfId> list = new ArrayList<IDfId>();
		this.setGlobalTfsPeulaSyncDisabled(isDisabled);
		
		for(int i = 0; i < num; i++){
			try {
				list.add(createObjectServicesTestCreateValidDoc(session, true, SCANCODE_WAIT, hearot, harigim));
				
			}catch (Exception e) {
					e.printStackTrace();
					Assert.fail();
				}
			} 
		
		return list;
	}
	
	protected List<String> getEventsById(String id) throws DfException {
		IDfSession session = null;
		List<String> list = new ArrayList<String>();
		try {
			Session.setUp();
			session = Session.getDocbaseSession();
			IDfSysObject custObj = (IDfSysObject) session.getObject(new DfId(id));
			if(custObj == null){
				return null;
			}
			String dql = "select ev.r_object_id  as r_object_id from bnhp_doc_event ev, dm_relation rel where " +
					"rel.parent_id ='" + custObj.getChronicleId().getId() + "' and rel.child_id=ev.r_object_id";
			IDfQuery query = new DfQuery();
			query.setDQL(dql);
			IDfCollection col = query.execute(session, 0);
				
			while(col.next()){
				list.add(col.getString("r_object_id"));
			}
			
			return list;
		} catch (DfIdNotFoundException nfex) {
			return null;
		}
		
		finally{
			Session.releaseDocbaseSession(session);
		}
	}
	
	protected IDfSysObject getCustomerDocByLegacyId(String legacyId) throws DfException {
		IDfSession session = null;
		
		try {
			Session.setUp();
			session = Session.getDocbaseSession();
			return (IDfSysObject) session.getObjectByQualification("bnhp_customer_doc where legacy_document_id='"+legacyId+"'");
		} catch (DfIdNotFoundException nfex) {
			return null;
		}
		
		finally{
			Session.releaseDocbaseSession(session);
		}
	}
	
	private String getDocKey(String docId) throws DfException {
		
		IDfSession session = null;
		String str = null, objId = null;
		try{
			Session.setUp();
			session = Session.getDocbaseSession();
			
			str = "select doc_key from bnhp_tfs_peula_doc where r_object_id='" + docId + "'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);
		
			IDfCollection col = query.execute(session, 0);
			
			if(col != null && col.next()){
				objId = col.getString("doc_key");	
			}
			
			return objId;
		}
		finally{
			Session.releaseDocbaseSession(session);
		}
	}
	
	private void assertObjectCreated(IDfId objId) throws DfException {
		
		IDfSession session = null;
		
		try{
			Session.setUp();
			session = Session.getDocbaseSession();
			
			ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(objId);
			Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
			String legacyId = srcDoc.getString("doc_key");
			Assert.assertTrue(legacyId.length()>0);
		}
		finally{
			Session.releaseDocbaseSession(session);
		}
	}
	
	public IDfId createObjectServicesTestCreateValidDoc(IDfSession session, boolean primaryContent,	int scanStatus, int hearot, int harigim) throws IOException, DFSDocumentumException, ServiceException{
		CreateObjectServicesWrapper wrapper = new CreateObjectServicesWrapper(); 
		
		SecurityContext securityContext = getLegacyTestSecurityContext();
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();

		DocDataForCreate doc = new DocDataForCreate();
		DocData docData = DataInitializer.createMinimalValidDocData();
		 
		if (primaryContent ) {
			DocFile docFile = DataInitializer.createDocFile("/HELLO.pdf", "pdf");
			docData.setDocFile(docFile);
		}
		
		docData.getDocCustomerData().setDocDetails(
				DataInitializer.createFullDocDetails());
		docData.getDocCustomerData().getDocDetails().setProjectId(
				LEGACY_PROJECT);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());
		// mapper will set valid sub area code from form id
		// docData.getDocCustomerData().getDocDetails().setBusinessSubAreaCode(
		// null);

		// those two are checked for validity by legacy app
		// this one is not mandatory in new service
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(scanStatus);
		docData.getDocCustomerData().getDocDetails().setObjectName(TEST_MIGRATION_DOC_OBJECT_NAME);
		// mandatory in the legacy service
		docData.getDocCustomerData().getExecutorDetails().setExecutingBankId(1);
		docData.getDocCustomerData().getExecutorDetails().setExecutingBranchId(
				1);
		docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
		docData.getDocCustomerData().getExecutorDetails()
				.setEmpIdDocumentTypeCode(1);
		docData.getDocCustomerData().getExecutorDetails().setTerminalChannelId(
				0);
		docData.getDocCustomerData().getExecutorDetails()
				.setExecutingEmpFullName("Ilan benAlon");

		// this one is mandatory in new service and is checked
		// for validity in the old one
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		// this one is mandatory in new service and is checked for validity in
		// the old
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);

		docData.getDocCustomerData().setBankAccounts(
				DataInitializer.createBankAccountList());

		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		bankAccountList.add(DataInitializer.createBankAccount1());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);
		doc.setDocData(docData);
		
		if (hearot > 0 || harigim > 0) {
				List<DocEventData> docEventData = new ArrayList<DocEventData>();
				doc.setDocEventData(docEventData);
				for (int i = 0; i < harigim; i++) {
					if (i > 0) {
						sleep(1001);
					}
					docEventData.add(LegacyDataInitializer.makeHarigCompatEvent());
				}
				for (int i = 0; i < hearot; i++) {
					if (i > 0) {
						sleep(1001);
					}
					docEventData.add(LegacyDataInitializer.makeHearaCompatEvent());
				}
		}
				
		docDataForCreateList.add(doc);

		List<DocIdData> ids = wrapper.createDocuments(
				docDataForCreateList, securityContext, "CURRENT");
		IDfId  result = null;
		for (DocIdData id : ids) {
			Assert.assertNotNull(id);
			Assert.assertNotNull(id.getDctmDocumentId());
			Assert.assertEquals(16,id.getDctmDocumentId().length());
			Assert.assertNotNull(id.getVersionLabel());
			result = new DfId (id.getDctmDocumentId());
		}
	
		return result;
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\tbo\AbstractLegacyTest.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import org.junit.BeforeClass;

import bnhp.infra.dfs.model.utils.AbstractDfsTest;

import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfLoginInfo;

public abstract class AbstractLegacyTest extends AbstractDfsTest {

	protected static class Session {
		public static String DOCBASE = "banhap_dev1";
		public static String USER = "dctm";
		public static String PASSWORD = "dctm1";
		public boolean isDFCinitialized = false;
		public static IDfSessionManager s_sm = null;
		public static IDfClientX s_clientx = null;
		public static IDfClient s_client = null;
		public static IDfLoginInfo s_identity = null;

		@BeforeClass
		public static void setUp() throws DfException {
			if (null == s_clientx) {
				s_clientx = new DfClientX();
			}
			if (null == s_client) {
				s_client = s_clientx.getLocalClient();
			}
			if (null == s_sm) {
				s_sm = s_client.newSessionManager();
			}
			if (null == s_identity) {
				s_identity = s_clientx.getLoginInfo();
				s_identity.setUser(USER);
				s_identity.setPassword(PASSWORD);
				s_sm.setIdentity(DOCBASE, s_identity);
			}
		}

		public static IDfSession getDocbaseSession() throws DfException {
			return s_sm.getSession(DOCBASE);
		}

		public static void releaseDocbaseSession(IDfSession pSess)
				throws DfException {
			s_sm.release(pSess);
		}
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\tbo\CustomerDocTest.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;

import org.junit.Assert;
import org.junit.Test;

import bnhp.infra.dfs.exceptions.DFSDocumentumException;
import bnhp.infra.dfs.exceptions.DFSValidationException;
import bnhp.infra.dfs.legacy_support.LegacyWrapperConstants;
import bnhp.infra.dfs.legacy_support.RetrieveObjectServicesWrapper;
import bnhp.infra.dfs.model.business.BankAccount;
import bnhp.infra.dfs.model.business.Custom;
import bnhp.infra.dfs.model.business.CustomerKey;
import bnhp.infra.dfs.model.business.DocData;
import bnhp.infra.dfs.model.business.DocEventData;
import bnhp.infra.dfs.model.business.DocFile;
import bnhp.infra.dfs.model.business.DocPropertyExtension;
import bnhp.infra.dfs.model.business.PropertyKeyValue;
import bnhp.infra.dfs.model.business.SecondaryDocFile;
import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.model.service.DocDataForCreate;
import bnhp.infra.dfs.model.service.DocDataForRetrieve;
import bnhp.infra.dfs.model.service.DocIdData;
import bnhp.infra.dfs.model.service.DocRetrievalFlags;
import bnhp.infra.dfs.model.service.FetchTypeSet;
import bnhp.infra.dfs.model.service.Full;
import bnhp.infra.dfs.population.business.BusinessDataConversionUtils;
import bnhp.infra.dfs.services.CreateObjectServices;
import bnhp.infra.dfs.services.DataInitializer;
import bnhp.infra.dfs.utils.content.ContentURIProviderFactory;
import bnhp.infra.dfs.utils.content.FileBasedUploadUtils;
import bnhp.infra.dfs.utils.content.IContentURIProvider;
import bnhp.infra.dfs.utils.content.IRenditionProvider;
import bnhp.infra.dfs.utils.content.RenditionProviderFactory;
import bnhp.infra.dfs.utils.dfc.DfcUtils;
import bnhp.infra.dfs.utils.service.PermissionTableUtils;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.customerdoc.tbo.ICustomerDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.IDfId;
import com.documentum.fc.common.IDfTime;
import com.emc.documentum.fs.rt.ServiceException;
import com.emc.documentum.fs.rt.context.IServiceContext;

public class CustomerDocTest extends AbstractLegacyTest {

	private static final Integer EVENT_CODE_HARIG = 201;
	private static final Integer EVENT_CODE_HEARA = 203;
	private static final Integer EVENT_CODE_SCAN = 301;

	private IServiceContext m_serviceContext = null;
	private SecurityContext m_securityContext = null;

	@Test
	public void testDocCreatedNoSecondaryFile() throws DfException {
		testDocCreated(false, false, 0);
	}

	@Test
	public void testDocCreatedNoSecondaryFileChangeFormat() throws DfException,
			IOException {
		String legacyId = testDocCreated(false, false, 0);
		Session.setUp();
		IDfSession session = Session.getDocbaseSession();

		ICustomerDoc srcDoc = (ICustomerDoc) session
				.getObjectByQualification("bnhp_customer_doc where legacy_document_id='"
						+ legacyId + "'");
		Assert.assertNotNull(srcDoc);

		ByteArrayOutputStream bytestr = new ByteArrayOutputStream();
		bytestr.write("TEXT".getBytes());
		bytestr.flush();
		srcDoc.setContentEx(bytestr, "text", 0);
		srcDoc.save();
	}

	@Test
	public void testDocCreatedNoSecondaryFileFileUpload() throws DfException {
		testDocCreated(false, false, 0, true);
	}

	@Test
	public void testDocCreatedWithSecondaryFile() throws DfException {
		testDocCreated(true, false, 0);
	}

	@Test
	public void testDocCreatedWithEventHarig() throws DfException {
		testDocCreated(false, true, EVENT_CODE_HARIG);
	}

	@Test
	public void testDocCreatedWithEventHeara() throws DfException {
		testDocCreated(false, true, EVENT_CODE_HEARA);
	}

	@Test
	public void testDocCreatedWithEventScan() throws DfException {
		testDocCreated(false, true, EVENT_CODE_SCAN);
	}

	@Test
	public void testDocCreatedNotSyncWithTwoVersions() throws DfException {
		testDocCreatedWithVersions(true, 2, 0);
	}

	@Test
	public void testDocCreatedNotSyncWithThreeVersions() throws DfException {
		testDocCreatedWithVersions(true, 3, 0);
	}

	@Test
	public void testDocCreatedWithTwoVersionsNotSyncWithThreeVersions()
			throws DfException {
		testDocCreatedWithVersions(true, 3, 2);
	}

	@Test
	public void testDocCreatedWithThreeVersionsNotSyncWithThreeVersions()
			throws DfException {
		testDocCreatedWithVersions(true, 3, 3);
	}

	@Test
	public void testDocCreatedWithFourVersionsNotSyncWithThreeVersions()
			throws DfException {
		testDocCreatedWithVersions(true, 3, 4);
	}

	@Test
	public void testDocCreatedWithTwoVersions() throws DfException {
		testDocCreatedWithVersions(false, 2, 0);
	}

	@Test
	public void testDocCreatedWithThreeVersions() throws DfException {
		testDocCreatedWithVersions(false, 3, 0);
	}

	@Test
	public void testDocCreatedWithFourVersions() throws DfException {
		testDocCreatedWithVersions(false, 4, 0);
	}

	@Test
	public void testDocCreatedWithFiveVersions() throws DfException {
		testDocCreatedWithVersions(false, 5, 0);
	}

	// @Test
	public void testGlobalSwitch() throws DfException, ServiceException {
		SecurityContext sctx = this.getTestSecurityContext();
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx
				.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());

		ISyncObject doc = (ISyncObject) session.newObject("bnhp_customer_doc");

		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP,
				"false");
		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP,
				"true");
		Assert.assertTrue(doc.isSyncGloballyDisabled());
		System.clearProperty(ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP);
		Assert.assertFalse(doc.isSyncGloballyDisabled());
	}

	// @Test
	public void testDisableGlobalProperty() throws DfException {

		CreateObjectServices createObjectServices = new CreateObjectServices();
		RetrieveObjectServicesWrapper wrapper = null;
		try {

			String propVal = System.setProperty(
					ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP, "true");
			// create the document
			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();
			DocData docData = getCreatedDocData(false, false, false,
					m_securityContext);

			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			String legacyDocumentId = docDataForCreate.getDocData()
					.getDocCustomerData().getDocDetails().getLegacyDocumentId();

			// create document with dfs
			List<DocIdData> idList = createObjectServices.createDocumentsDfs(
					docDataForCreateList, this.getBothTestSecurityContext(),
					"my version");
			System.out.println("**" + idList + "**");

			m_securityContext = getLegacyTestSecurityContext();
			m_serviceContext = getTestServiceContext(m_securityContext);

			// set up content URI provider
			IContentURIProvider uriProvider = ContentURIProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());
			IRenditionProvider renditionProvider = RenditionProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());

			wrapper = new RetrieveObjectServicesWrapper();
			wrapper.setRenditionProvider(renditionProvider);
			wrapper.setUriProvider(uriProvider);

			Full fetchType = new Full();
			FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
			fileFetchTypeSet.setFetchType(fetchType);

			DocRetrievalFlags flags = new DocRetrievalFlags();
			flags.setShouldRetrieveDocEventData(true);
			flags.setShouldRetrieveSecondaryFile(false);
			DocDataForRetrieve retrievedData = wrapper
					.retrieveDoc4LegacyDocumentId(legacyDocumentId, null,
							m_securityContext, fileFetchTypeSet, flags);

			setIgnoringValues(docDataForCreate, retrievedData);
			assertSameDocumentData(docDataForCreate, retrievedData);
			Assert.fail();
		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (DFSValidationException e) {
			// TODO Auto-generated catch block
			// e.printStackTrace();
			Assert.assertTrue(e.getMessage().contains("docId not exist"));
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}
	}

	public String testDocCreated(boolean isSecondaryFile, boolean isEvent,
			Integer eventCode) throws DfException {
		return testDocCreated(isSecondaryFile, isEvent, eventCode, false);
	}

	public String testDocCreated(boolean isSecondaryFile, boolean isEvent,
			Integer eventCode, boolean fileUpload) throws DfException {
		CreateObjectServices createObjectServices = new CreateObjectServices();
		RetrieveObjectServicesWrapper wrapper = null;
		String legacyDocumentId = null;
		try {

			// create the document
			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();
			DocData docData = getCreatedDocData(fileUpload, true,
					isSecondaryFile, this.getBothTestSecurityContext());

			if (isEvent) {
				// event
				docDataForCreate.setDocEventData(DataInitializer
						.createFullDocEventData());
				docDataForCreate.getDocEventData().get(0)
						.setConcatenatedEventId("10");
				docDataForCreate.getDocEventData().get(0).setEventCategoryCode(
						eventCode);
			}

			docDataForCreate.setDocData(docData);
			// docData.getDocCustomerData().getDocDetails().
			// setConcatenatedEventIds(new ArrayList<String>());
			docDataForCreateList.add(docDataForCreate);

			legacyDocumentId = docDataForCreate.getDocData()
					.getDocCustomerData().getDocDetails().getLegacyDocumentId();

			// create document with dfs
			DocFile saveDocFile = docData.getDocFile();
			DocFile saveSecDocFile = (null == docData.getSecondaryDocFile()) ? null
					: docData.getSecondaryDocFile().getDocFile();
			List<DocIdData> idList = createObjectServices.createDocumentsDfs(
					docDataForCreateList, this.getBothTestSecurityContext(),
					"my version");
			docData.setDocFile(saveDocFile);
			if (null != docData.getSecondaryDocFile()) {
				docData.getSecondaryDocFile().setDocFile(saveSecDocFile);
			}
			System.out.println("**" + idList + "**");

			m_securityContext = getLegacyTestSecurityContext();
			m_serviceContext = getTestServiceContext(m_securityContext);

			// set up content URI provider
			IContentURIProvider uriProvider = ContentURIProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());
			IRenditionProvider renditionProvider = RenditionProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());

			wrapper = new RetrieveObjectServicesWrapper();
			wrapper.setRenditionProvider(renditionProvider);
			wrapper.setUriProvider(uriProvider);

			Full fetchType = new Full();
			FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
			fileFetchTypeSet.setFetchType(fetchType);

			DocRetrievalFlags flags = new DocRetrievalFlags();
			flags.setShouldRetrieveDocEventData(false);
			flags.setShouldRetrieveSecondaryFile(isSecondaryFile);

			if (isEvent) {

				if (eventCode.equals(EVENT_CODE_SCAN)) {
					compareEventDetails(legacyDocumentId, docDataForCreate);

				} else {
					flags.setShouldRetrieveDocEventData(true);
					DocDataForRetrieve retrievedData = wrapper
							.retrieveDoc4LegacyDocumentId(legacyDocumentId,
									null, m_securityContext, fileFetchTypeSet,
									flags);
					setIgnoringValuesForEvent(docDataForCreate, retrievedData);
					assertSameDocumentData(docDataForCreate, retrievedData);
				}
			} else {

				DocDataForRetrieve retrievedData = wrapper
						.retrieveDoc4LegacyDocumentId(legacyDocumentId, null,
								m_securityContext, fileFetchTypeSet, flags);

				setIgnoringValues(docDataForCreate, retrievedData);
				assertSameDocumentData(docDataForCreate, retrievedData);
				assertStatus(legacyDocumentId);
			}

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}
		return legacyDocumentId;
	}

	public void testDocCreatedWithVersions(boolean isDisabled, int numVersions,
			int numVersionsNotSync) throws DfException {
		SecurityContext bothSecContext = this.getBothTestSecurityContext();
		CreateObjectServices createObjectServices = new CreateObjectServices();
		RetrieveObjectServicesWrapper wrapper = null;
		try {

			// create the document
			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();
			DocData docData = getCreatedDocData(isDisabled, true, false,
					bothSecContext);

			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			String legacyDocumentId = docDataForCreate.getDocData()
					.getDocCustomerData().getDocDetails().getLegacyDocumentId();

			if (isDisabled) {
				System
						.setProperty(
								ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP,
								"true");
			} else {
				System.setProperty(
						ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP,
						"false");
			}

			// create document with dfs
			List<DocIdData> idList = createObjectServices.createDocumentsDfs(
					docDataForCreateList, bothSecContext, "my version");

			Session.setUp();
			IDfSession session = Session.getDocbaseSession();
			String createdID = idList.get(0).getDctmDocumentId();

			if (createdID == null) {
				Assert.fail();
			}
			ICustomerDoc srcDoc = (ICustomerDoc) session.getObject(new DfId(
					getObjectId(createdID)));
			Assert
					.assertEquals("bnhp_customer_doc", srcDoc.getType()
							.getName());

			if (isDisabled) {
				Assert.assertTrue(srcDoc.getStatus().length() == 0);

			} else {
				Assert.assertTrue(srcDoc.getStatus().length() != 0);
				assertStatus(legacyDocumentId);
			}
			if (isDisabled) {
				int j = 0;
				for (j = 1; j < numVersionsNotSync; j++) {
					srcDoc.checkout();
					srcDoc
							.setObjectName(srcDoc.getObjectName() + "_"
									+ (j + 1));
					IDfId newSrcId = srcDoc.checkin(false, "CURRENT, VERSION_"
							+ (j + 1));
					srcDoc = (ICustomerDoc) session.getObject(newSrcId);
				}
			}

			m_securityContext = getLegacyTestSecurityContext();
			m_serviceContext = getTestServiceContext(m_securityContext);

			// set up content URI provider
			IContentURIProvider uriProvider = ContentURIProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());
			IRenditionProvider renditionProvider = RenditionProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());

			wrapper = new RetrieveObjectServicesWrapper();
			wrapper.setRenditionProvider(renditionProvider);
			wrapper.setUriProvider(uriProvider);

			Full fetchType = new Full();
			// Meta fetchType = new Meta();
			FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
			fileFetchTypeSet.setFetchType(fetchType);

			DocRetrievalFlags flags = new DocRetrievalFlags();
			flags.setShouldRetrieveDocEventData(false);
			flags.setShouldRetrieveSecondaryFile(false);

			if (!isDisabled) {
				DocDataForRetrieve retrievedData = wrapper
						.retrieveDoc4LegacyDocumentId(legacyDocumentId, null,
								m_securityContext, fileFetchTypeSet, flags);

				setIgnoringValues(docDataForCreate, retrievedData);
				assertSameDocumentData(docDataForCreate, retrievedData);
				assertStatus(legacyDocumentId);
			}
			if (isDisabled) {
				System.setProperty(
						ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP,
						"false");
			}
			int i = 0;
			for (i = 1; i < numVersions; i++) {
				srcDoc.checkout();
				String status = srcDoc.getString("a_status");
				srcDoc.setObjectName(srcDoc.getObjectName() + "_" + (i + 1));
				IDfId newSrcId = srcDoc.checkin(false, "CURRENT, VERSION_"
						+ (i + 1));
				srcDoc = (ICustomerDoc) session.getObject(newSrcId);
			}

			System.out.println("Retrieving doc with " + numVersions
					+ " versions with dfc");

			assertStatus(legacyDocumentId);
			assertVersions(legacyDocumentId);

			// System.out.println("Retrieving doc with three versions");
			// Meta fetchTypeMeta = new Meta();
			// fileFetchTypeSet.setFetchType(fetchTypeMeta);
			//			
			// SecurityContext ctx = new SecurityContext();
			// ctx.setUserName("dctm");
			// ctx.setPassword("dctm");
			// ctx.setRepositoryName(m_props.getProperty(TEST_DOCBASE));
			//			
			// m_securityContext = ctx;
			// retrievedData =
			// wrapper.retrieveDoc4LegacyDocumentId(legacyDocumentId,
			// null, m_securityContext, fileFetchTypeSet, flags );
			//			
			// setIgnoringValues(docDataForCreate, retrievedData);
			// retrievedData.getDocData().getDocCustomerData().getDocDetails().
			// setObjectName
			// (docDataForCreate.getDocData().getDocCustomerData().getDocDetails
			// ().getObjectName());
			// assertSameDocumentData(docDataForCreate, retrievedData);
			// assertStatus(legacyDocumentId);

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}
	}

	private void assertVersions(String legacyDocumentId) throws DfException {
		IDfSession session = null;
		String str = null;
		// String version1 = "", version2 = "";
		ArrayList<String> version1 = null;
		ArrayList<String> version2 = null;
		String status1;

		try {

			Session.setUp();
			session = Session.getDocbaseSession();

			HashMap<String, ArrayList<String>> custMap = new HashMap<String, ArrayList<String>>();
			HashMap<String, ArrayList<String>> peulaMap = new HashMap<String, ArrayList<String>>();

			str = "select r_object_id, r_version_label, a_status from bnhp_tfs_peula_doc (all) where doc_key='"
					+ legacyDocumentId + "'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);

			IDfCollection col = query.execute(session, 0);

			while (col != null && col.next()) {
				int k = col.getValueCount("r_version_label");
				StringBuffer version = new StringBuffer("");
				ArrayList<String> list = new ArrayList<String>(1);
				for (int i = 0; i < k; i++) {
					// version.append(
					// col.getRepeatingString("r_version_label",i)).append(",");
					list.add(col.getRepeatingString("r_version_label", i));
				}
				peulaMap.put(col.getString("a_status"), list);
			}

			str = "select r_object_id, r_version_label, a_status from bnhp_customer_doc (all) where legacy_document_id='"
					+ legacyDocumentId + "'";
			query = new DfQuery();
			query.setDQL(str);
			col = query.execute(session, 0);

			while (col != null && col.next()) {
				int k = col.getValueCount("r_version_label");
				StringBuffer version = new StringBuffer("");
				ArrayList<String> list = new ArrayList<String>(1);
				for (int i = 0; i < k; i++) {
					// version.append(
					// col.getRepeatingString("r_version_label",i)).append(",");
					list.add(col.getRepeatingString("r_version_label", i));
				}
				custMap.put(col.getString("r_object_id"), list);
			}

			Assert.assertTrue(custMap.size() == peulaMap.size());
			for (Entry<String, ArrayList<String>> entry : custMap.entrySet()) {
				if (entry.getValue() == null) {
					Assert.fail();
				}
				version1 = entry.getValue();
				status1 = entry.getKey();

				if (peulaMap.get(status1) == null) {
					Assert.fail();
				} else {
					version2 = peulaMap.get(status1);
					Assert.assertTrue(version2.size() == version1.size());
					for (String val : version2) {
						Assert.assertTrue(version1.contains(val));
					}
				}
			}

		} finally {
			Session.releaseDocbaseSession(session);
		}
	}

	public void docCreateWithEvent(Integer eventCode) throws DfException {

		CreateObjectServices createObjectServices = new CreateObjectServices();
		RetrieveObjectServicesWrapper wrapper = null;
		try {
			SecurityContext secContext = this.getBothTestSecurityContext();
			// create the document
			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocData docData = getCreatedDocData(false, true, false, secContext);
			DocDataForCreate docDataForCreate = new DocDataForCreate();

			// event
			docDataForCreate.setDocEventData(DataInitializer
					.createFullDocEventData());
			docDataForCreate.getDocEventData().get(0).setConcatenatedEventId(
					"10");
			docDataForCreate.getDocEventData().get(0).setEventCategoryCode(
					eventCode);

			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			String legacyDocumentId = docDataForCreate.getDocData()
					.getDocCustomerData().getDocDetails().getLegacyDocumentId();

			// create document with dfs
			List<DocIdData> idList = createObjectServices.createDocumentsDfs(
					docDataForCreateList, secContext, "my version");
			System.out.println("**" + idList + "**");

			m_securityContext = getLegacyTestSecurityContext();
			m_serviceContext = getTestServiceContext(m_securityContext);

			// set up content URI provider
			IContentURIProvider uriProvider = ContentURIProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());
			IRenditionProvider renditionProvider = RenditionProviderFactory
					.getProvider(m_serviceContext, m_securityContext
							.getRepositoryName(), m_securityContext
							.getUserName());

			wrapper = new RetrieveObjectServicesWrapper();
			wrapper.setRenditionProvider(renditionProvider);
			wrapper.setUriProvider(uriProvider);

			Full fetchType = new Full();
			FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
			fileFetchTypeSet.setFetchType(fetchType);

			DocRetrievalFlags flags = new DocRetrievalFlags();
			flags.setShouldRetrieveSecondaryFile(false);

			if (eventCode.equals(EVENT_CODE_SCAN)) {
				compareEventDetails(legacyDocumentId, docDataForCreate);

			} else {
				flags.setShouldRetrieveDocEventData(true);
				DocDataForRetrieve retrievedData = wrapper
						.retrieveDoc4LegacyDocumentId(legacyDocumentId, null,
								m_securityContext, fileFetchTypeSet, flags);
				setIgnoringValuesForEvent(docDataForCreate, retrievedData);
				assertSameDocumentData(docDataForCreate, retrievedData);
			}

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();

		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}
	}

	private DocData getCreatedDocData(boolean fileUpload, boolean primary,
			boolean secondary, SecurityContext secCont) throws IOException,
			DFSDocumentumException, ServiceException {
		DocData docData = DataInitializer.createMinimalValidDocData();

		docData.getDocCustomerData().getDocDetails().setProjectId(1);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());

		if (primary) {
			DocFile docFile = null;
			if (fileUpload) {
				docFile = new DocFile();
				PermissionTableUtils permUtils = new PermissionTableUtils();
				permUtils.setServiceContext(getTestServiceContext(secCont));
				permUtils.setSecContext(secCont);

				FileBasedUploadUtils utils = new FileBasedUploadUtils(this
						.getTestServiceContext(secCont), secCont, permUtils);

				String uploadPath = getTestUploadPath(permUtils, utils, docData);

				String testFileAbsPath = copyResourceForUpload(utils,
						DataInitializer.SMALL_FILE_PATH, uploadPath);
				File testFile = new File(testFileAbsPath);
				Assert.assertTrue(testFile.exists() && testFile.isFile()
						&& testFile.length() > 0);

				docFile.setDocFormat("pdf");
				docFile.setDocURL(makeUploadUrl(utils, testFile));
				docFile.setDocSize(testFile.length());
				System.out
						.println("primary upload url: " + docFile.getDocURL());
			} else {
				docFile = DataInitializer.createDocFile(
						DataInitializer.SMALL_FILE_PATH, "pdf");
			}
			docData.setDocFile(docFile);
		}
		if (secondary) {
			DocFile docFile = null;
			SecondaryDocFile secDocFile = new SecondaryDocFile();
			secDocFile.setTemplateSourceCode(0);
			if (fileUpload) {
				docFile = new DocFile();
				PermissionTableUtils permUtils = new PermissionTableUtils();
				permUtils.setServiceContext(getTestServiceContext(secCont));
				permUtils.setSecContext(secCont);

				FileBasedUploadUtils utils = new FileBasedUploadUtils(this
						.getTestServiceContext(secCont), secCont, permUtils);

				String uploadPath = getTestUploadPath(permUtils, utils, docData);

				String testFileAbsPath = copyResourceForUpload(utils,
						DataInitializer.TINY_FILE_PATH, uploadPath);
				File testFile = new File(testFileAbsPath);
				Assert.assertTrue(testFile.exists() && testFile.isFile()
						&& testFile.length() > 0);
				docFile.setDocFormat("pb");
				docFile.setDocURL(makeUploadUrl(utils, testFile));
				docFile.setDocSize(testFile.length());
				System.out
						.println("primary upload url: " + docFile.getDocURL());
			} else {
				docFile = DataInitializer.createDocFile(
						DataInitializer.TINY_FILE_PATH, "pb");
			}
			secDocFile.setDocFile(docFile);
			docData.setSecondaryDocFile(secDocFile);
		}

		setDocDetails(docData);

		// Executor details
		docData.getDocCustomerData().getExecutorDetails().setExecutingBankId(1);
		docData.getDocCustomerData().getExecutorDetails().setExecutingBranchId(
				1);
		docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
		docData.getDocCustomerData().getExecutorDetails()
				.setEmpIdDocumentTypeCode(1);
		docData.getDocCustomerData().getExecutorDetails().setTerminalChannelId(
				0);
		docData.getDocCustomerData().getExecutorDetails()
				.setExecutingEmpFullName("Diana");

		docData.getDocCustomerData().setBankAccounts(
				DataInitializer.createBankAccountList());

		docData.getDocCustomerData().setPensionFund(
				DataInitializer.createPensionFund());
		List<String> lst = new ArrayList<String>();
		docData.getDocCustomerData().getDocDetails().setConcatenatedEventIds(
				lst);

		// customer details
		CustomerKey custKey = DataInitializer.createFullCustomer();
		List<CustomerKey> customerKeyList = new ArrayList<CustomerKey>();
		customerKeyList.add(custKey);
		docData.getDocCustomerData().setCustomerKeys(customerKeyList);

		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		bankAccountList.add(DataInitializer.createFullBankAccount());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);

		// custom text
		Custom custom = new Custom();
		custom.setCustomText("kod_cheshbon_mushee=0");
		docData.getDocCustomerData().setCustom(custom);

		return docData;
	}

	private void setIgnoringValuesForEvent(DocDataForCreate docDataForCreate,
			DocDataForRetrieve retrievedData) {
		setIgnoringValues(docDataForCreate, retrievedData);
		if (retrievedData.getDocEventsDataList().size() > 0) {
			retrievedData.getDocEventsDataList().get(0).setChannelId(
					docDataForCreate.getDocEventData().get(0).getChannelId());
			retrievedData.getDocEventsDataList().get(0)
					.setEmpIdDocumentTypeCode(
							docDataForCreate.getDocEventData().get(0)
									.getEmpIdDocumentTypeCode());
			retrievedData.getDocEventsDataList().get(0)
					.setEventCategoryErrorCode(
							docDataForCreate.getDocEventData().get(0)
									.getEventCategoryErrorCode());
			retrievedData.getDocEventsDataList().get(0)
					.setEventCategoryStatusCode(
							docDataForCreate.getDocEventData().get(0)
									.getEventCategoryStatusCode());
			retrievedData.getDocEventsDataList().get(0)
					.setEventCategoryTypeCode(
							docDataForCreate.getDocEventData().get(0)
									.getEventCategoryTypeCode());
			retrievedData.getDocEventsDataList().get(0)
					.setEventGroupId(
							docDataForCreate.getDocEventData().get(0)
									.getEventGroupId());
			retrievedData.getDocEventsDataList().get(0).setExecutingBranchId(
					docDataForCreate.getDocEventData().get(0)
							.getExecutingBranchId());
			retrievedData.getDocEventsDataList().get(0).setIpAddress(
					docDataForCreate.getDocEventData().get(0).getIpAddress());
			retrievedData.getDocEventsDataList().get(0).setObjectName(
					docDataForCreate.getDocEventData().get(0).getObjectName());

		}
		docDataForCreate.getDocEventData().get(0).setConcatenatedEventId("10");
	}

	private void setDocDetails(DocData docData) {
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
		docData.getDocCustomerData().getDocDetails().setCurrencyCode(1);
		docData.getDocCustomerData().getDocDetails().setTransactionAmt(2.0);
		docData.getDocCustomerData().getDocDetails().setChannelId(1);
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);
		docData.getDocCustomerData().getDocDetails().setDocumentEditionNbr(1);
		docData.getDocCustomerData().getDocDetails().setTemplateDataExistsInd(
				true);
		docData.getDocCustomerData().getDocDetails().setSystemCode(1);
	}

	private void compareEventDetails(String legacyId,
			DocDataForCreate docDataForCreate) throws DfException {

		IDfSession session = null;
		String str = null;
		IDfQuery query = new DfQuery();
		IDfCollection col = null;
		Integer scanStatus;
		IDfTime scanDate;
		String pakidSorek;

		try {
			Session.setUp();
			session = Session.getDocbaseSession();

			str = "select scan_status, scan_date, mpr_zihuy_pakid_sorek from bnhp_tfs_peula_doc where doc_key='"
					+ legacyId + "'";
			query.setDQL(str);

			col = query.execute(session, 0);

			if (col != null && col.next()) {
				scanStatus = col.getInt("scan_status");
				scanDate = col.getTime("scan_date");
				pakidSorek = col.getString("mpr_zihuy_pakid_sorek");

				Assert.assertTrue(scanStatus.equals(docDataForCreate
						.getDocData().getDocCustomerData().getDocDetails()
						.getScanStatusCode()));
				SimpleDateFormat format = new SimpleDateFormat(
						"HH:mm:ss dd/MM/yyyy");

				Date parsed = null;
				try {
					parsed = format.parse(scanDate.toString());

				} catch (Exception e) {
					Assert.fail();
					return;
				}

				Assert.assertTrue(parsed.equals(docDataForCreate.getDocData()
						.getDocCustomerData().getDocDetails()
						.getLegacyDocumentEntryDttm()));
				Assert.assertTrue(pakidSorek.equals(docDataForCreate
						.getDocEventData().get(0).getExecutingEmpIdCode()));
			} else {
				Assert.fail();
			}

		} finally {
			Session.releaseDocbaseSession(session);
		}
	}

	private void assertStatus(String legacyId) throws DfException {

		IDfSession session = null;
		String str = null;
		try {
			Session.setUp();
			session = Session.getDocbaseSession();

			str = "select r_object_id, a_status from bnhp_customer_doc (all) where legacy_document_id='"
					+ legacyId + "'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);
			IDfCollection col = query.execute(session, 0);

			HashMap<String, String> custMap = new HashMap<String, String>();

			String status1 = "", status2 = "";
			String objId1 = "", objId2 = "";

			while (col != null && col.next()) {
				custMap.put(col.getString("r_object_id"), col
						.getString("a_status"));
			}

			HashMap<String, String> peulaMap = new HashMap<String, String>();

			str = "select r_object_id, a_status from bnhp_tfs_peula_doc (all) where doc_key='"
					+ legacyId + "'";
			query.setDQL(str);

			col = query.execute(session, 0);
			while (col != null && col.next()) {
				peulaMap.put(col.getString("r_object_id"), col
						.getString("a_status"));
			}

			Assert.assertTrue(custMap.size() == peulaMap.size());

			for (Map.Entry<String, String> entry : custMap.entrySet()) {
				objId1 = entry.getKey();
				status1 = entry.getValue();

				status2 = peulaMap.get(status1);
				if (status2 == null) {
					Assert.fail();
				} else {
					Assert.assertTrue(status2.equals(objId1));
				}
			}

		} finally {
			Session.releaseDocbaseSession(session);
		}
	}

	private String getObjectId(String docId) throws DfException {

		IDfSession session = null;
		String str = null, objId = null;
		try {
			Session.setUp();
			session = Session.getDocbaseSession();

			str = "select r_object_id from bnhp_customer_doc where dctm_document_id='"
					+ docId + "'";
			IDfQuery query = new DfQuery();
			query.setDQL(str);

			IDfCollection col = query.execute(session, 0);

			if (col != null && col.next()) {
				objId = col.getString("r_object_id");
			}

			return objId;
		} finally {
			Session.releaseDocbaseSession(session);
		}
	}

	private void setIgnoringValues(DocDataForCreate docDataForCreate,
			DocDataForRetrieve retreivedData) {

		retreivedData.getDocData().getDocCustomerData().getExecutorDetails()
				.setInstructionReceiveTypeCode(
						docDataForCreate.getDocData().getDocCustomerData()
								.getExecutorDetails()
								.getInstructionReceiveTypeCode());
		retreivedData.getDocData().getDocCustomerData().getCustomerKeys()
				.get(0).setOccasionalCustomerInd(
						docDataForCreate.getDocData().getDocCustomerData()
								.getCustomerKeys().get(0)
								.getOccasionalCustomerInd());
		retreivedData.getDocData().getDocCustomerData().getDocDetails()
				.setBusinessProcessId(
						docDataForCreate.getDocData().getDocCustomerData()
								.getDocDetails().getBusinessProcessId());
		retreivedData.getDocData().getDocCustomerData().getDocDetails()
				.setSignatureStatusCode(
						docDataForCreate.getDocData().getDocCustomerData()
								.getDocDetails().getSignatureStatusCode());
	}

	protected List<DocDataForCreate> makeDocDataForCreateListFull()
			throws IOException {
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
		DocDataForCreate docDataForCreate = new DocDataForCreate();
		DocData docData = DataInitializer.createValidFullDocData();
		docDataForCreate.setDocData(docData);
		docDataForCreate.setDocEventData(DataInitializer
				.createFullDocEventData());
		docDataForCreateList.add(docDataForCreate);
		return docDataForCreateList;
	}

	protected void assertSameDocumentData(DocDataForCreate doc,
			DocDataForRetrieve docDataForRetrieve) {
		if (docDataForRetrieve.getDocData().getDocPropertyExtensions() != null) {
			List<DocPropertyExtension> retrPropExts = docDataForRetrieve
					.getDocData().getDocPropertyExtensions();
			List<DocPropertyExtension> creatPropExts = doc.getDocData()
					.getDocPropertyExtensions();
			Assert.assertNotNull(creatPropExts);

			for (int i = 0; i < creatPropExts.size(); i++) {
				DocPropertyExtension retrPropExt = retrPropExts.get(i);
				DocPropertyExtension creatPropExt = creatPropExts.get(i);
				List<PropertyKeyValue> retrKeyValues = retrPropExt
						.getPropertyKeyValues();
				List<PropertyKeyValue> creatKeyValues = creatPropExt
						.getPropertyKeyValues();
				for (PropertyKeyValue val : creatKeyValues) {
					if (LegacyWrapperConstants.BOX_DOC_SERIAL_NBR_EXT_ATTR
							.equals(val.getKey())) {
						retrKeyValues.add(val);
					}
				}
			}
		}

		BusinessDataConversionUtils
				.convertNullBooleansToFalse(doc.getDocData());
		Assert.assertEquals(doc.getDocData().getDocCustomerData(),
				docDataForRetrieve.getDocData().getDocCustomerData());

		// File ...
		if (docDataForRetrieve.getDocData().getDocFile() != null) {
			System.out.println("File exist - "
					+ docDataForRetrieve.getDoesFileContentExist());
			Assert
					.assertEquals(doc.getDocData().getDocFile().getDocFormat(),
							docDataForRetrieve.getDocData().getDocFile()
									.getDocFormat());
			System.out
					.println("created file: " + doc.getDocData().getDocFile());
			System.out.println("retrieved file: "
					+ docDataForRetrieve.getDocData().getDocFile());
			if (null != doc.getDocData().getDocFile().getDocStream()
					&& null != docDataForRetrieve.getDocData().getDocFile()
							.getDocStream()) {
				Assert.assertArrayEquals(doc.getDocData().getDocFile()
						.getDocStream(), docDataForRetrieve.getDocData()
						.getDocFile().getDocStream());
			} else if (null != docDataForRetrieve.getDocData().getDocFile()
					.getDocStream()) {
				System.out.println("checking only doc size");
				Assert.assertEquals(doc.getDocData().getDocFile().getDocSize().intValue(),
						docDataForRetrieve.getDocData().getDocFile()
								.getDocStream().length);
			} else {
				System.out.println("checking NOT IMPLEMENTED");
			}
			Assert
					.assertEquals(doc.getDocData().getDocFile().getDocFormat(),
							docDataForRetrieve.getDocData().getDocFile()
									.getDocFormat());
		} else {
			Assert.assertNull(doc.getDocData().getDocFile());
		}

		if (docDataForRetrieve.getDocData().getSecondaryDocFile() != null) {
			Assert.assertEquals(doc.getDocData().getSecondaryDocFile()
					.getDocFile().getDocFormat(), docDataForRetrieve
					.getDocData().getSecondaryDocFile().getDocFile()
					.getDocFormat());
			System.out.println("created sec file: "
					+ doc.getDocData().getSecondaryDocFile().getDocFile());
			System.out.println("retrieved sec file: "
					+ docDataForRetrieve.getDocData().getSecondaryDocFile()
							.getDocFile());
			if (null != doc.getDocData().getSecondaryDocFile().getDocFile()
					.getDocStream()
					&& null != docDataForRetrieve.getDocData()
							.getSecondaryDocFile().getDocFile().getDocStream()) {
				Assert.assertArrayEquals(doc.getDocData().getSecondaryDocFile()
						.getDocFile().getDocStream(), docDataForRetrieve
						.getDocData().getSecondaryDocFile().getDocFile()
						.getDocStream());
			} else if (null != docDataForRetrieve.getDocData().getDocFile()
					.getDocStream()) {
				System.out.println("checking only doc size");
				Assert.assertEquals(doc.getDocData().getSecondaryDocFile()
						.getDocFile().getDocSize().intValue(), docDataForRetrieve
						.getDocData().getSecondaryDocFile().getDocFile()
						.getDocStream().length);
			} else {
				System.out.println("checking NOT IMPLEMENTED");
			}
			Assert.assertEquals(doc.getDocData().getSecondaryDocFile()
					.getDocFile().getDocFormat(), docDataForRetrieve
					.getDocData().getSecondaryDocFile().getDocFile()
					.getDocFormat());
		} else {
			Assert.assertNull(doc.getDocData().getSecondaryDocFile());
		}

		// test events
		if (docDataForRetrieve.getDocEventsDataList() != null) {
			List<DocEventData> docEventDataRetrieve = docDataForRetrieve
					.getDocEventsDataList();
			Assert.assertEquals(doc.getDocEventData(), docEventDataRetrieve);
		} else {
			Assert.assertNull(doc.getDocEventData());
		}

	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\tbo\CustomerRelationTboAbstractTest.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import org.junit.BeforeClass;

import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfLoginInfo;

public abstract class CustomerRelationTboAbstractTest {
	public static String DOCBASE = "banhap_dev1";
	public static String USER = "dctm";
	public static String PASSWORD = "dctm1";
	public boolean isDFCinitialized = false;
	public static IDfSessionManager s_sm = null;
	public static IDfClientX s_clientx = null;
	public static IDfClient s_client = null;
	public static IDfLoginInfo s_identity = null;

	@BeforeClass
	public static void setUp() throws DfException {
		if (null == s_clientx) {
			s_clientx = new DfClientX();
		}
		if (null == s_client) {
			s_client = s_clientx.getLocalClient();
		}
		if (null == s_sm) {
			s_sm = s_client.newSessionManager();
		}
		if (null == s_identity) {
			s_identity = s_clientx.getLoginInfo();
			s_identity.setUser(USER);
			s_identity.setPassword(PASSWORD);
			s_sm.setIdentity(DOCBASE, s_identity);
		}
	}

	public IDfSession getDocbaseSession() throws DfException {
		return s_sm.getSession(DOCBASE);
	}

	public void releaseDocbaseSession(IDfSession pSess) throws DfException {
		s_sm.release(pSess);
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\tbo\CustomerRelationTboTest.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import org.junit.Assert;
import org.junit.Test;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;

public class CustomerRelationTboTest extends CustomerRelationTboAbstractTest {

	@Test
	public void testCounterUpdate() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;

		IDfSysObject doc = null;
		IDfSysObject event = null;
		IDfPersistentObject obj = null;
		IDfQuery query = null;
		try {
			session = this.getDocbaseSession();

			// create document
			doc = (IDfSysObject) session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(doc);
			doc.setString("object_name", "TEST_CUSTOMER_RELATION_DOC");
			doc.save();

			// create event
			event = (IDfSysObject) session.newObject("bnhp_doc_event");
			Assert.assertNotNull(event);
			event.setString("object_name", "TEST_CUSTOMER_RELATION_EVENT");
			event.setString("event_category_code", "1");
			event.save();

			// create relation
			obj = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj);
			obj.setString("child_id", event.getObjectId().getId());
			obj.setString("parent_id", doc.getObjectId().getId());
			obj.save();

			checkCounterValue(obj.getObjectId().getId(), 1, session);

		} catch (DfException ex) {
			ex.printStackTrace();
			Assert.assertTrue(ex.getMessage().matches(
					".*Business constraint violated.*"));

		} finally {

			if (null != col) {
				try {
					col.close();
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}

			if (null != session) {
				releaseDocbaseSession(session);
			}
		}
	}

	@Test
	public void testCounterUpdateZeroPreviousRelations() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;

		IDfSysObject doc = null;
		IDfSysObject event = null;
		IDfPersistentObject obj1 = null, obj2 = null;

		try {
			session = this.getDocbaseSession();

			// create document
			doc = (IDfSysObject) session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(doc);
			doc.setString("object_name", "TEST_CUSTOMER_RELATION_DOC");
			doc.save();

			// create event
			event = (IDfSysObject) session.newObject("bnhp_doc_event");
			Assert.assertNotNull(event);
			event.setString("object_name", "TEST_CUSTOMER_RELATION_EVENT");
			event.setString("event_category_code", "1");
			event.save();

			// create relation
			obj1 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj1);
			obj1.setString("child_id", event.getObjectId().getId());
			obj1.setString("parent_id", doc.getObjectId().getId());
			obj1.setString("relation_name", "bnhp_cust_rel_temp");
			obj1.save();

			// create relation
			obj2 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj2);
			obj2.setString("child_id", event.getObjectId().getId());
			obj2.setString("parent_id", doc.getObjectId().getId());
			obj2.setString("relation_name", "bnhp_cust_rel_temp");
			obj2.save();

			checkCounterValue(obj1.getObjectId().getId(), 0, session);
			checkCounterValue(obj2.getObjectId().getId(), 1, session);

		} catch (DfException ex) {
			ex.printStackTrace();

		} finally {

			if (null != col) {
				try {
					col.close();
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}

			if (null != session) {
				releaseDocbaseSession(session);
			}
		}
	}

	@Test
	public void testCounterUpdateThreeRelationsCreated() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;

		IDfSysObject doc = null;
		IDfSysObject event = null;
		IDfPersistentObject obj1 = null, obj2 = null, obj3 = null;
		IDfQuery query = null;
		try {
			session = this.getDocbaseSession();

			// create document
			doc = (IDfSysObject) session.newObject("bnhp_customer_doc");
			Assert.assertNotNull(doc);
			doc.setString("object_name", "TEST_CUSTOMER_RELATION_DOC");
			doc.save();

			// create event
			event = (IDfSysObject) session.newObject("bnhp_doc_event");
			Assert.assertNotNull(event);
			event.setString("object_name", "TEST_CUSTOMER_RELATION_EVENT");
			event.setString("event_category_code", "1");
			event.save();

			// create relation
			obj1 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj1);
			obj1.setString("child_id", event.getObjectId().getId());
			obj1.setString("parent_id", doc.getObjectId().getId());
			obj1.setString("relation_name", "bnhp_cust_rel_temp");
			obj1.save();

			// create relation
			obj2 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj2);
			obj2.setString("child_id", event.getObjectId().getId());
			obj2.setString("parent_id", doc.getObjectId().getId());
			obj2.setString("relation_name", "bnhp_cust_rel_temp");
			obj2.save();

			checkCounterValue(obj1.getObjectId().getId(), 0, session);
			checkCounterValue(obj2.getObjectId().getId(), 1, session);

			// create relation
			obj3 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj3);
			obj3.setString("child_id", event.getObjectId().getId());
			obj3.setString("parent_id", doc.getObjectId().getId());
			obj3.setString("relation_name", "bnhp_cust_rel_temp");
			obj3.save();

			checkCounterValue(obj2.getObjectId().getId(), 0, session);
			checkCounterValue(obj3.getObjectId().getId(), 1, session);

		} catch (DfException ex) {
			ex.printStackTrace();

		} finally {

			if (null != col) {
				try {
					col.close();
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}

			if (null != session) {
				releaseDocbaseSession(session);
			}
		}
	}

	@Test
	public void testCounterUpdateTwoEventCodes() throws DfException {
		IDfSession session = null;
		IDfCollection col = null;

		IDfSysObject doc = null;
		IDfSysObject event1 = null, event2 = null;
		IDfPersistentObject obj1 = null, obj2 = null, obj3 = null;

		try {
			session = this.getDocbaseSession();

			// create document
			doc = (IDfSysObject) session.newObject("bnhp_customer_doc");
			doc.setString("object_name", "TEST_CUSTOMER_RELATION_DOC");
			doc.save();

			// create event
			event1 = (IDfSysObject) session.newObject("bnhp_doc_event");
			event1.setString("object_name", "TEST_CUSTOMER_RELATION_EVENT");
			event1.setString("event_category_code", "1");
			event1.save();

			// create event
			event2 = (IDfSysObject) session.newObject("bnhp_doc_event");
			event2.setString("object_name", "TEST_CUSTOMER_RELATION_EVENT");
			event2.setString("event_category_code", "2");
			event2.save();

			// create relation 1
			obj1 = session.newObject("bnhp_cust_rel_temp");
			obj1.setString("child_id", event1.getObjectId().getId());
			obj1.setString("parent_id", doc.getObjectId().getId());
			obj1.setString("relation_name", "bnhp_cust_rel_temp");
			obj1.save();

			// create relation 2
			obj2 = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj1);
			obj2.setString("child_id", event2.getObjectId().getId());
			obj2.setString("parent_id", doc.getObjectId().getId());
			obj2.setString("relation_name", "bnhp_cust_rel_temp");
			obj2.save();

			checkCounterValue(obj1.getObjectId().getId(), 1, session);
			checkCounterValue(obj2.getObjectId().getId(), 1, session);

		} catch (DfException ex) {
			ex.printStackTrace();

		} finally {

			if (null != col) {
				try {
					col.close();
				} catch (Exception ex) {
					ex.printStackTrace();
				}
			}

			if (null != session) {
				releaseDocbaseSession(session);
			}
		}
	}

	private void checkCounterValue(String id, int i, IDfSession session)
			throws DfException {

		String str = "select rel.r_object_id, rel.order_no from dm_relation rel where r_object_id ='"
				+ id + "'";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = query.execute(session, 0);

		int counter = 0;

		if (col != null && col.next()) {
			counter = col.getInt("order_no");
		}

		Assert.assertTrue("Object should have had counter=" + i, counter == i);
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\bnhp\infra\base\tbo\EventTest.java
-----------------------------------------------------
package bnhp.infra.base.tbo;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import junit.framework.Assert;

import org.junit.Test;

import bnhp.infra.dfs.exceptions.DFSDocumentumException;
import bnhp.infra.dfs.legacy_support.CreateObjectServicesWrapper;
import bnhp.infra.dfs.legacy_support.LegacyRetrieveDocEventDataServiceWrapper;
import bnhp.infra.dfs.model.business.BankAccount;
import bnhp.infra.dfs.model.business.DocData;
import bnhp.infra.dfs.model.business.DocEventData;
import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.model.service.DocDataForCreate;
import bnhp.infra.dfs.model.service.DocIdData;
import bnhp.infra.dfs.services.DataInitializer;
import bnhp.infra.dfs.services.DocDataForCreateAndId;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.customerdoc.tbo.ICustomerDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfTime;
import com.documentum.fc.common.IDfId;
import com.emc.documentum.fs.rt.ServiceException;

public class EventTest extends AbstractLegacyTest {

	private SecurityContext m_securityContext = null;
	private SecurityContext m_leggacySecurityContext;
	public static final int LEGACY_PROJECT = 2;

	private static final int EVENT_CODE_HARIG = 201;
	private static final int EVENT_CODE_HEARA = 203;
	private static final int EVENT_CODE_SCAN = 301;

	@Test
	public void testEventHearaCreated() throws DfException {
		LegacyRetrieveDocEventDataServiceWrapper legacyRetrieveWrapper = null;
		try {

			m_securityContext = getTestSecurityContext();
			m_leggacySecurityContext = getLegacyTestSecurityContext();
			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();
			List<DocEventData> dataList = new ArrayList<DocEventData>();
			// create document

			// dataList.add(createdDocEventData);
			DocData docData = DataInitializer.createMinimalValidDocData();
			docData.setDocFile(DataInitializer.createDocFile("/HELLO.pdf",
					"pdf"));
			// docDataForCreate.setDocEventData(null);
			docData.getDocCustomerData().getDocDetails().setProjectId(
					LEGACY_PROJECT);
			docData.setDocPropertyExtensions(DataInitializer
					.createFullDocPropertyExtensions());
			docData.getDocCustomerData().setBankAccounts(
					DataInitializer.createBankAccountList());

			// one group only
			ArrayList<String> documentGroupIds = new ArrayList<String>();
			documentGroupIds.add("group1");
			docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
					documentGroupIds);
			// only one bank account
			List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
			bankAccountList.add(DataInitializer.createBankAccount1());
			docData.getDocCustomerData().setBankAccounts(bankAccountList);
			docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBankId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBranchId(1);
			// docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setEmpIdDocumentTypeCode(1);
			// docData.getDocCustomerData().getExecutorDetails().
			// setTerminalChannelId(
			// 0);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingEmpFullName("Diana");
			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			CreateObjectServicesWrapper oldSystemWrapper = new CreateObjectServicesWrapper();
			List<DocIdData> ids = null;

			// create doc in new system ( the doc copied to the old system also)
			List<DocDataForCreateAndId> docDataForCreateAndIdList = createDocumentInDctm(
					m_leggacySecurityContext, docData, null);
			System.out
					.println("testDocCreated: created documents in new system - "
							+ docDataForCreateAndIdList.size());
			DocIdData docIdData = docDataForCreateAndIdList.get(0)
					.getDocIdData();

			Session.setUp();
			IDfSession session = Session.getDocbaseSession();

			// create event
			DocEventData createdDocEventData = createDocEventData(EVENT_CODE_HEARA);

			IDfSysObject event = createEvent(session, EVENT_CODE_HEARA,
					createdDocEventData);

			// create relation
			String docObjID = findDocObjId(docIdData.getDctmDocumentId(),
					session);
			// String eventObjID = findObjEventId(docObjID, session);

			String legacyId = findLegacyObjId(docIdData.getDctmDocumentId(),
					session);

			IDfPersistentObject obj = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj);
			obj.setString("child_id", event.getObjectId().getId());
			obj.setString("parent_id", docObjID);
			obj.save();

			legacyRetrieveWrapper = new LegacyRetrieveDocEventDataServiceWrapper();

			List<DocEventData> retrievedDocEventDataList = legacyRetrieveWrapper
					.retrieveDocEventDataListByLegacyDocumentId(legacyId,
							m_leggacySecurityContext);
			Assert.assertNotNull(retrievedDocEventDataList);
			Assert.assertTrue(retrievedDocEventDataList.size() > 0);

			for (DocEventData retrievedDocEventData : retrievedDocEventDataList) {
				// Currently, the formats are different - dctm doesn't save the
				// millis
				setIgnoredData(retrievedDocEventData, createdDocEventData);
				Assert.assertEquals(retrievedDocEventData, createdDocEventData);
			}

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}

	}

	@Test
	public void testEventHarigCreated() throws DfException {
		LegacyRetrieveDocEventDataServiceWrapper legacyRetrieveWrapper = null;
		try {

			m_securityContext = getTestSecurityContext();
			m_leggacySecurityContext = getLegacyTestSecurityContext();

			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();

			// create document
			DocData docData = DataInitializer.createMinimalValidDocData();
			docData.setDocFile(DataInitializer.createDocFile("/HELLO.pdf",
					"pdf"));
			// docDataForCreate.setDocEventData(null);
			docData.getDocCustomerData().getDocDetails().setProjectId(
					LEGACY_PROJECT);
			docData.setDocPropertyExtensions(DataInitializer
					.createFullDocPropertyExtensions());
			docData.getDocCustomerData().setBankAccounts(
					DataInitializer.createBankAccountList());

			// one group only
			ArrayList<String> documentGroupIds = new ArrayList<String>();
			documentGroupIds.add("group1");
			docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
					documentGroupIds);
			// only one bank account
			List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
			bankAccountList.add(DataInitializer.createBankAccount1());
			docData.getDocCustomerData().setBankAccounts(bankAccountList);
			docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBankId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBranchId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setEmpIdDocumentTypeCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingEmpFullName("Diana");
			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			// create doc in new system ( the doc copied to the old system also)
			List<DocDataForCreateAndId> docDataForCreateAndIdList = createDocumentInDctm(
					m_leggacySecurityContext, docData, null);
			System.out
					.println("testDocCreated: created documents in new system - "
							+ docDataForCreateAndIdList.size());
			DocIdData docIdData = docDataForCreateAndIdList.get(0)
					.getDocIdData();

			Session.setUp();
			IDfSession session = Session.getDocbaseSession();

			// create event
			DocEventData createdDocEventData = createDocEventData(EVENT_CODE_HARIG);

			IDfSysObject event = createEvent(session, EVENT_CODE_HARIG,
					createdDocEventData);

			// create relation
			String docObjID = findDocObjId(docIdData.getDctmDocumentId(),
					session);

			String legacyId = findLegacyObjId(docIdData.getDctmDocumentId(),
					session);

			IDfPersistentObject obj = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj);
			obj.setString("child_id", event.getObjectId().getId());
			obj.setString("parent_id", docObjID);
			obj.save();

			legacyRetrieveWrapper = new LegacyRetrieveDocEventDataServiceWrapper();

			List<DocEventData> retrievedDocEventDataList = legacyRetrieveWrapper
					.retrieveDocEventDataListByLegacyDocumentId(legacyId,
							m_leggacySecurityContext);
			Assert.assertNotNull(retrievedDocEventDataList);
			Assert.assertEquals(1, retrievedDocEventDataList.size());

			for (DocEventData retrievedDocEventData : retrievedDocEventDataList) {
				// Currently, the formats are different - dctm doesn't save the
				// millis
				setIgnoredData(retrievedDocEventData, createdDocEventData);
				Assert.assertEquals(retrievedDocEventData, createdDocEventData);
			}

			IDfQuery query = new DfQuery();
			query
					.setDQL("select r_object_id from dm_relation where parent_id = '"
							+ docObjID + "'");
			IDfCollection col = query.execute(session, IDfQuery.DF_EXEC_QUERY);
			int count = 0;
			while (col.next()) {
				IDfId relId = col.getId("r_object_id");
				System.out.println("CREATED RELATION: " + relId.toString());
				count++;
			}
			Assert.assertEquals(1, count);
		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}

	}

	@Test
	public void testEventScanCreated() throws DfException {
		LegacyRetrieveDocEventDataServiceWrapper legacyRetrieveWrapper = null;
		try {

			m_securityContext = getTestSecurityContext();
			m_leggacySecurityContext = getLegacyTestSecurityContext();

			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();

			// create document
			DocData docData = DataInitializer.createMinimalValidDocData();
			docData.setDocFile(DataInitializer.createDocFile("/HELLO.pdf",
					"pdf"));
			// docDataForCreate.setDocEventData(null);
			docData.getDocCustomerData().getDocDetails().setProjectId(
					LEGACY_PROJECT);
			docData.setDocPropertyExtensions(DataInitializer
					.createFullDocPropertyExtensions());
			docData.getDocCustomerData().setBankAccounts(
					DataInitializer.createBankAccountList());

			// one group only
			ArrayList<String> documentGroupIds = new ArrayList<String>();
			documentGroupIds.add("group1");
			docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
					documentGroupIds);
			// only one bank account
			List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
			bankAccountList.add(DataInitializer.createBankAccount1());
			docData.getDocCustomerData().setBankAccounts(bankAccountList);
			docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBankId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBranchId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setEmpIdDocumentTypeCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingEmpFullName("Diana");
			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			// create doc in new system ( the doc copied to the old system also)
			List<DocDataForCreateAndId> docDataForCreateAndIdList = createDocumentInDctm(
					m_leggacySecurityContext, docData, null);
			System.out
					.println("testDocCreated: created documents in new system - "
							+ docDataForCreateAndIdList.size());
			DocIdData docIdData = docDataForCreateAndIdList.get(0)
					.getDocIdData();

			Session.setUp();
			IDfSession session = Session.getDocbaseSession();

			DocEventData createdDocEventData = createDocEventData(EVENT_CODE_SCAN);

			// create event
			IDfSysObject event = createEvent(session, EVENT_CODE_SCAN,
					createdDocEventData);

			// create relation
			String docObjID = findDocObjId(docIdData.getDctmDocumentId(),
					session);

			String legacyId = findLegacyObjId(docIdData.getDctmDocumentId(),
					session);

			IDfPersistentObject obj = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj);
			obj.setString("child_id", event.getObjectId().getId());
			obj.setString("parent_id", docObjID);
			obj.save();

			legacyRetrieveWrapper = new LegacyRetrieveDocEventDataServiceWrapper();

			List<DocEventData> retrievedDocEventDataList = legacyRetrieveWrapper
					.retrieveDocEventDataListByLegacyDocumentId(legacyId,
							m_leggacySecurityContext);
			Assert.assertNotNull(retrievedDocEventDataList);
			Assert.assertTrue(retrievedDocEventDataList.size() > 0);

			for (DocEventData retrievedDocEventData : retrievedDocEventDataList) {
				// Currently, the formats are different - dctm doesn't save the
				// millis

				setIgnoredData(retrievedDocEventData, createdDocEventData);

				Assert.assertEquals(retrievedDocEventData, createdDocEventData);
			}

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}

	}

	@Test
	public void testEventHarigCreatedOnNonSyncronizedDoc() throws DfException {
		LegacyRetrieveDocEventDataServiceWrapper legacyRetrieveWrapper = null;
		try {
			String propVal = System.setProperty(
					ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP, "true");
			m_securityContext = getTestSecurityContext();
			m_leggacySecurityContext = getLegacyTestSecurityContext();

			List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
			DocDataForCreate docDataForCreate = new DocDataForCreate();

			// create document
			DocData docData = DataInitializer.createMinimalValidDocData();
			docData.setDocFile(DataInitializer.createDocFile("/HELLO.pdf",
					"pdf"));
			// docDataForCreate.setDocEventData(null);
			docData.getDocCustomerData().getDocDetails().setProjectId(
					LEGACY_PROJECT);
			docData.setDocPropertyExtensions(DataInitializer
					.createFullDocPropertyExtensions());
			docData.getDocCustomerData().setBankAccounts(
					DataInitializer.createBankAccountList());

			// one group only
			ArrayList<String> documentGroupIds = new ArrayList<String>();
			documentGroupIds.add("group1");
			docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
					documentGroupIds);
			// only one bank account
			List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
			bankAccountList.add(DataInitializer.createBankAccount1());
			docData.getDocCustomerData().setBankAccounts(bankAccountList);
			docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBankId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingBranchId(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setEmpIdDocumentTypeCode(1);
			docData.getDocCustomerData().getExecutorDetails()
					.setExecutingEmpFullName("Diana");
			docDataForCreate.setDocData(docData);
			docDataForCreateList.add(docDataForCreate);

			// create doc in new system ( the doc copied to the old system also)
			List<DocDataForCreateAndId> docDataForCreateAndIdList = createDocumentInDctm(
					m_leggacySecurityContext, docData, null);
			System.out
					.println("testDocCreated: created documents in new system - "
							+ docDataForCreateAndIdList.size());
			DocIdData docIdData = docDataForCreateAndIdList.get(0)
					.getDocIdData();

			Session.setUp();
			IDfSession session = Session.getDocbaseSession();

			// create event
			propVal = System.setProperty(
					ICustomerDoc.GLOBAL_SYNC_CUSTOMER_DISABLED_PROP, "false");
			DocEventData createdDocEventData = createDocEventData(EVENT_CODE_HARIG);

			IDfSysObject event = createEvent(session, EVENT_CODE_HARIG,
					createdDocEventData);

			// create relation
			String docObjID = findDocObjId(docIdData.getDctmDocumentId(),
					session);

			String legacyId = findLegacyObjId(docIdData.getDctmDocumentId(),
					session);

			IDfPersistentObject obj = session.newObject("bnhp_cust_rel_temp");
			Assert.assertNotNull(obj);
			obj.setString("child_id", event.getObjectId().getId());
			obj.setString("parent_id", docObjID);
			obj.save();

			legacyRetrieveWrapper = new LegacyRetrieveDocEventDataServiceWrapper();

			List<DocEventData> retrievedDocEventDataList = legacyRetrieveWrapper
					.retrieveDocEventDataListByLegacyDocumentId(legacyId,
							m_leggacySecurityContext);
			Assert.assertNotNull(retrievedDocEventDataList);
			Assert.assertTrue(retrievedDocEventDataList.size() == 1);

			for (DocEventData retrievedDocEventData : retrievedDocEventDataList) {
				// Currently, the formats are different - dctm doesn't save the
				// millis
				setIgnoredData(retrievedDocEventData, createdDocEventData);
				Assert.assertEquals(retrievedDocEventData, createdDocEventData);
			}

		} catch (ServiceException e) {
			e.printStackTrace();
			Assert.fail();
		} catch (DFSDocumentumException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
			Assert.fail();
		}

	}

	private DocEventData createDocEventData(int eventCodeHeara) {

		DocEventData createdDocEventData = new DocEventData();
		createdDocEventData.setAutoEventInd(true);
		createdDocEventData.setLegacyEventEntryDttm(DataInitializer
				.createDateNoMili());
		createdDocEventData.setEventCategoryCode(EVENT_CODE_HEARA);
		createdDocEventData.setObjectName("TEST_CUSTOMER_RELATION_EVENT");
		createdDocEventData.setConcatenatedEventId("1");
		createdDocEventData.setExecutingEmpFullName("Diana");
		createdDocEventData.setExecutingEmpIdCode("20");
		createdDocEventData.setBankolId(20);
		createdDocEventData.setEventDescText("event_desc");
		createdDocEventData.setTerminalChannelId(1);

		return createdDocEventData;
	}

	private void setIgnoredData(DocEventData retrievedDocEventData,
			DocEventData createdDocEventData) {
		retrievedDocEventData.setLegacyEventEntryDttm(createdDocEventData
				.getLegacyEventEntryDttm());
		retrievedDocEventData.setChannelId(createdDocEventData.getChannelId());
		retrievedDocEventData.setEmpIdDocumentTypeCode(createdDocEventData
				.getEmpIdDocumentTypeCode());
		retrievedDocEventData.setEventCategoryErrorCode(createdDocEventData
				.getEventCategoryErrorCode());
		retrievedDocEventData.setEventCategoryStatusCode(createdDocEventData
				.getEventCategoryStatusCode());
		retrievedDocEventData.setEventCategoryTypeCode(createdDocEventData
				.getEventCategoryTypeCode());
		retrievedDocEventData.setEventGroupId(createdDocEventData
				.getEventGroupId());
		retrievedDocEventData.setExecutingBranchId(createdDocEventData
				.getExecutingBranchId());
		retrievedDocEventData.setIpAddress(createdDocEventData.getIpAddress());
		retrievedDocEventData
				.setObjectName(createdDocEventData.getObjectName());
	}

	private IDfSysObject createEvent(IDfSession session, int eventCodeScan,
			DocEventData createdDocEventData) throws DfException {
		IDfSysObject event = (IDfSysObject) session.newObject("bnhp_doc_event");

		event.setString("object_name", createdDocEventData.getObjectName());
		event.setInt("event_category_code", createdDocEventData
				.getEventCategoryCode());
		event.setTime("legacy_event_entry_dttm", new DfTime(createdDocEventData
				.getLegacyEventEntryDttm()));
		event.setString("concatenated_event_id", createdDocEventData
				.getConcatenatedEventId());
		event.setString("executing_emp_full_name", createdDocEventData
				.getExecutingEmpFullName());
		event.setString("executing_emp_id_code", createdDocEventData
				.getExecutingEmpIdCode());
		event.setBoolean("auto_event_ind", createdDocEventData
				.getAutoEventInd());
		event.setInt("bankol_id", createdDocEventData.getBankolId());
		event.setString("event_desc_text", createdDocEventData
				.getEventDescText());
		event.setInt("terminal_channel_id", createdDocEventData
				.getTerminalChannelId());
		event.save();

		return event;
	}

	private String findLegacyObjId(String dctmDocumentId, IDfSession session)
			throws DfException {
		String str = "select legacy_document_id from bnhp_customer_doc where dctm_document_id='"
				+ dctmDocumentId + "'";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = query.execute(session, 0);
		String objId = null;

		if (col != null && col.next()) {
			objId = col.getString("legacy_document_id");

		} else {
			throw new DfException("dctm_document_id doesn't exist");
		}

		return objId;
	}

	private String findDocObjId(String dctmDocumentId, IDfSession session)
			throws DfException {
		String str = "select r_object_id from bnhp_customer_doc where dctm_document_id='"
				+ dctmDocumentId + "'";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = query.execute(session, 0);
		String objId = null;

		if (col != null && col.next()) {
			objId = col.getString("r_object_id");

		} else {
			throw new DfException("dctm_document_id doesn't exist");
		}

		return objId;
	}

	private String findObjEventId(String docId, IDfSession session)
			throws DfException {

		String str = "select r_object_id from bnhp_doc_event where r_object_id in('"
				+ "select child_id from dm_relation where order_no=1 and parent_id in( '"
				+ docId + "')')";
		IDfQuery query = new DfQuery();
		query.setDQL(str);
		IDfCollection col = query.execute(session, 0);
		String objId = null;

		if (col != null && col.next()) {
			objId = col.getString("r_object_id");

		} else {
			throw new DfException("legacy_document_id doesn't exist");
		}

		return objId;
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\com\documentum\fc\client\tfspeula\tbo\TfsPeulaEruaTests.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;

import java.io.ByteArrayOutputStream;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Ignore;
import org.junit.Test;

import bnhp.infra.base.sbo.IMigrationConfigService;
import bnhp.infra.dfs.legacy_support.CreateObjectServicesWrapper;
import bnhp.infra.dfs.legacy_support.ILegacyConstants;
import bnhp.infra.dfs.legacy_support.LegacyDataInitializer;
import bnhp.infra.dfs.legacy_support.LegacyWrapperConstants;
import bnhp.infra.dfs.model.business.BankAccount;
import bnhp.infra.dfs.model.business.DocData;
import bnhp.infra.dfs.model.business.DocEventData;

import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.model.service.DocDataForCreate;
import bnhp.infra.dfs.model.service.DocIdData;
import bnhp.infra.dfs.model.utils.AbstractDfsTest;
import bnhp.infra.dfs.services.DataInitializer;
import bnhp.infra.dfs.utils.dfc.DfcUtils;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfIdNotFoundException;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfPersistentObject;

import com.documentum.fc.client.IDfQuery;

import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.IDfId;
import com.emc.documentum.fs.rt.ServiceException;

public class TfsPeulaEruaTests extends AbstractDfsTest  implements ILegacyConstants {
	public static int EV_CATEGORY_HARIG = 201;
	public static int EV_CATEGORY_HEARA = 203;
	private static final String CUSTOMER_DOC_REPRINT_RELATION = "bnhp_cust_rel";
	private static final String CUSTOMER_DOC_RELATION_SUBTYPE = "bnhp_cust_rel_temp";
	private static final String CUSTOMER_DOC_EVENT_TYPE = "bnhp_doc_event";
	private static final String CUSTOMER_DOC_TYPE = "bnhp_customer_doc";
	
	
	
	
	
	public void setGlobalTfsPeulaSyncDisabled(boolean value) {
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(value));
	}
	
	@Test 
	public void testGlobalSwitch() throws DfException, ServiceException  {
		SecurityContext sctx = this.getTestSecurityContext();
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		ITfsPeulaErua doc = (ITfsPeulaErua) session.newObject("bnhp_yoman_eruim_table");
		
		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(false));
		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(true));
		Assert.assertTrue(doc.isSyncGloballyDisabled());
		System.clearProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP);
		Assert.assertFalse(doc.isSyncGloballyDisabled());
	}
	
	@Test 
	public void testGlobalDocbaseSwitch() throws DfException, ServiceException  {
		SecurityContext sctx = this.getTestSecurityContext();
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfPersistentObject obj = session.getObjectByQualification("bnhp_migration_config"); 
		Assert.assertNotNull(obj);
		Assert.assertTrue(obj.getBoolean("old_to_new"));
		
		
		IDfClient client = DfClient.getLocalClient();
		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
				.newService(IMigrationConfigService.class.getName(),sm);
		Assert.assertNotNull(migrationConfigService);
		Assert.assertTrue(migrationConfigService.isSyncOldToNewEnabled(session));
		
	}
	
	@Ignore
	@Test 
	public void testGlobalDocbaseSwitchChange() throws DfException, ServiceException  {
		SecurityContext sctx = this.getBothTestSecurityContext();
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfPersistentObject obj = session.getObjectByQualification("bnhp_migration_config"); 
		Assert.assertNotNull(obj);
		Assert.assertTrue(obj.getBoolean("old_to_new"));
		
		
		IDfClient client = DfClient.getLocalClient();
		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
				.newService(IMigrationConfigService.class.getName(),sm);
		Assert.assertNotNull(migrationConfigService);
		Assert.assertTrue(migrationConfigService.isSyncOldToNewEnabled(session));
		
		obj.setBoolean("old_to_new", false);
		obj.save();
		Assert.assertTrue(migrationConfigService.isSyncOldToNewEnabled(session));
		sleep(61000);
		Assert.assertFalse(migrationConfigService.isSyncOldToNewEnabled(session));
		
		obj.setBoolean("old_to_new", true);
		obj.save();
		sleep(61000);
		Assert.assertTrue(migrationConfigService.isSyncOldToNewEnabled(session));
	}
	
	@Test
	public void testSingleHarigWithOwner() throws Exception  {
		 testSingleNewEruaSync(new int [] {201},  true);
	}
	
	@Test
	public void testSingleHarigWithoutOwner() throws Exception  {
		 testSingleNewEruaSync(new int [] {201},  false);
	}
	
	
	@Test
	public void testTwoHarigWithOwner() throws Exception  {
		 testSingleNewEruaSync(new int [] {201, 201},  true);
	}
	
	@Test
	public void testTwoHarigWithoutOwner() throws Exception  {
		 testSingleNewEruaSync(new int [] {201, 201},  false);
	}
	/**
	 * detached - true test sync of event and of object syncronized from event tbo
	 *          - false test sync of event syncronization and of object syncronized from event tbo
	 * 
	 * @param category
	 * @param detached
	 * @throws Exception
	 */
	public void testSingleNewEruaSync(int categories[],   boolean detached) throws Exception  {
		setGlobalTfsPeulaSyncDisabled(true);
		SecurityContext sctx = this.getLegacyTestSecurityContext();
			
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		System.out.println("docbase is "+sctx.getRepositoryName());
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfId singleObjectId = createObjectServicesTestCreateValidDoc(session, categories);
			
			
		ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(singleObjectId);
			
			
		Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
		String legacyId = srcDoc.getString("doc_key");
		System.out.println("legacy id is "+legacyId);
		Assert.assertTrue(legacyId.length()>0);
			
		Assert.assertTrue(srcDoc.getStatus().length()==0);
		List <ITfsPeulaErua> events = new ArrayList <ITfsPeulaErua>();
		
		IDfQuery q = new DfQuery();
		q.setDQL("select r_object_id from bnhp_yoman_eruim_table where doc_key='"+srcDoc.getString("doc_key")+"' order by taarich_erua asc");
		//IDfCollection col	= srcDoc.getChildRelatives(CUSTOMER_DOC_REPRINT_RELATION);
		IDfCollection col=null;
		try {
			col = q.execute(session, IDfQuery.DF_QUERY);
			while (col.next()) {
				events.add((ITfsPeulaErua)session.getObject(col.getId("r_object_id")));
			}
		} finally {
			if (null!=col) {
				col.close();
			}
		}
		Assert.assertEquals(categories.length,events.size());
		
		setGlobalTfsPeulaSyncDisabled(false);
		
		boolean eventSaved = false;
		for (ITfsPeulaErua event: events) {
			Assert.assertFalse(event.isSyncDisabled());
			Assert.assertTrue(event.getString("a_status").isEmpty());
			
		}
		if (detached && events.size() >0) {
			ITfsPeulaErua event = events.get(0);
			event.setDirty(true);
			event.save();
		} else  {
			srcDoc.setDirty(true);
			srcDoc.save();
		}
		
		Assert.assertFalse(srcDoc.isDirty());
		Assert.assertTrue(srcDoc.hasBeenSynced());
		IDfId docCopyId = srcDoc.getLinkToCopy();
		Assert.assertFalse(docCopyId.isNull());
		Assert.assertTrue(docCopyId.isObjectId());

		
		IDfSysObject docCopy =  (IDfSysObject) session.getObject(docCopyId);
		Assert.assertNotNull(docCopy);
		Assert.assertEquals(CUSTOMER_DOC_TYPE,docCopy.getTypeName());
		
		Set eventCopyIds = new HashSet<DfId>();
		for (ITfsPeulaErua event: events) {
			Assert.assertTrue(event.hasBeenSynced());
			Assert.assertFalse(event.isDirty());
			Assert.assertFalse(event.getString("a_status").isEmpty());
			IDfId eventCopyId = new DfId(event.getString("a_status"));
			Assert.assertFalse(eventCopyId.isNull());
			Assert.assertTrue(eventCopyId.isObjectId());
			Assert.assertFalse(eventCopyIds.contains(eventCopyId));
			eventCopyIds.add(eventCopyId);
			IDfSysObject eventCopyObj = (IDfSysObject) session.getObject(eventCopyId);
			Assert.assertTrue(eventCopyObj.getTypeName().equals(CUSTOMER_DOC_EVENT_TYPE));
			IDfCollection parents = eventCopyObj.getParentRelatives(CUSTOMER_DOC_REPRINT_RELATION);
			int countParent = 0;
			while (parents.next()) {
				countParent++;
				Assert.assertEquals(docCopyId, parents.getId("parent_id"));
			}
			Assert.assertEquals(1,countParent);
			//ssert.assertEquals(docCopyId, eventCopyObj.getObjectId() );
		}

		
	}
	
		


	
	protected IDfSysObject getCustomerDocByLegacyId(IDfSession session , String legacyId) throws DfException {
		try {
			return (IDfSysObject) session.getObjectByQualification("bnhp_customer_doc where legacy_document_id='"+legacyId+"'");
		} catch (DfIdNotFoundException nfex) {
			return null;
		}
		
	}
	
	public IDfId createObjectServicesTestCreateValidDoc(IDfSession session , int [] statuses) throws Exception {
		CreateObjectServicesWrapper wrapper = new CreateObjectServicesWrapper(); 

		SecurityContext securityContext = getLegacyTestSecurityContext();
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();

		DocDataForCreate doc = new DocDataForCreate();
		DocData docData = DataInitializer.createMinimalValidDocData();

		

		docData.getDocCustomerData().setDocDetails(
				DataInitializer.createFullDocDetails());
		docData.getDocCustomerData().getDocDetails().setProjectId(
				LEGACY_PROJECT);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());
		// mapper will set valid sub area code from form id
		// docData.getDocCustomerData().getDocDetails().setBusinessSubAreaCode(
		// null);

		// those two are checked for validity by legacy app
		// this one is not mandatory in new service
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
		// mandatory in the legacy service
		docData.getDocCustomerData().getExecutorDetails().setExecutingBankId(1);
		docData.getDocCustomerData().getExecutorDetails().setExecutingBranchId(
				1);
		docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
		docData.getDocCustomerData().getExecutorDetails()
		.setEmpIdDocumentTypeCode(1);
		docData.getDocCustomerData().getExecutorDetails().setTerminalChannelId(
				0);
		docData.getDocCustomerData().getExecutorDetails()
		.setExecutingEmpFullName("Ilan benAlon");

		// this one is mandatory in new service and is checked
		// for validity in the old one
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		// this one is mandatory in new service and is checked for validity in
		// the old
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);

		docData.getDocCustomerData().setBankAccounts(
				DataInitializer.createBankAccountList());

		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		bankAccountList.add(DataInitializer.createBankAccount1());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);
		doc.setDocData(docData);

		if (statuses!=null && statuses.length >0) {
			List<DocEventData> docEventData = new ArrayList<DocEventData>();
			doc.setDocEventData(docEventData);
			for (int i = 0; i < statuses.length; i++) {
				if (i > 0) {
					sleep(1001);
				}
				if (EV_CATEGORY_HARIG == statuses[i]) { 
					docEventData.add(LegacyDataInitializer.makeHarigCompatEvent());
				} else if (EV_CATEGORY_HEARA == statuses[i]) {
					docEventData.add(LegacyDataInitializer.makeHearaCompatEvent());
				} else {
					throw new Exception("invalid event category");
				}
			}

		}

		docDataForCreateList.add(doc);

		List<DocIdData> ids = wrapper.createDocuments(
				docDataForCreateList, securityContext, "CURRENT");
		IDfId  result = null;
		for (DocIdData id : ids) {
			Assert.assertNotNull(id);
			Assert.assertNotNull(id.getDctmDocumentId());
			Assert.assertEquals(16,id.getDctmDocumentId().length());
			Assert.assertNotNull(id.getVersionLabel());
			System.out.println("Created document: "+id.getDctmDocumentId());
			result = new DfId (id.getDctmDocumentId());
		}

		
		return result;
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\junit\com\documentum\fc\client\tfspeula\tbo\TfsPeulaSyncTests.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;
import java.io.ByteArrayOutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;


import org.junit.Assert;
import org.junit.BeforeClass;
import org.junit.Test;

import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfIdNotFoundException;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfRelation;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.tfspeula.tbo.ITfsPeulaDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.IDfId;
import com.emc.documentum.fs.rt.ServiceException;
import com.emc.documentum.fs.rt.context.DfcSessionManager;
import com.emc.documentum.fs.rt.context.IServiceContext;
import com.emc.documentum.fs.rt.impl.ResourceLookupException;
import com.emc.documentum.fs.services.core.client.IQueryService;



import bnhp.infra.dfs.exceptions.DFSDocumentumException;
import bnhp.infra.dfs.exceptions.DFSUnexpectedQueryResultException;
import bnhp.infra.dfs.exceptions.DFSValidationException;
import bnhp.infra.dfs.legacy_support.CreateObjectServicesWrapper;
import bnhp.infra.dfs.legacy_support.ILegacyConstants;
import bnhp.infra.dfs.legacy_support.LegacyDataInitializer;
import bnhp.infra.dfs.legacy_support.LegacyWrapperConstants;
import bnhp.infra.dfs.legacy_support.RetrieveObjectServicesWrapper;
import bnhp.infra.dfs.model.business.BankAccount;
import bnhp.infra.dfs.model.business.DocData;
import bnhp.infra.dfs.model.business.DocEventData;
import bnhp.infra.dfs.model.business.DocFile;
import bnhp.infra.dfs.model.business.DocPropertyExtension;
import bnhp.infra.dfs.model.business.ExecutorDetails;
import bnhp.infra.dfs.model.business.PropertyKeyValue;
import bnhp.infra.dfs.model.business.SecondaryDocFile;
import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.model.service.DocDataForCreate;
import bnhp.infra.dfs.model.service.DocDataForRetrieve;
import bnhp.infra.dfs.model.service.DocIdData;
import bnhp.infra.dfs.model.service.DocRetrievalFlags;
import bnhp.infra.dfs.model.service.FetchTypeSet;
import bnhp.infra.dfs.model.service.Full;
import bnhp.infra.dfs.model.service.Meta;
import bnhp.infra.dfs.model.utils.AbstractDfsTest;
import bnhp.infra.dfs.population.business.BusinessDataConversionUtils;
import bnhp.infra.dfs.services.DataInitializer;
import bnhp.infra.dfs.services.RandomDataInitializer;
import bnhp.infra.dfs.services.RetrieveObjectServices;
import bnhp.infra.dfs.utils.business.MakatTablesUtils;
import bnhp.infra.dfs.utils.content.ContentURIProviderFactory;
import bnhp.infra.dfs.utils.content.IContentURIProvider;
import bnhp.infra.dfs.utils.content.IRenditionProvider;
import bnhp.infra.dfs.utils.content.RenditionProviderFactory;
import bnhp.infra.dfs.utils.dfc.DfcUtils;
import bnhp.infra.dfs.validation.business.BasicCompletenessValidation;
import bnhp.infra.dfs.validation.business.BusinessValidationsByDocumentFormId;


public class TfsPeulaSyncTests extends AbstractDfsTest  implements ILegacyConstants {
	public final int SCANCODE_WAIT = 1;
	public final int SCANCODE_SCAN_IN_ARCHIVE = 2;
	public final int SCANCODE_SCAN_IN_BRANCH = 3;
	
	@BeforeClass
	public static void clearGlobalDisable() {
		System.clearProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP);
		IDfClientX clientX = new DfClientX();
		clientX.getAcsTransferPreferences().preferAcsTransfer(false);
		
	}
	
	public void setGlobalTfsPeulaSyncDisabled(boolean value) {
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(value));
	}
	
	@Test 
	public void testGlobalSwitch() throws DfException, ServiceException  {
		SecurityContext sctx = this.getTestSecurityContext();
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		ITfsPeulaDoc doc = (ITfsPeulaDoc) session.newObject(LegacyWrapperConstants.LEGACY_DOC_TYPE);
		
		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(false));
		Assert.assertFalse(doc.isSyncGloballyDisabled());
		System.setProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP, Boolean.toString(true));
		Assert.assertTrue(doc.isSyncGloballyDisabled());
		System.clearProperty(ITfsPeulaDoc.GLOBAL_SYNC_DISABLED_PROP);
		Assert.assertFalse(doc.isSyncGloballyDisabled());
	}
	
	@Test
	public void testSingleNewObjectSync() throws Exception  {
		testSingleNewObjectSync(false,false, SCANCODE_WAIT, 0 , 0, false );
		testSingleNewObjectSync(false,false, SCANCODE_WAIT, 1 , 0, false );
		testSingleNewObjectSync(false,false, SCANCODE_WAIT, 0 , 1, false );
		testSingleNewObjectSync(false,false, SCANCODE_WAIT, 1 , 1, false );
		testSingleNewObjectSync(false,false, SCANCODE_WAIT, 2 , 2, false );
	}
	
	@Test
	public void testSingleNewObjectUpdateCopyContent() throws Exception  {
		IDfSysObject obj = testSingleNewObjectSync(true,false, SCANCODE_WAIT, 0 , 0, false );
		IDfId copyId = new DfId(obj.getString("a_status"));
		IDfSession s = obj.getSession();
		IDfSysObject copyObj = (IDfSysObject) s.getObject(copyId);
		String f = copyObj.getFile(null);
		System.out.println("got content into "+f);
		copyObj.setFileEx(f, copyObj.getContentType(), 0, null);
		copyObj.save();
		
	}
	
	@Test
	public void testSingleNewObjectUpdateContent() throws Exception  {
		IDfSysObject obj = testSingleNewObjectSync(true,false, SCANCODE_WAIT, 0 , 0, false );
		IDfSession s = obj.getSession();
		s.flushCache(true);
		//IDfSysObject copyObj = (IDfSysObject) s.getObject(copyId);
		String f = obj.getFile(null);
		System.out.println("got content into "+f);
		obj.setFileEx(f, obj.getContentType(), 0, null);
		obj.save();
		obj.setTitle(obj.getObjectId().getId());
		obj.save();
	}
	
	@Test
	public void testSingleNewObjectSyncContent() throws Exception  {
		testSingleNewObjectSync(true,false, SCANCODE_WAIT, 0 , 0, false);
		testSingleNewObjectSync(true,false, SCANCODE_WAIT, 1 , 0, false);
		testSingleNewObjectSync(true,false, SCANCODE_WAIT, 0 , 1, false);
		testSingleNewObjectSync(true,false, SCANCODE_WAIT, 2 , 2, false);
		testSingleNewObjectSync(true,false, SCANCODE_WAIT, 2 , 1, false);
	}
	
	@Test
	public void testSingleNewObjectSyncReprint() throws Exception  {
		testSingleNewObjectSync(false,true, SCANCODE_WAIT, 0 , 0, false);
		testSingleNewObjectSync(false,true, SCANCODE_WAIT, 1 , 0, false);
		testSingleNewObjectSync(false,true, SCANCODE_WAIT, 0 , 1, false);
		testSingleNewObjectSync(false,true, SCANCODE_WAIT, 1 , 1, false);
	}
	
	@Test
	public void testSingleNewObjectSyncContentReprint() throws Exception  {
		testSingleNewObjectSync(true,true, SCANCODE_WAIT, 0 , 0, false);
		testSingleNewObjectSync(true,true, SCANCODE_WAIT, 0 , 1, false);
		testSingleNewObjectSync(true,true, SCANCODE_WAIT, 1 , 0, false);
		testSingleNewObjectSync(true,true, SCANCODE_WAIT, 2 , 2, false);
	}
	
	@Test
	public void testSingleNewObjectSyncRandom() throws Exception  {
		for (int i = 0; i <8; i++) {
			testSingleNewObjectSync(true,true, SCANCODE_WAIT, 0 , 0, true);
		}
	}
	
	@Test
	public void testSingleNewObjectSyncRandomWithEventScan() throws Exception  {
		for (int i = 0; i <8; i++) {
			testSingleNewObjectSync(true,true, 2, 0 , 2, true);
		}
	}
	
	@Test
	public void testSingleNewObjectSyncRandomWithEventScanContent() throws Exception  {
		for (int i = 0; i <8; i++) {
			testSingleNewObjectSync(true,true, SCANCODE_WAIT, 2 , 2, true);
		}
	}
	
	@Test
	public void testSingleNewObjectSyncUpdateContent() throws Exception  {
		IDfSysObject srcObj = testSingleNewObjectSync(false,false, SCANCODE_WAIT, 0 , 0, false);
		DocFile docFile = DataInitializer.createDocFile("/HELLO.pdf", "pdf");
		srcObj.setContentType(docFile.getDocFormat());
		ByteArrayOutputStream os = new ByteArrayOutputStream(docFile.getDocStream().length);
		os.write(docFile.getDocStream());
		srcObj.setContent(os);
		srcObj.save();
	}
	
	@Test
	public void testSingleNewObjectSyncReprintScan() throws Exception  {
		testSingleNewObjectSync(false,true, 2, 0 , 0, false);
	}
	
	@Test
	public void testSingleNewObjectSyncContentReprintScan() throws Exception  {
		testSingleNewObjectSync(true,true, 3, 0 , 0, false);
	}
	
	@Test
	public void testSingleNewObjectSyncUpdateContentScan() throws Exception  {
		IDfSysObject srcObj = testSingleNewObjectSync(false,false, 2, 0 , 0, false);
		DocFile docFile = DataInitializer.createDocFile("/HELLO.pdf", "pdf");
		srcObj.setContentType(docFile.getDocFormat());
		ByteArrayOutputStream os = new ByteArrayOutputStream(docFile.getDocStream().length);
		os.write(docFile.getDocStream());
		srcObj.setContent(os);
		srcObj.save();
	}
	
	
	
	public IDfSysObject testSingleNewObjectSync(boolean content, boolean reprint, int scanStatus, int harigim, int hearot, boolean random) throws Exception  {
		
		SecurityContext sctx = this.getLegacyTestSecurityContext();
		
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		System.out.println("docbase is "+sctx.getRepositoryName());
		IDfSession session = sm.getSession(sctx.getRepositoryName());
		
		IDfId singleObjectId = createObjectServicesTestCreateValidDoc(session, content,reprint, scanStatus, harigim, hearot, random);
		
		session.flushCache(true);
		sleep(1000);
		session.flushCache(true);
		ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(singleObjectId);
		srcDoc.fetch(null);
		
		
		Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
		String legacyId = srcDoc.getString("doc_key");
		System.out.println("legacy id is "+legacyId);
		Assert.assertTrue(legacyId.length()>0);
		
		Assert.assertTrue(srcDoc.getString("a_status").length()==16);
		
		IDfSysObject custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNotNull(custDoc);
		checkVersionsChain(session,srcDoc,custDoc);
		return srcDoc;
	} 
	
	@Test
	public void testSingleObjectSync() throws Exception  {
		testSingleObjectSync(false,false, SCANCODE_WAIT, 0 , 0);
		testSingleObjectSync(false,false, SCANCODE_WAIT, 0 , 1);
		testSingleObjectSync(false,false, SCANCODE_WAIT, 1 , 0);
		testSingleObjectSync(false,false, SCANCODE_WAIT, 2 , 2);
	}
	
	@Test
	public void testSingleObjectSyncContent() throws Exception  {
		testSingleObjectSync(true,false, SCANCODE_WAIT, 0 , 0);
		testSingleObjectSync(true,false, SCANCODE_WAIT, 0 , 1);
		testSingleObjectSync(true,false, SCANCODE_WAIT, 1 , 0);
		testSingleObjectSync(true,false, SCANCODE_WAIT, 2 , 2);
	}
	
	@Test
	public void testSingleObjectSyncReprint() throws Exception  {
		testSingleObjectSync(false,true, SCANCODE_WAIT, 0 , 0);
		testSingleObjectSync(false,true, SCANCODE_WAIT, 0 , 1);
		testSingleObjectSync(false,true, SCANCODE_WAIT, 1 , 0);
		testSingleObjectSync(false,true, SCANCODE_WAIT, 2 , 2);
	}
	
	@Test
	public void testSingleObjectSyncContentReprint() throws Exception  {
		testSingleObjectSync(true,true, SCANCODE_WAIT, 0 , 0);
		testSingleObjectSync(true,true, SCANCODE_WAIT, 1 , 0);
		testSingleObjectSync(true,true, SCANCODE_WAIT, 0 , 1);
	}
	
	@Test
	public void testSingleObjectSyncScan() throws Exception  {
		testSingleObjectSync(false,false, 2, 0 , 0);
		testSingleObjectSync(false,false, 2, 0 , 1);
		testSingleObjectSync(false,false, 2, 0 , 2);
	}
	
	@Test
	public void testSingleObjectSyncContentScan() throws Exception  {
		testSingleObjectSync(true,false, 2, 0 , 0);
		testSingleObjectSync(true,false, 2, 0 , 1);
		testSingleObjectSync(true,false, 2, 0 , 2);
	}
	
	@Test
	public void testSingleObjectSyncReprintScan() throws Exception  {
		testSingleObjectSync(false,true, 2, 0 , 0);
		testSingleObjectSync(false,true, 2, 0 , 1);
		testSingleObjectSync(false,true, 2, 0 , 2);
	}
	
	@Test
	public void testSingleObjectSyncContentReprintScan() throws Exception  {
		testSingleObjectSync(true,true, 2, 0 , 0);
		testSingleObjectSync(true,true, 2, 0 , 1);
		testSingleObjectSync(true,true, 2, 0 , 2);
	}
	public void testSingleObjectSync(boolean primaryContent, boolean reprint, int scanCode, int harigim, int hearot) throws Exception  {
		this.setGlobalTfsPeulaSyncDisabled(true);
		SecurityContext sctx = this.getLegacyTestSecurityContext();
		
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfId singleObjectId = createObjectServicesTestCreateValidDoc(session, primaryContent, reprint, scanCode, harigim, hearot, false);
	
		ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(singleObjectId);
		Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
		String legacyId = srcDoc.getString("doc_key");
		System.out.println("legacy id is "+legacyId);
		Assert.assertTrue(legacyId.length()>0);
		
		Assert.assertTrue(srcDoc.getString("a_status").length()==0);
		
		
		IDfSysObject custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNull(custDoc);
		
		//System.out.println(srcDoc.dump());
		srcDoc.setObjectName(srcDoc.getObjectName());
		Assert.assertTrue(srcDoc.isDirty());
		this.setGlobalTfsPeulaSyncDisabled(false);
		
		srcDoc.save();
		session.flushCache(true);
		custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNotNull(custDoc);
		srcDoc.fetch(null);
		checkVersionsChain(session,srcDoc,custDoc);
	}  

	@Test 
	public void testVersion2ObjectSync() throws Exception  {
		testVersionsObjectSync(false, false, 2, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSync() throws Exception  {
		testVersionsObjectSync(false, false, 5, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncComments() throws Exception  {
		testVersionsObjectSync(false, false, 5, SCANCODE_WAIT, 0 , 1);
	}
	
	@Test 
	public void testVersion5ObjectSyncHarig() throws Exception  {
		testVersionsObjectSync(false, false, 5, SCANCODE_WAIT, 1 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncHarigComment() throws Exception  {
		testVersionsObjectSync(false, false, 5, SCANCODE_WAIT, 1 , 1);
	}
	
	@Test 
	public void testVersion5ObjectSyncContentHarigComment() throws Exception  {
		testVersionsObjectSync(true, false, 5, SCANCODE_WAIT, 1 , 1);
	}
	
	@Test 
	public void testVersion5ObjectSyncReprintHarigComment() throws Exception  {
		testVersionsObjectSync(false, true, 5, SCANCODE_WAIT, 1 , 1);
	}
	
	@Test 
	public void testVersion5ObjectSyncContentReprintHarigComment() throws Exception  {
		testVersionsObjectSync(true, true, 5, SCANCODE_WAIT, 1 , 1);
	}
	
	@Test 
	public void testVersion2ObjectSyncContent() throws Exception  {
		testVersionsObjectSync(true, false, 2, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncContent() throws Exception  {
		testVersionsObjectSync(true, false,  5, SCANCODE_WAIT, 0 , 0);
	}

	@Test 
	public void testVersion2ObjectSyncReprint() throws Exception  {
		testVersionsObjectSync(false, true, 2 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncReprint() throws Exception  {
		testVersionsObjectSync(false, true, 5 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion2ObjectSyncContentReprint() throws Exception  {
		testVersionsObjectSync(true, true, 2 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncContentReprint() throws Exception  {
		testVersionsObjectSync(true, true,  5 , SCANCODE_WAIT, 0 , 0);
	}
	
	
	
	@Test 
	public void testVersion2ObjectSyncScan() throws Exception  {
		testVersionsObjectSync(false, false, 2, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncScan() throws Exception  {
		testVersionsObjectSync(false, false, 5, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersion2ObjectSyncContentScan() throws Exception  {
		testVersionsObjectSync(true, false, 2, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncContentScan() throws Exception  {
		testVersionsObjectSync(true, false,  5, SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}

	@Test 
	public void testVersion2ObjectSyncReprintScan() throws Exception  {
		testVersionsObjectSync(false, true, 2 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncReprintScan() throws Exception  {
		testVersionsObjectSync(false, true, 5 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	@Test 
	public void testVersion2ObjectSyncContentReprintScan() throws Exception  {
		testVersionsObjectSync(true, true, 2 , SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersion5ObjectSyncContentReprintScan() throws Exception  {
		testVersionsObjectSync(true, true,  5 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	public void testVersionsObjectSync(boolean primaryContent, boolean reprint, int numVersions, int scan, int harigim, int hearot) throws Exception  {
		this.setGlobalTfsPeulaSyncDisabled(true);
		SecurityContext sctx = this.getLegacyTestSecurityContext();
		
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfId singleObjectId = createObjectServicesTestCreateValidDoc(session , primaryContent, reprint, scan, harigim, hearot, false);
		
		ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(singleObjectId);
		Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
		String legacyId = srcDoc.getString("doc_key");
		System.out.println("legacy id is "+legacyId);
		Assert.assertTrue(legacyId.length()>0);
		Assert.assertTrue(srcDoc.getString("a_status").length()==0);

		dumpVersion("source document labels:", srcDoc);
		for (int i = 1; i < numVersions; i++) {
			System.out.println("making version "+(i+1));
			srcDoc.checkout();
			srcDoc.setObjectName(srcDoc.getObjectName()+"_"+(i+1));
			srcDoc.setInt("girsa", srcDoc.getInt("girsa")+1);
			IDfId newSrcId = srcDoc.checkin(false, "CURRENT,VERSION_"+(i+1));
			srcDoc = (ITfsPeulaDoc) session.getObject(newSrcId);
			dumpVersion("source document labels:", srcDoc);
		}
		
		IDfSysObject custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNull(custDoc);
		
		srcDoc.setObjectName(srcDoc.getObjectName());
		Assert.assertTrue(srcDoc.isDirty());
		this.setGlobalTfsPeulaSyncDisabled(false);
		
		srcDoc.save();
		session.flushCache(true);
		
		custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNotNull(custDoc);
		
		
		if (primaryContent) {
			Assert.assertTrue(custDoc.getContentSize() > 0);
			Assert.assertEquals(custDoc.getContentType(), srcDoc.getContentType());
		}
		if (numVersions > 1) {
			Assert.assertNotSame(custDoc.getObjectId(), custDoc.getChronicleId());
		} else {
			Assert.assertEquals(custDoc.getObjectId(), custDoc.getChronicleId());
		}
		
		checkVersionsChain(session, srcDoc,custDoc);
		srcDoc.fetch(null);
	}  
	
	
	@Test 
	public void testVersion2CheckinObjectSync() throws Exception  {
		testVersionsCheckinObjectSync(false, false, 2, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSync() throws Exception  {
		testVersionsCheckinObjectSync(false, false, 5, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion2CheckinObjectSyncContent() throws Exception  {
		testVersionsCheckinObjectSync(true, false, 2, SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncContent() throws Exception  {
		testVersionsCheckinObjectSync(true, false,  5, SCANCODE_WAIT, 0 , 0);
	}

	@Test 
	public void testVersion2CheckinObjectSyncReprint() throws Exception  {
		testVersionsCheckinObjectSync(false, true, 2 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncReprint() throws Exception  {
		testVersionsCheckinObjectSync(false, true, 5 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersion2CheckinObjectSyncContentReprint() throws Exception  {
		testVersionsCheckinObjectSync(true, true, 2 , SCANCODE_WAIT, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncContentReprint() throws Exception  {
		testVersionsCheckinObjectSync(true, true,  5 , SCANCODE_WAIT, 0 , 0);
	}
	
	
	
	@Test 
	public void testVersion2CheckinObjectSyncScan() throws Exception  {
		testVersionsCheckinObjectSync(false, false, 2, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncScan() throws Exception  {
		testVersionsCheckinObjectSync(false, false, 5, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersion2CheckinObjectSyncContentScan() throws Exception  {
		testVersionsCheckinObjectSync(true, false, 2, SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncContentScan() throws Exception  {
		testVersionsCheckinObjectSync(true, false,  5, SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}

	@Test 
	public void testVersion2CheckinObjectSyncReprintScan() throws Exception  {
		testVersionsCheckinObjectSync(false, true, 2 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncReprintScan() throws Exception  {
		testVersionsCheckinObjectSync(false, true, 5 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	@Test 
	public void testVersion2CheckinObjectSyncContentReprintScan() throws Exception  {
		testVersionsCheckinObjectSync(true, true, 2 , SCANCODE_SCAN_IN_BRANCH, 0 , 0);
	}
	
	@Test 
	public void testVersionCheckin5ObjectSyncContentReprintScan() throws Exception  {
		testVersionsObjectSync(true, true,  5 , SCANCODE_SCAN_IN_BRANCH, 0, 0);
	}
	
	public void testVersionsCheckinObjectSync(boolean primaryContent, boolean reprint, int numVersions, int scan, int harigim, int hearot) throws Exception  {
		this.setGlobalTfsPeulaSyncDisabled(false);
		SecurityContext sctx = this.getLegacyTestSecurityContext();
		
		IDfSessionManager sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
		IDfSession session = sm.newSession(sctx.getRepositoryName());
		
		IDfId singleObjectId = createObjectServicesTestCreateValidDoc(session , primaryContent, reprint, scan, harigim, hearot, false);
		
		ITfsPeulaDoc srcDoc = (ITfsPeulaDoc) session.getObject(singleObjectId);
		Assert.assertEquals(LegacyWrapperConstants.LEGACY_DOC_TYPE, srcDoc.getType().getName());
		String legacyId = srcDoc.getString("doc_key");
		System.out.println("legacy id is "+legacyId);
		Assert.assertTrue(legacyId.length()>0);
		Assert.assertTrue(srcDoc.getString("a_status").length()==16);

		dumpVersion("source document labels:", srcDoc);
		for (int i = 1; i < numVersions; i++) {
			System.out.println("making version "+(i+1));
			srcDoc.checkout();
			srcDoc.setObjectName(srcDoc.getObjectName()+"_"+(i+1));
			srcDoc.setInt("girsa", srcDoc.getInt("girsa")+1);
			IDfId newSrcId = srcDoc.checkin(false, "CURRENT,VERSION_"+(i+1));
			srcDoc = (ITfsPeulaDoc) session.getObject(newSrcId);
			dumpVersion("source document labels:", srcDoc);
		}
		
		
		Assert.assertFalse(srcDoc.isDirty());
		
		session.flushCache(true);
		
		IDfSysObject custDoc = getCustomerDocByLegacyId(session, legacyId);
		Assert.assertNotNull(custDoc);
		Assert.assertNotNull(custDoc);
		
		
		if (primaryContent) {
			Assert.assertTrue(custDoc.getContentSize() > 0);
			Assert.assertEquals(custDoc.getContentType(), srcDoc.getContentType());
		}
		if (numVersions > 1) {
			Assert.assertNotSame(custDoc.getObjectId(), custDoc.getChronicleId());
		} else {
			Assert.assertEquals(custDoc.getObjectId(), custDoc.getChronicleId());
		}
		
		checkVersionsChain(session, srcDoc,custDoc);
		srcDoc.fetch(null);
	} 
	
	
	protected void checkVersionsChain(IDfSession session, ITfsPeulaDoc srcDoc, IDfSysObject custDoc) throws DfException, ServiceException, DFSDocumentumException {
		flushCache();
		RetrieveObjectServicesWrapper rw = new RetrieveObjectServicesWrapper();
		RetrieveObjectServices rs = new RetrieveObjectServices();
		checkVersionsChainRecursive(rw,rs, session,srcDoc,custDoc);
		
	}
	
	protected void checkVersionsChainRecursive(RetrieveObjectServicesWrapper rw, RetrieveObjectServices rs, IDfSession session, ITfsPeulaDoc srcDoc, IDfSysObject custDoc) throws DfException, ServiceException, DFSDocumentumException {
		
		Assert.assertEquals(null == srcDoc, null == custDoc);
		if (null == srcDoc) {
			return;
		}
		System.out.println("checking documents "+srcDoc.getObjectId().getId()+" "+custDoc.getObjectId().getId());
		dumpVersion("source document labels:", srcDoc);
		dumpVersion("dst document labels:", custDoc);
		
		

		ensureDocumentsMatch(srcDoc,custDoc);
		//DocDataForRetrieve retrievedData = rw.retrieveDoc4LegacyDocumentId(legacyDocumentId,
		//		null, m_securityContext, fileFetchTypeSet, flags );
		
		srcDoc = (ITfsPeulaDoc) (srcDoc.getAntecedentId().isNull() ? null : session.getObject(srcDoc.getAntecedentId()));
		custDoc = (IDfSysObject) (custDoc.getAntecedentId().isNull() ? null : session.getObject(custDoc.getAntecedentId()));
		checkVersionsChainRecursive(rw, rs, session,srcDoc,custDoc);
	}

	protected void ensureDocumentsMatch(ITfsPeulaDoc srcDoc, IDfSysObject custDoc) throws DfException, ServiceException, DFSDocumentumException {
		srcDoc.getString("a_status").equals(custDoc.getObjectId().getId());
		custDoc.getString("a_status").equals(srcDoc.getObjectId().getId());
		Assert.assertEquals(srcDoc.isImmutable(), !srcDoc.getHasFolder());
		Assert.assertEquals(srcDoc.isImmutable(), custDoc.isImmutable());
		Assert.assertEquals(srcDoc.getHasFolder(), custDoc.getHasFolder());
		Assert.assertEquals(srcDoc.getVersionLabelCount(),custDoc.getVersionLabelCount());
		
		for (int i = 0; i < srcDoc.getVersionLabelCount(); i++) {
			Assert.assertEquals(srcDoc.getVersionLabel(i),custDoc.getVersionLabel(i));
		}
		if (srcDoc.getHasFolder() && custDoc.getHasFolder()) {
		
			DocData srcDocData = retrieveLegacyDoc(srcDoc.getString("doc_key"), null);
			DocData custDocData = retrieveCustDoc(custDoc.getString("dctm_document_id"), null);
			// HACK
			/*if (srcDocData.getDocCustomerData()!=null &&
					srcDocData.getDocCustomerData().getPensionFund()!=null) {
				if (srcDocData.getDocCustomerData().getPensionFund().getPensionFundNbr() != null &&
						srcDocData.getDocCustomerData().getPensionFund().getPensionFundNbr()==0 &&
						srcDocData.getDocCustomerData().getPensionFund().getPlanholderNumber() != null &&
						srcDocData.getDocCustomerData().getPensionFund().getPlanholderNumber() == 0) {
					srcDocData.getDocCustomerData().setPensionFund(null);
					if (null!=custDocData.getDocCustomerData().getPensionFund()) {
						custDocData.getDocCustomerData().setPensionFund(null);
					}
				}
						
			}*/
			this.assertConsistency(custDocData);
			setIgnoringValues(srcDocData, custDocData);
			assertSameDocumentData(srcDocData, custDocData);
		}
		//DocData custDocData = retrieveCustDoc(srcDoc.getString("doc_key"), null);
	}
	
	

	protected void assertConsistency(DocData custDocData) throws DFSValidationException, DFSUnexpectedQueryResultException, ServiceException {
		BasicCompletenessValidation.validateDocData(custDocData);
		MakatTablesUtils tablesUtils = new MakatTablesUtils(); 
		tablesUtils.setServiceContext(this.getTestServiceContext(this.getLegacyTestSecurityContext()));
		tablesUtils.setSecContext(this.getLegacyTestSecurityContext());
		
		BusinessValidationsByDocumentFormId v = new BusinessValidationsByDocumentFormId();
		v.setMakatTablesUtils(tablesUtils);
		v.validateEntity(custDocData);
	}

	protected void flushCache() {
		// TODO Auto-generated method stub
		try {
			IDfSessionManager smgr = DfcSessionManager.getSessionManager();
			IDfSession s = null;
			try {
				s=smgr.getSession(getTestSecurityContext().getRepositoryName());
				s.flushCache(true);
			} catch (DfException e) {
				System.out.println("WARN: failed to flush cache: "+e.getMessage());
			} finally {
				if (null!=s) {
					smgr.release(s);
				}
			}
		} catch (ResourceLookupException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

	protected void assertSameDocumentData(DocData doc,	DocData docDataForRetrieve) {
		if (docDataForRetrieve.getDocPropertyExtensions() != null) {
			List<DocPropertyExtension> retrPropExts = docDataForRetrieve
					.getDocPropertyExtensions();
			List<DocPropertyExtension> creatPropExts = doc.getDocPropertyExtensions();
			Assert.assertNotNull(creatPropExts);

			for (int i = 0; i < creatPropExts.size(); i++) {
				DocPropertyExtension retrPropExt = retrPropExts.get(i);
				DocPropertyExtension creatPropExt = creatPropExts.get(i);
				List<PropertyKeyValue> retrKeyValues = retrPropExt
						.getPropertyKeyValues();
				List<PropertyKeyValue> creatKeyValues = creatPropExt
						.getPropertyKeyValues();
				for (PropertyKeyValue val : creatKeyValues) {
					if (LegacyWrapperConstants.BOX_DOC_SERIAL_NBR_EXT_ATTR
							.equals(val.getKey())) { 
						retrKeyValues.add(val);
					}
				}
			}
		}

		BusinessDataConversionUtils.convertNullBooleansToFalse(doc);
		Assert.assertEquals(doc.getDocCustomerData().getDocDetails(),
				docDataForRetrieve.getDocCustomerData().getDocDetails());
		Assert.assertEquals(doc.getDocCustomerData().getPensionFund(),
				docDataForRetrieve.getDocCustomerData().getPensionFund());
		Assert.assertEquals(doc.getDocCustomerData().getBankAccounts(),
				docDataForRetrieve.getDocCustomerData().getBankAccounts());
		Assert.assertEquals(doc.getDocCustomerData().getCustomerKeys(),
				docDataForRetrieve.getDocCustomerData().getCustomerKeys());
		Assert.assertEquals(doc.getDocCustomerData().getCustom(),
				docDataForRetrieve.getDocCustomerData().getCustom());
		Assert.assertEquals(doc.getDocCustomerData().getExecutorDetails(),
				docDataForRetrieve.getDocCustomerData().getExecutorDetails());
		Assert.assertEquals(doc.getDocCustomerData().getGroup(),
				docDataForRetrieve.getDocCustomerData().getGroup());
		
		Assert.assertEquals(doc.getDocCustomerData(),
				docDataForRetrieve.getDocCustomerData());
		
		// File ...  
		if (docDataForRetrieve.getDocFile()!=null) {
			System.out.println("File exist - " + doc.getDocFile());
			Assert.assertEquals(doc.getDocFile(),docDataForRetrieve.getDocFile());
			System.out.println(doc.getDocFile());
			System.out.println(docDataForRetrieve.getDocFile());
		} else {
			Assert.assertNull( doc.getDocFile());
		}
		
		// test events
		/*if (docDataForRetrieve.getDocEventsDataList() != null) {
			List <DocEventData> docEventDataRetrieve = docDataForRetrieve
					.getDocEventsDataList();
			Assert.assertEquals(doc.getDocEventData(), docEventDataRetrieve );
		} else {
			Assert.assertNull(doc.getDocEventData());
		}
		 */
	}
	
	private void setIgnoringValues(DocData srcDocData, DocData custDocData) {
		Assert.assertNull(srcDocData.getDocCustomerData().getDocDetails().getDocDeliveryNum());
		Assert.assertEquals(0,custDocData.getDocCustomerData().getDocDetails().getDocDeliveryNum().intValue());
		srcDocData.getDocCustomerData().getDocDetails().setDocDeliveryNum(0);
		
		custDocData.getDocCustomerData().setGroup(null);
		//retreivedData.getDocData().getDocCustomerData().getExecutorDetails().setInstructionReceiveTypeCode(docDataForCreate.getDocData().getDocCustomerData().getExecutorDetails().getInstructionReceiveTypeCode());
		//retreivedData.getDocData().getDocCustomerData().getCustomerKeys().get(0).setOccasionalCustomerInd(docDataForCreate.getDocData().getDocCustomerData().getCustomerKeys().get(0).getOccasionalCustomerInd());
		//retreivedData.getDocData().getDocCustomerData().getDocDetails().setBusinessProcessId(docDataForCreate.getDocData().getDocCustomerData().getDocDetails().getBusinessProcessId());
		//retreivedData.getDocData().getDocCustomerData().getDocDetails().setSignatureStatusCode(docDataForCreate.getDocData().getDocCustomerData().getDocDetails().getSignatureStatusCode());
	}
	
	protected DocData retrieveCustDoc(String dctmId, String version) throws ServiceException, DFSDocumentumException {
		RetrieveObjectServices svc = new RetrieveObjectServices();
		SecurityContext securityContext = getLegacyTestSecurityContext();
		IServiceContext serviceContext = getTestServiceContext(securityContext);
	
		// set up content URI provider
		
		ExecutorDetails executor = new ExecutorDetails();
		executor.setIpAddress("127.0.0.1");
		executor.setExecutingEmpIdCode("test");
		Meta fetchType = new Meta();
		FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
		fileFetchTypeSet.setFetchType(fetchType);
	
		DocRetrievalFlags flags = new DocRetrievalFlags();
		flags.setShouldRetrieveDocEventData(true);
		flags.setShouldRetrieveSecondaryFile(false);
		DocDataForRetrieve retrievedData = 
			svc.retrieveDoc4DctmDocumentId(dctmId, null, securityContext, 
					executor, 
					fileFetchTypeSet, flags, 
					version);
		return retrievedData.getDocData();
		
	}
	
	protected DocData retrieveLegacyDoc(String legacyId, String version) throws ServiceException, DFSDocumentumException {
		RetrieveObjectServicesWrapper wrapper = new RetrieveObjectServicesWrapper();
		SecurityContext securityContext = getLegacyTestSecurityContext();
		IServiceContext serviceContext = getTestServiceContext(securityContext);
	
		// set up content URI provider
		IContentURIProvider uriProvider = 
			ContentURIProviderFactory.getProvider(serviceContext,
					securityContext.getRepositoryName(),
					securityContext.getUserName());
		IRenditionProvider renditionProvider = 
			RenditionProviderFactory.getProvider(serviceContext,
					securityContext.getRepositoryName(),
					securityContext.getUserName());
		wrapper.setRenditionProvider(renditionProvider);
		wrapper.setUriProvider(uriProvider);
	
		Meta fetchType = new Meta();
		FetchTypeSet fileFetchTypeSet = new FetchTypeSet();
		fileFetchTypeSet.setFetchType(fetchType);
	
		DocRetrievalFlags flags = new DocRetrievalFlags();
		flags.setShouldRetrieveDocEventData(false);
		flags.setShouldRetrieveSecondaryFile(false);
		DocDataForRetrieve retrievedData = wrapper.retrieveDoc4LegacyDocumentId(legacyId,
				null, securityContext, fileFetchTypeSet, flags );
		return retrievedData.getDocData();
		//setIgnoringValues(docDataForCreate, retrievedData);
		//assertSameDocumentData(docDataForCreate, retrievedData);
	}
	
	protected void assertSameDocumentData(DocDataForCreate doc,
			DocDataForRetrieve docDataForRetrieve) {
		if (docDataForRetrieve.getDocData().getDocPropertyExtensions() != null) {
			List<DocPropertyExtension> retrPropExts = docDataForRetrieve
					.getDocData().getDocPropertyExtensions();
			List<DocPropertyExtension> creatPropExts = doc.getDocData()
					.getDocPropertyExtensions();
			Assert.assertNotNull(creatPropExts);

			for (int i = 0; i < creatPropExts.size(); i++) {
				DocPropertyExtension retrPropExt = retrPropExts.get(i);
				DocPropertyExtension creatPropExt = creatPropExts.get(i);
				List<PropertyKeyValue> retrKeyValues = retrPropExt
						.getPropertyKeyValues();
				List<PropertyKeyValue> creatKeyValues = creatPropExt
						.getPropertyKeyValues();
				for (PropertyKeyValue val : creatKeyValues) {
					if (LegacyWrapperConstants.BOX_DOC_SERIAL_NBR_EXT_ATTR
							.equals(val.getKey())) { 
						retrKeyValues.add(val);
					}
				}
			}
		}

		BusinessDataConversionUtils.convertNullBooleansToFalse(doc.getDocData());
		Assert.assertEquals(doc.getDocData().getDocCustomerData(),
				docDataForRetrieve.getDocData().getDocCustomerData());
		
		// File ...  
		if (docDataForRetrieve.getDocData().getDocFile()!=null) {
			System.out.println("File exist - " + docDataForRetrieve.getDoesFileContentExist());
			Assert.assertEquals(doc.getDocData().getDocFile(),docDataForRetrieve.getDocData().getDocFile());
			System.out.println(doc.getDocData().getDocFile());
			System.out.println(docDataForRetrieve.getDocData().getDocFile());
		} else {
			Assert.assertNull( doc.getDocData().getDocFile());
		}
		
		// test events
		if (docDataForRetrieve.getDocEventsDataList() != null) {
			List <DocEventData> docEventDataRetrieve = docDataForRetrieve
					.getDocEventsDataList();
			Assert.assertEquals(doc.getDocEventData(), docEventDataRetrieve );
		} else {
			Assert.assertNull(doc.getDocEventData());
		}

	}
	
	protected void dumpVersion(String string, IDfSysObject srcDoc) throws DfException {
		System.out.print(string);
		for (int i = 0; i < srcDoc.getVersionLabels().getVersionLabelCount(); i++) {
			System.out.print("\n  "+srcDoc.getVersionLabels().getVersionLabel(i));
		}
		System.out.println();
	}

	protected IDfSysObject getCustomerDocByLegacyId(IDfSession session , String legacyId) throws DfException {
		try {
			return (IDfSysObject) session.getObjectByQualification("bnhp_customer_doc where legacy_document_id='"+legacyId+"'");
		} catch (DfIdNotFoundException nfex) {
			return null;
		}
		
	}

	public IDfId createObjectServicesTestCreateValidDoc(IDfSession session , boolean primaryContent, boolean reprint, int scanStatus, int harigim, int hearot, boolean random) throws Exception {
		CreateObjectServicesWrapper wrapper = new CreateObjectServicesWrapper(); 
		
		SecurityContext securityContext = getLegacyTestSecurityContext();
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();
		
		RandomDataInitializer rdi = null;
		if (random) {
			rdi = new RandomDataInitializer();
		}
		
		DocDataForCreate doc = new DocDataForCreate();
		DocData docData = random ? rdi.createMinimalCompatibleValidDocData(): DataInitializer.createValidFullDocDataNoFiles();
		 
		if (primaryContent ) {
			DocFile docFile = DataInitializer.createDocFile("/HELLO.pdf", "pdf");
			
			docData.setDocFile(docFile);
		}
		DocFile  secDocFile = null; 
		if (reprint) {
			secDocFile = DataInitializer.createDocFile("/KISHKUSH.pdf", "pdf");
			if (!primaryContent) {			
				SecondaryDocFile sec = new SecondaryDocFile();
				sec.setDocFile(secDocFile);
				docData.setSecondaryDocFile(sec);
			}
		}
		
		if (random) {
		} else {
			docData.getDocCustomerData().setDocDetails(
					DataInitializer.createFullDocDetails());
		}
		docData.getDocCustomerData().getDocDetails().setProjectId(
				LEGACY_PROJECT);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());
		// mapper will set valid sub area code from form id
		// docData.getDocCustomerData().getDocDetails().setBusinessSubAreaCode(
		// null);

		// those two are checked for validity by legacy app
		// this one is not mandatory in new service
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(scanStatus);
		// mandatory in the legacy service
		docData.getDocCustomerData().getExecutorDetails().setExecutingBankId(1);
		docData.getDocCustomerData().getExecutorDetails().setExecutingBranchId(
				1);
		docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
		docData.getDocCustomerData().getExecutorDetails()
				.setEmpIdDocumentTypeCode(1);
		docData.getDocCustomerData().getExecutorDetails().setTerminalChannelId(
				0);
		docData.getDocCustomerData().getExecutorDetails()
				.setExecutingEmpFullName("Ilan benAlon");

		// this one is mandatory in new service and is checked
		// for validity in the old one
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		// this one is mandatory in new service and is checked for validity in
		// the old
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);

		docData.getDocCustomerData().setBankAccounts(
				DataInitializer.createBankAccountList());

		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		//bankAccountList.add(DataInitializer.createBankAccount1());
		bankAccountList.add(DataInitializer.createFullBankAccount());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);
		doc.setDocData(docData);
		
		if (hearot > 0 || harigim > 0) {
			List<DocEventData> docEventData = new ArrayList<DocEventData>();
			doc.setDocEventData(docEventData);
			for (int i = 0; i < harigim; i++) {
				if (i > 0) {
					sleep(1001);
				}
				docEventData.add(LegacyDataInitializer.makeHarigCompatEvent());
			}
			for (int i = 0; i < hearot; i++) {
				if (i > 0) {
					sleep(1001);
				}
				docEventData.add(LegacyDataInitializer.makeHearaCompatEvent());
			}
		}
		
		docDataForCreateList.add(doc);

		List<DocIdData> ids = wrapper.createDocuments(
				docDataForCreateList, securityContext, "CURRENT");
		IDfId  result = null;
		for (DocIdData id : ids) {
			Assert.assertNotNull(id);
			Assert.assertNotNull(id.getDctmDocumentId());
			Assert.assertEquals(16,id.getDctmDocumentId().length());
			Assert.assertNotNull(id.getVersionLabel());
			System.out.println("Created document: "+id.getDctmDocumentId());
			result = new DfId (id.getDctmDocumentId());
		}
	
		if (null!=secDocFile && primaryContent) {
			IDfSysObject obj = (IDfSysObject) session.newObject("dm_document");
			obj.setContentType(secDocFile.getDocFormat());
			ByteArrayOutputStream os = new ByteArrayOutputStream(secDocFile.getDocStream().length);
			os.write(secDocFile.getDocStream());
			obj.setContent(os);
			obj.setObjectName("REPRINT for "+result.getId());
			obj.save();
			
			IDfRelation rel = (IDfRelation)session.newObject("dm_relation");
			rel.setRelationName("bnhp_tofes_reprint_relation");
			rel.setChildId(obj.getObjectId());
			rel.setParentId(result);
			rel.save();
		} 
		return result;
	}

	/*
	@Test
	public void createObjectServicesTestCreateValidDocWithContent()
			throws Exception {
		if (isLegacyDisabled()) {
			return;
		}
		ILegacyCreateObjectServicePerformer performer = getPerformer();
		SecurityContext securityContext = getLegacyTestSecurityContext();
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();

		DocDataForCreate doc = new DocDataForCreate();
		DocData docData = DataInitializer.createMinimalValidDocData();
		docData.setDocFile(DataInitializer.createDocFile("/HELLO.pdf", "pdf"));
		docData.getDocCustomerData().setDocDetails(
				DataInitializer.createFullDocDetails());
		docData.getDocCustomerData().getDocDetails().setProjectId(
				LEGACY_PROJECT);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());
		
		// those two are checked for validity by legacy app
		// this one is not mandatory in new service
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);
		// mandatory in the legacy service
		docData.getDocCustomerData().getExecutorDetails().setExecutingBankId(1);
		docData.getDocCustomerData().getExecutorDetails().setExecutingBranchId(
				1);
		docData.getDocCustomerData().getExecutorDetails().setBankolId(1);
		docData.getDocCustomerData().getExecutorDetails()
				.setEmpIdDocumentTypeCode(1);
		docData.getDocCustomerData().getExecutorDetails().setTerminalChannelId(
				0);
		docData.getDocCustomerData().getExecutorDetails()
				.setExecutingEmpFullName("Ilan benAlon");

		// this one is mandatory in new service and is checked
		// for validity in the old one
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		// this one is mandatory in new service and is checked for validity in
		// the old
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);

		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		bankAccountList.add(DataInitializer.createBankAccount1());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);

		doc.setDocData(docData);
		docDataForCreateList.add(doc);

		List<DocIdData> ids = performer.legCreateDocuments(
				docDataForCreateList, securityContext, null);
		for (DocIdData id : ids) {
			Assert.assertNotNull(id);
			Assert.assertNotNull(id.getDctmDocumentId());
			Assert.assertNotNull(id.getVersionLabel());
		}
	}

	@Test
	public void createObjectServicesTestCreateValidDocWithTwoContents()
			throws Exception {
		if (isLegacyDisabled()) {
			return;
		}
		ILegacyCreateObjectServicePerformer performer = getPerformer();
		SecurityContext securityContext = getLegacyTestSecurityContext();
		List<DocDataForCreate> docDataForCreateList = new ArrayList<DocDataForCreate>();

		DocDataForCreate doc = new DocDataForCreate();
		DocData docData = DataInitializer.createMinimalValidDocData();
		DocFile file = DataInitializer.createDocFile("/HELLO.pdf", "pdf");
		docData.setDocFile(file);
		SecondaryDocFile secondaryDocFile = new SecondaryDocFile();
		DocFile secFile = DataInitializer.createDocFile("/KISHKUSH.pdf", "pdf");
		secondaryDocFile.setDocFile(secFile);
		secondaryDocFile.setTemplateSourceCode(1);
		docData.setSecondaryDocFile(secondaryDocFile);
		docData.getDocCustomerData().setDocDetails(
				DataInitializer.createFullDocDetails());
		docData.getDocCustomerData().getDocDetails().setProjectId(
				LEGACY_PROJECT);
		// mapper will set valid sub area code from form id
		// docData.getDocCustomerData().getDocDetails().setBusinessSubAreaCode(
		// null);

		// those two are checked for validity by legacy app
		// this one is not mandatory in new service
		docData.getDocCustomerData().getDocDetails().setScanStatusCode(1);

		// this one is mandatory in new service and is checked
		// for validity in the old one
		docData.getDocCustomerData().getDocDetails().setDocCompletenessCode(0);
		// this one is mandatory in new service and is checked for validity in
		// the old
		docData.getDocCustomerData().getDocDetails().setOngoingOrHistoryCode(1);

		docData.getDocCustomerData().setBankAccounts(
				DataInitializer.createBankAccountList());

		ExecutorDetails executor = DataInitializer.createFullExecutorDetails();
		docData.getDocCustomerData().setExecutorDetails(executor);
		docData.setDocPropertyExtensions(DataInitializer
				.createFullDocPropertyExtensions());
		
		
		// one group only
		ArrayList<String> documentGroupIds = new ArrayList<String>();
		documentGroupIds.add("group1");
		docData.getDocCustomerData().getDocDetails().setDocumentGroupIds(
				documentGroupIds);
		// only one bank account
		List<BankAccount> bankAccountList = new ArrayList<BankAccount>();
		bankAccountList.add(DataInitializer.createBankAccount1());
		docData.getDocCustomerData().setBankAccounts(bankAccountList);
		// one group only

		doc.setDocData(docData);
		docDataForCreateList.add(doc);

		String legacyId = docData.getDocCustomerData().getDocDetails().getLegacyDocumentId();
		
		List<DocIdData> ids = performer.legCreateDocuments(
				docDataForCreateList, securityContext, null);
		Assert.assertEquals(1, ids.size());
		for (DocIdData id : ids) {
			Assert.assertNotNull(id);
			Assert.assertNotNull(id.getDctmDocumentId());
			Assert.assertNotNull(id.getVersionLabel());
			RetrieveObjectServices retrieveSvc = new RetrieveObjectServices();
			FetchTypeSet fetchTypeSet = new FetchTypeSet();
			
			FetchType fetchType = new Full();
			fetchTypeSet.setFetchType(fetchType);
			DocRetrievalFlags flags = new DocRetrievalFlags(); 
			flags.setShouldRetrieveSecondaryFile(true);
			
			DocDataForRetrieve fetchResult = retrieveSvc.retrieveDoc4LegacyDocumentId(legacyId, null, securityContext, 
					executor, fetchTypeSet, flags);
			Assert.assertEquals(id.getDctmDocumentId(), fetchResult.getDocIdData().getDctmDocumentId());
			//Assert.assertEquals(fetchResult.getContentSize(),file.getDocStream().length);
			Assert.assertEquals(fetchResult.getDocData().getDocFile().getDocStream().length,file.getDocStream().length);
			
			Assert.assertTrue(Arrays.equals(fetchResult.getDocData().getDocFile().getDocStream(),
					file.getDocStream()));
			
			
			Assert.assertNotNull(fetchResult.getDocData().getSecondaryDocFile());
			Assert.assertNotNull(fetchResult.getDocData().getSecondaryDocFile().getDocFile());
			Assert.assertNotNull(fetchResult.getDocData().getSecondaryDocFile().getDocFile().getDocStream());
			
			Assert.assertTrue(Arrays.equals(fetchResult.getDocData().getSecondaryDocFile().getDocFile().getDocStream(),
					secFile.getDocStream()));
			
			IQueryService queryService = ServiceUtils.getService(IQueryService.class, this.getTestServiceContext(securityContext));
			String count = BasicQueryUtils.getSingleStringResult("select count(*) from bnhp_tfs_peula_doc (all) where i_chronicle_id in (select i_chronicle_id  from bnhp_tfs_peula_doc where r_object_id='"+id.getDctmDocumentId()+"')", queryService,securityContext.getRepositoryName());
			Assert.assertEquals("2.0", count);
		}
	}
*/

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\migration_package.xml
-----------------------------------------------------
<?xml version="1.0"?>
<project name="migration_package" default="main" basedir=".">
  <!-- Sets variables which can later be used. -->
  <!-- The value of a property is accessed via ${} -->
  <property name="src.dir" location="bin" />
  <property name="dist.dir" location="dist" />
 
  <!--Creates the deployable jar file  -->
  <target name="jar">
    <jar destfile="migration.jar" basedir="${src.dir}">
      <!--<manifest>
        <attribute name="Main-Class" value="bnhp.infra.base.migration.Migration" />
      </manifest> -->
    </jar>
  </target>

 <target name="main" depends="jar">
    <description>Main target</description>
  </target>

</project> 


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	
	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0</version>
	</parent>
	
	<artifactId>BnhpTfsPeulaMigrBOF</artifactId>
	<packaging>jar</packaging>
	<name>BnhpTfsPeulaMigrBOF</name>
	<description>BnhpTfsPeulaMigrBOF</description>		

	
	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
	
	
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>				
		</dependency>
		
		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>						
		</dependency>
		
		<dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>build-dar</artifactId>
        	<version>1.0-SNAPSHOT</version>
        	<type>xml</type>
        	<scope>assembly</scope>
        </dependency>
        
        <dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>BnhpInfraDFServices</artifactId>
        	<type>jar</type>
        	<classifier>full</classifier>
        </dependency>
        
        <dependency>
		 	<groupId>com.poalim.documentum</groupId>
        	<artifactId>BnhpInfraDFArtifacts</artifactId>
        	<version>1.0-SNAPSHOT</version>
        	<type>jar</type>
        </dependency>
	</dependencies>
	
	
	<build>
	
		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
				<testSourceDirectory>junit</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>test/resources</directory>
			</testResource>
		</testResources>
		

		<plugins>
		    <plugin>
    			<artifactId>maven-clean-plugin</artifactId>
			    <configuration>
			      <filesets>
			        <fileset>
			          <directory>bin-dar</directory>
			          <includes>
			            <include>*.dar</include>
			          </includes>
			          <followSymlinks>false</followSymlinks>
			        </fileset>
			      </filesets>
			    </configuration>
			</plugin>
		    <plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals><goal>run</goal></goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals><goal>run</goal></goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals><goal>run</goal></goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
               <groupId>org.codehaus.mojo</groupId>
               <artifactId>build-helper-maven-plugin</artifactId>
               <executions>
                  <execution>
                    <id>attach-dar</id>
                    <phase>package</phase>
              	  </execution>
               </executions>
            </plugin>
            <plugin>
				<artifactId>maven-resources-plugin</artifactId>        
				<executions>
					<execution>
						<id>copy-resources</id>
						
						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>          
								<resource>
									<directory>target/classes</directory>									
								</resource>
							</resources>              
						</configuration>            
					</execution>
				</executions>
			</plugin>
		</plugins>
		
	</build>
	
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\bnhp\infra\base\migration\Migration.java
-----------------------------------------------------
package bnhp.infra.base.migration;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.LinkedBlockingQueue;



import com.documentum.com.DfClientX;
import com.documentum.com.IDfClientX;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfType;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfList;
import com.documentum.fc.common.IDfList;
import com.documentum.fc.common.IDfLoginInfo;


public class Migration{

	public static final long POLL_TIMEOUT = 2000;
	
	//args
	private static String m_dql = "";
	private static String m_repository = "";
	private static String m_user = "";
	private static String m_password = "";
	
	//default values
	private static int MAX_QUEUE_SIZE = 1000;
	private static int MAX_THREADS = 10;
	
	private static volatile int counterRollback = 0;
	private static long totalMigrated = 0;
	private static long totalFailed = 0;
		
	public static int MAX_COUNTER_SIZE = 100;
	
	public static LinkedBlockingQueue<String> queue = null;
	private static boolean m_isRollback = false;
	private static IDfClientX m_clientx = null;
	private static IDfClient m_client = null;
	private static IDfSessionManager m_sessionManager = null;
	private static IDfLoginInfo m_identity;
	private static long startTime;
	private static long lastProgressTime = 0L;
	private static long lastSum = 0L;
	private static String m_prioritySpec = null;
	
	public static volatile boolean failFlag=false;
	public static volatile boolean finishFlag;
	
	public String getPrioritySpec() {
		return m_prioritySpec;
	}
	
	public static boolean  run(String[] args) {
		finishFlag = false;
		if(!parseArgs(args)){
			return false;
		}
		try{
			setUp();
		}catch (Exception e) {
			e.printStackTrace();
		}
		Migration migration = new Migration();
		Migration.queue = new LinkedBlockingQueue<String>(MAX_QUEUE_SIZE);
		System.out.println("queue max size = " + MAX_QUEUE_SIZE);
		System.out.println("threads max num = " + MAX_THREADS);
		startTime = System.currentTimeMillis();
		
		boolean result = false;
		try {
			if(m_isRollback){
				result = migration.rollbackMigration();
			}else{
				result = migration.executeMigration();
			}
			return result;
		} finally{
			long elapsed = System.currentTimeMillis() - startTime;
			printElapsedTime(elapsed);
			
		}
	}
	
	
	public static void main(String [] argv) {
		System.exit(run(argv) ? ( (totalMigrated + totalFailed == 0) ? 1 : 0 ) : 2);
	}
	
	public static void setUp() throws DfException {
		if (null == m_clientx ) {
			m_clientx = new DfClientX();
		}
		if (null == m_client) {
			m_client = m_clientx.getLocalClient();
		}
		if (null == m_sessionManager) {
			m_sessionManager = m_client.newSessionManager();
		}
		if (null == m_identity) {
			m_identity = m_clientx.getLoginInfo();
			m_identity.setUser(m_user);
			m_identity.setPassword(m_password);
			m_sessionManager.setIdentity(m_repository, m_identity);
		}
	}
	
	 private static boolean parseArgs(String[] args) {
		 
		 boolean res = false;
		 for(int i = 0; i < args.length; i++){
			 if(args[i] == null || args[i].isEmpty()){
				 continue;
			 }
			 if(args[i].equals("-dql")){
				 m_dql = args[++i];
			 
			 } else if(args[i].equals("-t")){
				 MAX_THREADS = Integer.parseInt(args[++i]);
			 
			 } else if(args[i].equals("-q")){
				 MAX_QUEUE_SIZE = Integer.parseInt(args[++i]);
			 
			 } else if(args[i].equals("-docbase")){
				 m_repository = args[++i];
			 
			 } else if(args[i].equals("-user")){
				 m_user = args[++i];
			 
			 } else if(args[i].equals("-pwd")){
				 m_password = args[++i];
			 } else if(args[i].equals("-rollback")){
				 m_isRollback  = true;
			 } else if(args[i].equals("-p")){
				 m_prioritySpec = args[++i];
			 }
		 }
		
		 if(m_dql == null || m_dql.trim().length() == 0 || m_repository == null || m_repository.trim().length() == 0 ||
				 m_user == null || m_user.trim().length() == 0 || m_password == null || m_password.trim().length() == 0){
			 System.out.println("Missing mandatory arguments");
			 System.out.println("Mandatory arguments are:");
			 System.out.println("1. -dql");
			 System.out.println("2. -docbase");
			 System.out.println("3. -user");
			 System.out.println("4. -pwd");
			 System.out.println("Optional arguments are:");
			 System.out.println("5. -t threads number");
			 System.out.println("6. -q queue size");
			 System.out.println("7. -rollback");
			 System.out.println("8. -p priority_spec");
		 } else{
			 res = true;
		 }
		 
		 return res;
	}

	private static void printElapsedTime(long milliseconds){   
	       long seconds, minutes, hours;   
	       seconds = milliseconds / 1000;   
	       hours = seconds / 3600;   
	       seconds = seconds % 3600;   
	       minutes = seconds / 60;   
	       seconds = seconds % 60;   
	  
	       System.out.println(hours + " hours, " + minutes + " mins, " + seconds + " seconds");   
	  }   

	private boolean executeMigration()  {
		 failFlag = false;
		 List<IDfSession> sessionList = new ArrayList<IDfSession>();
		 List<Thread> threadList = new ArrayList<Thread>();
		 try{
			
			 //threadList.add(thr);
			 System.out.println("creating threads");
			 IDfSession session = m_sessionManager.newSession(m_repository);
			 sessionList.add(session);
			 Thread readThr = new Thread(new ReadingThread(session));
			 String name = "ReadingThread";
			 readThr.setName(name);
			 System.out.println(name+" ("+session.getConnectionConfig().getString("connection_id")+")");
			 
			 for (int i = 0; i < MAX_THREADS; i++) {
				 name = "MigrationThread"+i;
				 IDfSession newSession = m_sessionManager.newSession(m_repository);
				 sessionList.add(newSession);
				 Thread thr = new Thread(new MigrationThread(newSession, i));
				 thr.setName(name);
				 thr.start();
				 System.out.println(name+" ("+newSession.getConnectionConfig().getString("connection_id")+")");
				 threadList.add(thr);
			 }
			 System.out.println();
			 if (null != getPrioritySpec()) {
				 setProcessPriorities(sessionList, getPrioritySpec());
			 }
			 readThr.start();
			 readThr.join();
			 if (failFlag) {
				 System.out.println("Reading thread failed, waiting for queue to empty");
			 } else {
				 System.out.println("Finished reading objects, waiting for queue to empty");
			 }
			 waitQueueEmpty();
			 System.out.println("The queue is now empty, waiting for thread to exit");
			 finishFlag = true;
			 waitForAllThreads(threadList);
			 System.out.println("*** Finished all threads");
			 printProgress("migrated",true);
			 return !failFlag;

		 } catch (Exception e) {
			 System.out.println("*** Exception in main thread");
			 e.printStackTrace();
			 return false;
		 } finally{
			 for(IDfSession session: sessionList){
				 if(session != null && session.isConnected()){
					 m_sessionManager.release(session);
				 }
			 }
		 }
	}
	
	private void setProcessPriorities(List<IDfSession> sessionList,
			String prioritySpec) throws DfException {
		System.out.println("Setting server process priorities to "+ prioritySpec);
		final String METHOD_NAME = "bnhp_dctm_nice";
		IDfSession s = sessionList.get(0);
		IDfCollection col = null;
		StringBuffer argsBuf = new StringBuffer();
		argsBuf.append(prioritySpec);
		for (IDfSession curSess:  sessionList) {
			argsBuf.append(" ");
			argsBuf.append(curSess.getConnectionConfig().getString("connection_id"));
		}
		try {
			IDfList args = new DfList(IDfType.DF_STRING);
			IDfList dataType = new DfList(IDfType.DF_STRING);
			IDfList values = new DfList(IDfType.DF_STRING);
			
			args.appendString("METHOD");dataType.appendString("S");values.appendString(METHOD_NAME);
			args.appendString("ARGUMENTS");dataType.appendString("S");values.appendString(argsBuf.toString());
			args.appendString("LAUNCH_ASYNC");dataType.appendString("B");values.appendString("FALSE");
			
			col = s.apply(null, "DO_METHOD", args , dataType, values);
			Boolean launchFailed = null;
            while(col.next()) {
            	launchFailed = col.getBoolean("launch_failed");
            }
			if (null==launchFailed || launchFailed) {
				System.out.println("failed to launch "+METHOD_NAME+" method");
			} else {
				System.out.println(METHOD_NAME+ " method succeeded");
			}
		} finally {
			if (null!=col) {
				try {
					col.close();
				} catch (DfException e) {
					System.out.println("WARN: failed to colse collection: "+e.getMessage());
				}
			}
		}
		
	}

	private void interruptAllThreads(List<Thread> threadList) {
		boolean hasActiveThreads = false;
		do {
			hasActiveThreads = false;
			for (Thread th : threadList) {
				if (th.isAlive()) {
					System.out.println("Interrupting thread "+th.getName());
					th.interrupt();
					hasActiveThreads = true;
				}
			}
			try {Thread.sleep(1000);} catch (InterruptedException iex) {} 				
		} while (hasActiveThreads);
	}

	private void waitForAllThreads(List<Thread> threadList) {
		int hasActiveThreads;
		do {
			hasActiveThreads = 0;
			for (Thread th : threadList) {
				if (th.isAlive()) {
					hasActiveThreads++;
				}
			}
			if (hasActiveThreads>0) {
				System.out.println(""+hasActiveThreads + " threads still active");
				try {Thread.sleep(POLL_TIMEOUT*2);} catch (InterruptedException iex) {
					interruptAllThreads(threadList);
				} 
			}
		} while (hasActiveThreads > 0);
	}

	
	private void waitQueueEmpty() {
		do {
			try {
				Thread.sleep(500);
			} catch (InterruptedException iex) {
				System.out.println("wait queue empty interrupted");
				break;
			}
		} while (!queue.isEmpty());
	}

	private boolean rollbackMigration()  {
		
		// SecurityContext sctx = this.getLegacyTestSecurityContext();
		 List<IDfSession> sessionList = new ArrayList<IDfSession>();
		 List<Thread> threadList = new ArrayList<Thread>();
		 try{
			 IDfSession session = m_sessionManager.newSession(m_repository);
			 sessionList.add(session);
			 Thread readThr = new Thread(new ReadingThread(session));
			 readThr.setName("ReadingThread");
			 Thread thr = new Thread(new ReadingThread(session));
		
			 threadList.add(thr);
		 
			 System.out.println("creating threads");
			 for (int i = 0; i < MAX_THREADS; i++) {
				 System.out.print(i+" ");
				 IDfSession newSession = m_sessionManager.newSession(m_repository);
				 sessionList.add(newSession);
				 thr = new Thread(new RollbackThread(newSession, i));
				 thr.setName("RollBackThread"+i);
				 thr.start();
				 threadList.add(thr);
			 }
			 readThr.start();
			 readThr.join();
			 System.out.println("Finished reading objects, waiting for queue to empty");
			 waitQueueEmpty();
			 System.out.println("The queue is now empty, waiting for thread to exit");
			 finishFlag = true;
			 waitForAllThreads(threadList);
			 System.out.println("*** Finished all threads");
			 printProgress("rolled back",true);
			 return true;
		 }catch(Exception ex){
			 System.out.println("*** Exception in main thread");
			 ex.printStackTrace();
			 return false;
		 }finally{
			 for(IDfSession session: sessionList){
				 if(session != null && session.isConnected()){
					 m_sessionManager.release(session);
				 }
			 }
		 }
	}

	
	
	public static void resetRollbackCounter() {
		counterRollback = 0;
	}
	
	public static int getThreadsNum() {
		return MAX_THREADS;
	}

	
	
	public synchronized static long getTotalFailed() {
		return totalFailed;
	}

	public synchronized static void increaseTotalFailed() {
		Migration.totalFailed ++;
	}

	public synchronized static void  increaseTotalMigrated() {
		Migration.totalMigrated ++;
	}

	public synchronized static long  getTotalMigrated() {
		return totalMigrated;
	}
	
	public static String getDql() {
		return m_dql;
	}

	public static void setTotalMigrated(long totalMigrated) {
		Migration.totalMigrated = totalMigrated;
	}

	public static void setTotalFailed(long totalFailed) {
		Migration.totalFailed = totalFailed;
	}
	
	public static void resetAllArgs(){
		m_repository = "";
		m_password = "";
		m_user = "";
		m_dql = "";
		m_isRollback = false;
		MAX_QUEUE_SIZE = 1000;
		MAX_THREADS = 10;
	}

	
	public synchronized static int getRollbackCounter() {
		return counterRollback;
	}
	
	public static void resetCounters(){
		totalMigrated = 0;
		totalFailed = 0;
	}

	public static void printProgress(String name) {
		printProgress(name, false);
	}
		
	public static void printProgress(String name, boolean force) {
		long aMigrated = Migration.getTotalMigrated();
		long aFailed = Migration.getTotalFailed();
		long sum = aMigrated + aFailed;
		long time = System.currentTimeMillis();
		if (sum % 100 == 0 || force) {
			long tdiff = time - lastProgressTime;
			long ddiff = sum - lastSum;
			String cperf = "0";
			if (lastProgressTime > 0) {
				cperf = String.format("%.2f",((double) ddiff) / (((double)tdiff) / 1000));
			}
			lastSum= sum;
			lastProgressTime = time;
			long secs = (time - startTime ) / 1000;
			long perf = (sum) / ( 0 == secs ? 1 : secs);
			System.out.println(name+"=" + aMigrated+	", failed=" + aFailed+", Cur. perf.= "+ cperf +" doc/s, Total perf.="+perf+" doc/s" + ", que. depth="+queue.size());
		}
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\bnhp\infra\base\migration\MigrationThread.java
-----------------------------------------------------
package bnhp.infra.base.migration;

import java.util.concurrent.TimeUnit;

import bnhp.infra.base.tbo.ISyncObject;

import com.documentum.fc.client.DfSysObject;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.IDfTypedObject;
import com.documentum.fc.client.tfspeula.tbo.ITfsPeulaDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;

public class MigrationThread implements Runnable{

	private IDfSession session = null;
	private int id = -1;
	private int serverPID = 0;

		
	public MigrationThread(IDfSession session, int i) {
		super();
		this.session = session;
		id = i;
	}
	
	@Override
	public void run() {
		String objId = null;
		try {
			while(!Thread.currentThread().isInterrupted() && !Migration.finishFlag){
				objId = Migration.queue.poll(Migration.POLL_TIMEOUT, TimeUnit.MILLISECONDS);
				if (null == objId) {
					continue;
				}
				try{
					ISyncObject  obj = (ISyncObject) session.getObject(new DfId(objId));
					obj.setDirty(true);
					((IDfSysObject)obj).save();
					//System.out.println("MigrationThread" + id + ": Created " + ((DfSysObject)obj).getObjectId().getId());
					Migration.increaseTotalMigrated();
				} catch(Exception ex){
					Migration.increaseTotalFailed();
					System.out.println("MigrationThread" + id + "failed to migrate object: "+objId+" : Exception "+ex.getMessage());
					ex.printStackTrace();
				}
				Migration.printProgress("migrated");
			}
			System.out.println("MigrationThread" + id + ": finished");
			return;
		} catch (InterruptedException e) {
			System.out.println("MigrationThread" + id + ": interrupted exception");
			Thread.currentThread().interrupt();
		}
		finally { 
			System.out.println("MigrationThread" + id + ": exit");
		}
	} 

}




file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\bnhp\infra\base\migration\ReadingThread.java
-----------------------------------------------------
package bnhp.infra.base.migration;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

import bnhp.infra.dfs.model.business.SecurityContext;
import bnhp.infra.dfs.utils.dfc.DfcUtils;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.common.DfException;
import com.emc.documentum.fs.rt.ServiceException;

public class ReadingThread implements Runnable{

	private SecurityContext sctx= null;
	private IDfSession session = null;
	
//	public ReadingThread(SecurityContext sctx) {
//		super();
//		this.sctx = sctx;
//	}
	
	public ReadingThread(IDfSession session) {
		super();
		this.session = session;
	}

//	@Override
//	public void run() {
//		List<IDfSession> sessionList = new ArrayList<IDfSession>();
//		IDfSessionManager sm  = null;
//		try{
//			sm = DfcUtils.getSessionManager(sctx.getRepositoryName(), sctx);
//		}catch (ServiceException e) {
//			System.out.println("Couldn't create session manager.");
//			e.printStackTrace();
//		}
//		try {
//				// create a pool of threads
//			ExecutorService threadPool = Executors.newFixedThreadPool(Migration.getThreadsNum());
//			String dql = Migration.getDql();
//			IDfQuery query = new DfQuery();
//			query.setDQL(dql);
//			IDfCollection col = null;
//
//			System.out.println("dql = " + dql);
//			IDfSession session = sm.newSession(sctx.getRepositoryName());
//			col = query.execute(session, 0);
//
//			List<String> list = null;
//			int ind = 0;
//			
//			while (col != null && col.next()) {
//				if(list == null){
//					list = new ArrayList<String>(Migration.MAX_LIST_SIZE);
//				}
//				try {
//					if (col.getString("r_object_id") != null) {
//						synchronized (this) {
//							
//							list.add(col.getString("r_object_id"));
//							if(list.size() == Migration.MAX_LIST_SIZE){
//								System.out.println("Creating migration thread " + ind);
//								IDfSession newSession = sm.newSession(sctx.getRepositoryName());
//								sessionList.add(newSession);
//								MigrationThread th = new MigrationThread(newSession, list, ind++);
//								threadPool.execute(th);
//								list = null;
//							}
//						}
//					}
//				} catch (DfException ex) {	
//					Migration.increaseTotalFailed();
//				}
//			}
//			if(list != null && list.size() != 0){
//				IDfSession newSession = sm.newSession(sctx.getRepositoryName());
//				sessionList.add(newSession);
//				MigrationThread th = new MigrationThread(newSession, list, ind++);
//				threadPool.execute(th);
//				list = null;
//			}
//			threadPool.shutdown();        
//			while (!threadPool.isTerminated()) { }        
//			System.out.println("Finished all threads");
//			System.out.println("Total migrated = " + Migration.getTotalMigrated());
//			System.out.println("Total failed = " + Migration.getTotalFailed());
//			return;
//		
//		} catch (DfException e) {
//			// TODO Auto-generated catch block
//			e.printStackTrace();
//		}
//		
//		finally {
//			for( int i = 0; i < sessionList.size() ; i++){
//				if (null != sm && null != sessionList.get(i) ) {
//					sm.release(sessionList.get(i));
//				}
//			}				
//		}
//	}

	@Override
	public void run() {
		IDfCollection col = null;
		try {
			String dql = Migration.getDql();
			IDfQuery query = new DfQuery();
			query.setDQL(dql);
			
			
			System.out.println("dql = " + dql);
			
			col = query.execute(session, 0);

			String objId = null;
			while (col != null && col.next()) {
				try {
					objId = col.getString("r_object_id");
					
						if (objId != null) {
							try{
								Migration.queue.put(objId);
								//System.out.println("Inserted to queue tfs_peula id=" +  col.getString("r_object_id"));
							}catch (InterruptedException e) {
							    throw new RuntimeException("add to queue interrupted");
							 }
						}
					
				} catch (DfException ex) {	
					Migration.increaseTotalFailed();
				}
			}
			return;

		} catch (DfException e) {
			Migration.failFlag = true;
			e.printStackTrace();
		} finally {
			if (null!=col) {
				try {col.close();} catch (DfException dfex) {
					System.out.println("failed to close collection: "+dfex.getMessage());
				}
			}
		}
		
	}

}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\bnhp\infra\base\migration\RollbackThread.java
-----------------------------------------------------
package bnhp.infra.base.migration;

import java.util.List;
import java.util.concurrent.TimeUnit;

import bnhp.infra.base.tbo.ISyncObject;

import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.DfServiceException;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfSession;
import com.documentum.fc.client.IDfSessionManager;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.client.customerdoc.tbo.ICustomerDoc;
import com.documentum.fc.client.tfspeula.tbo.ITfsPeulaDoc;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.DfUtil;
import com.documentum.fc.common.IDfId;
import com.documentum.operations.impl.common.GetObject;

public class RollbackThread implements Runnable{

	private IDfSession session = null;
	private int id = -1;
	
	public RollbackThread(IDfSession session, int i) {
		super();
		this.session = session;
		id = i;
	}
	
	@Override
	public void run() {
		
		String objId = null;
		try {
			while(!Thread.currentThread().isInterrupted() && !Migration.finishFlag) {
				objId = Migration.queue.poll(1000, TimeUnit.MILLISECONDS);
				if (null == objId) {
					continue;
				}
				try{
					
					ITfsPeulaDoc  obj = (ITfsPeulaDoc) session.getObject(new DfId(objId));
					String customerDocId = obj.getString("a_status");
					
					if(customerDocId != null && customerDocId.length() != 0){
						deleteMigrationForThisObj(obj, customerDocId);
					}
				
					Migration.increaseTotalMigrated();
				} catch(Exception ex){
					ex.printStackTrace();
					Migration.increaseTotalFailed();
				}
				Migration.printProgress("rolled back");
			}
			return;
		} catch (InterruptedException e) {
			System.out.println("MigrationThread" + id + ": interrupted exception");
			Thread.currentThread().interrupt();
		}
		finally { 
			System.out.println("RollbackThread" + id + ": exit");
      } 

	}

	private void deleteMigrationForThisObj(ITfsPeulaDoc obj, String customerDocId) throws Exception {
		boolean isTransOpened = false;
		
		//IDfSessionManager mgr = session.getSessionManager();
		
		try{
			if(!session.isTransactionActive()){
				session.beginTrans();
				isTransOpened = true;
			}
			deleteCustomerDoc(customerDocId);
			eraseStatus(obj);
			obj.save();
		} catch (Exception ex) {
			if (isTransOpened && session.isTransactionActive()) {
				session.abortTrans();
			}
			throw ex;
		} finally{
			if (isTransOpened && session.isTransactionActive()) {
				session.commitTrans();
			}
		}
	}

	private void eraseStatus(ITfsPeulaDoc obj) throws DfException {
		IDfCollection col = null;
		try {
			col = obj.getVersions(null);
			while(col.next()){
				IDfId objectId = col.getId("r_object_id");
				IDfSysObject sysObj = null;
				try {
					sysObj = (IDfSysObject) session.getObject(objectId);
					boolean isImmutable = sysObj.isImmutable();
					if(isImmutable){
						sysObj.setBoolean("r_immutable_flag", false);
					}
					sysObj.setString("a_status", null);
					((ISyncObject)sysObj).disableSync();
					sysObj.save();
					if(isImmutable){
						sysObj.setBoolean("r_immutable_flag", true);
						sysObj.save();
					}
					eraseYomanStatuses(sysObj);
				} finally {
					((ISyncObject)sysObj).enableSync();
				}
			}
		} finally {
			if (null!=col) {
				col.close();
			}
		}
	}

	private void eraseYomanStatuses(IDfSysObject tfsPeulaObj) throws DfException {
		String dql = "select r_object_id from bnhp_yoman_eruim_table where doc_key='" +
		DfUtil.escapeQuotedString(tfsPeulaObj.getString("doc_key"))+ "' and a_status<>' '";
		IDfQuery query = new DfQuery();
		query.setDQL(dql);
		IDfCollection col = null;
		try {
			col = query.execute(session, 0);
			while(col.next()){
				IDfId evId = col.getId("r_object_id");
				if (null == evId || evId.isNull() || !evId.isObjectId() ) {
					continue;
				}
				IDfPersistentObject obj = null;
				try {
					obj = (IDfPersistentObject) session.getObject(evId);
					obj.setString("a_status", null);
					((ISyncObject)obj).disableSync();
					obj.save();
				} catch (DfException ex) {
					System.out.println("MigrationThread" + id + ": failed to get erase status of an eruim entry "+ evId.getId());
				} finally {
					if (null!=obj) {
						((ISyncObject)obj).enableSync();
					}
				}
				
			}
		}  finally  {
			if (null!=col) {
				col.close();
			}
		}
	}

	private void deleteCustomerDoc(String customerDocId) throws DfException {
		
		IDfSysObject obj = (IDfSysObject) session.getObject(new DfId(customerDocId));
		
		deleteEvents(obj.getChronicleId().getId());
		
		obj.destroyAllVersions();
	}

	private void deleteEvents(String customerDocId) throws DfException {
		String dql = "select ev.r_object_id  as r_object_id from bnhp_doc_event ev, dm_relation rel where " +
		"rel.parent_id ='" + customerDocId + "' and rel.child_id=ev.r_object_id";

		IDfQuery query = new DfQuery();
		query.setDQL(dql);
		IDfCollection col = null;
		try {
			col = query.execute(session, 0);
		
			while(col.next()){
				IDfId evId = col.getId("r_object_id");
				if (null == evId || evId.isNull() || !evId.isObjectId() ) {
					continue;
				}
				IDfSysObject obj = null;
				try {
					obj = (IDfSysObject) session.getObject(evId);
				} catch (DfException ex) {
					System.out.println("MigrationThread" + id + ": failed to find event "+ evId.getId());
				}
				obj.destroyAllVersions();
			}
		}  finally  {
			if (null!=col) {
				col.close();
			}
		}
	}
}



file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\com\documentum\fc\client\tfspeula\tbo\ITfsPeulaDoc.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;

import bnhp.infra.base.tbo.ISyncObject;

import com.documentum.fc.client.IDfDocument;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

public interface ITfsPeulaDoc extends IDfDocument , ISyncObject {
	public static final String GLOBAL_SYNC_DISABLED_PROP = "SYNC_DISABLED_TFS_PEULA_TO_CUSTOMER_DOC";
	public IDfId getLinkToCopy() throws DfException;
	
	/*public boolean isSyncDisabled() ;
	public void disableSync() ;
	public void enableSync() ;
	public IDfId doSyncRecursive() throws DfException ;
	
	
	public boolean isSyncGloballyDisabled()  throws DfException;
	public boolean hasBeenSynced() throws DfException;
	public void setDirty(boolean value) throws DfException;*/
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\com\documentum\fc\client\tfspeula\tbo\ITfsPeulaErua.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;

import bnhp.infra.base.tbo.ISyncObject;

import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.IDfId;

public interface ITfsPeulaErua extends IDfPersistentObject, ISyncObject {
	public static final String GLOBAL_SYNC_DISABLED_PROP = "SYNC_DISABLED_TFS_PEULA_TO_CUSTOMER_DOC";
	
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\com\documentum\fc\client\tfspeula\tbo\TfsPeulaDoc.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;



import java.lang.reflect.Field;

import java.lang.reflect.InvocationTargetException;

import java.lang.reflect.Method;



import bnhp.infra.base.sbo.IMigrationConfigService;

import bnhp.infra.base.tbo.ISyncObject;



import com.documentum.fc.client.DfClient;

import com.documentum.fc.client.DfDocument;

import com.documentum.fc.client.DfQuery;

import com.documentum.fc.client.DfSysObject;

import com.documentum.fc.client.IDfClient;

import com.documentum.fc.client.IDfCollection;

import com.documentum.fc.client.IDfFolder;

import com.documentum.fc.client.IDfPersistentObject;

import com.documentum.fc.client.IDfQuery;

import com.documentum.fc.client.IDfRelation;

import com.documentum.fc.client.IDfSysObject;

import com.documentum.fc.client.IDfVersionLabels;

import com.documentum.fc.client.aspect.IDfAspects;

import com.documentum.fc.client.content.impl.ContentManager;

import com.documentum.fc.common.DfException;

import com.documentum.fc.common.DfId;

import com.documentum.fc.common.DfList;

import com.documentum.fc.common.DfLogger;

import com.documentum.fc.common.DfUtil;

import com.documentum.fc.common.IDfAttr;

import com.documentum.fc.common.IDfId;

import com.documentum.fc.common.IDfList;

import com.documentum.fc.common.IDfTime;



public class TfsPeulaDoc extends DfDocument implements ITfsPeulaDoc {

	private static final String CUSTOMER_DOC_TYPE = "bnhp_customer_doc";

	private static final String CUSTOMER_DOC_EVENT_TYPE = "bnhp_doc_event";

	private static final String TFS_PEULA_YOMAN_TYPE = "bnhp_yoman_eruim_table";

	private static final String PAPER_DOC_ASPECT_NAME = "bnhp_paper_doc";

	

	private static final String TFS_PEULA_REPRINT_RELATION = "bnhp_tofes_reprint_relation"; 

	private static final String CUSTOMER_DOC_REPRINT_RELATION = "bnhp_cust_rel";

	private static final String CUSTOMER_DOC_REPRINT_RELATION_SUBTYPE = "bnhp_cust_rel_temp";

	

	private static final int SCAN_EVENT_CATEGORY = 301;	

	

	private static final String[][] docPaperDocMapping = {

		{"sent_date", "bnhp_paper_doc.archive_date"},

		{"original_box", "bnhp_paper_doc.bank_archive_id"},

		{"box", "bnhp_paper_doc.archive_box_nbr"},

		{"portion", "bnhp_paper_doc.box_batch_nbr"},

		{"portion_inner_num", "bnhp_paper_doc.box_doc_serial_nbr"},

		{"page_count", "bnhp_paper_doc.document_page_cnt"},

		{"physical_location", "bnhp_paper_doc.doc_location_code"},

		{"expiration_date", "bnhp_paper_doc.paper_destruction_dttm"},

		{"barcode", "bnhp_paper_doc.bar_code"}

	};



	private static final String[][] scanEvAttrMatchMapping = {

		{"scan_status", "event_category_status_code"},

		{"scan_date", "legacy_event_entry_dttm"},

		{"mpr_zihuy_pakid_sorek", "executing_emp_id_code"}

	};



	

	private static final String[][] scanEvAttrMapping = {

		{"scan_status", "event_category_status_code"},

		{"scan_date", "legacy_event_entry_dttm"},

		{"mpr_zihuy_pakid_sorek", "executing_emp_id_code"},

		{"doc_key", "dctm_document_id"},

		{"mispar_tachana", "terminal_channel_id"},

		{"mispar_bankol", "bankol_id"},

		{"kod_arutz", "channel_id"}

	};

	

	private static final String[][] docAccountAttrMapping = {

		{"mispar_bank", "account_bank_id"},

		{"mispar_snif", "branch_id"},

		{"mispar_cheshbon", "account_nbr"},

		{"kod_cheshbon_mutzpan", "special_handling_code"},

		{"mispar_hativa", "division_id"}

	};

	

	private static final String[][] docCustomerAttrMapping = {

		{"sifrur_lakoach", "customer_id"},

		{"sug_mismach_mezahe_lak", "customer_id_doc_type_code"},

		{"mpr_zihuy_lakoach", "complete_customer_id_code"},

		{"mpr_siduri_lakoach", "customer_serial_nbr"},

		{"shem_lakoach", "customer_full_name"},

	};

	

	private static final String[][] docAttrMapping = {

		{"object_name", "object_name"},

		{"mispar_kupat_gemel", "pension_fund_nbr"},

		{"mispar_amit", "planholder_number"},

		{"doc_key", "legacy_document_id"},

		{"app_id", "system_code"},

		{"doc_type", "business_area_code"},

		{"doc_sub_type", "business_sub_area_code"},

		{"project_id", "project_id"},

		{"shotef_or_history", "ongoing_or_history_code"},

		{"mispar_tofes", "document_form_id"},

		{"mispar_mahadura", "document_edition_nbr"},

		{"reprint_exists", "template_data_exists_ind"},

		{"co_date", "legacy_document_entry_dttm"},

		{"process_id", "document_group_id"},

		{"incomplete", "doc_completeness_code"},

		{"kod_arutz", "channel_id"},

		{"schum_iska", "transaction_amt"},

		{"sug_matbea", "currency_code"},

		{"doc_status", "signature_status_code"},

		{"scan_status", "scan_status_code"},

		{"bank_mevatzea", "executing_bank_id"},

		{"snif_mevatzea", "executing_branch_id"},

		{"sug_mismach_mezahe_pak", "emp_id_document_type_code"},

		{"mpr_zihuy_pakid_mevatzea", "executing_emp_id_code"},

		{"tachana_ip_addrs", "ip_address"},

		{"mispar_tachana", "terminal_channel_id"},

		{"mispar_bankol", "bankol_id"},

		{"shem_pakid_mevatzea", "executing_emp_full_name"},

		{"kod_ofen_kabalat_horaa","instruction_rcv_type_code"},

		{"release_num","doc_delivery_num"}

	};

	

	protected void linkCopyToProjectFolder(IDfSysObject copyObj) throws DfException {

		int projectId = this.getInt("project_id");

		if (projectId <=0) {

			DfLogger.warn(this, "document with invalid projectId: "+projectId, null, null);

			return;

		}

		IDfFolder targetFolder = (IDfFolder)getSession().getObjectByQualification("bnhp_doc_folder WHERE project_id="+projectId+" AND document_type='"+copyObj.getTypeName()+"'");

		if (null==targetFolder) {

			DfLogger.error(this, "Cannot find project folder for project id="+projectId+" type "+copyObj.getTypeName(), null, null);

			throw new DfException("Cannot find project folder for project id="+projectId+" type "+copyObj.getTypeName());

		}

		String linkPath = targetFolder.getFolderPath(0);

		DfLogger.debug(this, "linking object to folder "+linkPath, null, null);

		copyObj.link(linkPath);

		DfLogger.debug(this, "linked OK ", null, null);

	}

	

	protected boolean isSyncDisabled;

	

	@Override

	protected void doSave(boolean arg0, String arg1, Object[] arg2)

			throws DfException {

		DfLogger.debug(this, getLogId()+ ": entered doSave() on bnhp_tfs_peula_doc "+this.getObjectId().getId(), null,null);

		boolean openedTransaction = false;

		boolean success = false;

		

		boolean isSyncDisabled = false;

		if (isSyncDisabled()) {

			isSyncDisabled = true;

			DfLogger.debug(this, getLogId()+ ": sync disabled locally", null,null);

		}

		if (this.isSyncGloballyDisabled()) {

			isSyncDisabled = true;

			DfLogger.debug(this, getLogId()+ ": sync disabled globally", null,null);

		}

		if (!isDirty()){

			isSyncDisabled = true;

			DfLogger.debug(this, getLogId()+ ": sync disabled since is not dirty", null,null);

		}

		int projectId = getInt("project_id");

		if ((1 != projectId) && (2!=projectId)) {

			isSyncDisabled = true;

			DfLogger.debug(this, getLogId()+ ": sync disabled since project_id="+projectId, null,null);

		}

		if (isSyncDisabled) {

			DfLogger.debug(this, getLogId()+ ": doing super.doSave() ", null,null);

			super.doSave(arg0, arg1, arg2);

			DfLogger.debug(this, getLogId()+ ": super.doSave() finished, all done", null,null);

			return;

		}

		

		try {

			if (!(this.getSessionManager().isTransactionActive() ||

					this.getSession().isTransactionActive())) {

				openedTransaction = true;

				DfLogger.debug(this, getLogId()+ ": opening transaction", null,null);

				this.getSession().beginTrans();

			}

			//if (this.isNew() && this.getContentSize() >0) {

			//	super.doSave(arg0, arg1, arg2);

			//}

			this.doSyncRecursive();

			DfLogger.debug(this, getLogId()+": check/syncing events", null,null);

			checkCopyEvents();

			super.doSave(arg0, arg1, arg2);

			success = true;

		}  finally {

			if (success) {

				DfLogger.debug(this, getLogId()+ ":  syncronization succeeded", null,null);

			} else {

				DfLogger.debug(this, getLogId()+ ": syncronization failed, look for exceptions in log", null,null);

			}

			if (openedTransaction && this.getSession().isTransactionActive()) {

				if (success) {

					DfLogger.debug(this, getLogId()+ ": commiting transaction", null,null);

					this.getSession().commitTrans();

					DfLogger.debug(this, getLogId()+ ": transaction commited", null,null);

				} else {

					DfLogger.debug(this, getLogId()+ ": aborting transaction transaction", null,null);

					this.getSession().abortTrans();

					DfLogger.debug(this, getLogId()+ ": transaction aborted", null,null);

				}

			}

		}

	}



	@Override

	public void disableSync() {

		isSyncDisabled = true;

	}



	@Override

	public void enableSync() {

		isSyncDisabled = false;

	}



	@Override

	public boolean isSyncDisabled() {

		return this.isSyncDisabled;

	}



	@Override

	public boolean isSyncGloballyDisabled() throws DfException {

		String propVal = System.getProperty(GLOBAL_SYNC_DISABLED_PROP); 

		if ("true".equalsIgnoreCase(propVal)) {

			return true;

		}

		IDfClient client = DfClient.getLocalClient();

		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client

				.newService(IMigrationConfigService.class.getName(),this.getSessionManager());

		return !migrationConfigService.isSyncOldToNewEnabled(this.getSession());

	}

	

	

	public void disableObjSync(IDfPersistentObject copy) {

		if (copy instanceof ISyncObject) {

			((ISyncObject) copy).disableSync();

		}

	}

	

	public void enableObjSync(IDfPersistentObject copy) {

		if (copy instanceof ISyncObject) {

			((ISyncObject) copy).enableSync();

		}

	}

	

	protected String getLogId() {

		return "bnhp_tfs_peula_doc "+this.getObjectId().getId();

	}

	

	/*

	 * returns shadow object

	 * 

	 * @see bnhp.tfspeula.tbo.ITfsPeulaDoc#doSyncRecursive()

	 */

	@Override

	public IDfId doSyncRecursive() throws DfException {

		DfLogger.debug(this, getLogId()+": entered doSyncRecursive()", null,null);

		IDfId  copyId = null;

		boolean success = false;

		try {

			copyId = this.getLinkToCopy();

			IDfId copyChronicleId ;

			if (null!= copyId) {

				DfLogger.debug(this, getLogId()+": fetching copy object "+copyId.getId(), null,null);				

				IDfSysObject copy = (IDfSysObject) this.getSession().getObject(copyId);

				try {

					copyChronicleId = copy.getChronicleId();

					DfLogger.debug(this, getLogId()+": updating copy object "+copyId.getId()+ " cronicleId is "+copyChronicleId, null,null);

					disableObjSync(copy);

					boolean isCopyImmutable= copy.isImmutable();

					if (isCopyImmutable) {

						setImmutable(copy, false);

					}

					copyDataToObject(copy);

					DfLogger.debug(this, getLogId()+": saving copy object "+copyId.getId(), null,null);

					copy.save();

					DfLogger.debug(this, getLogId()+": copy object "+copyId.getId()+" saved", null,null);

					if (isCopyImmutable) {

						DfLogger.debug(this, getLogId()+": setting back immutable on copy "+copyId.getId(), null,null);

						setImmutable(copy, true);

						copy.save();

					}

				} catch (DfException dfex) {

					DfLogger.error(this, getLogId()+": failed to save copy object: "+dfex.getMessage(), null,null);

					copy.revert();

					throw dfex;

				} finally {

					enableObjSync(copy);

				}

			} else {

				DfLogger.debug(this, getLogId()+": there is still no copy object", null,null);

				if (this.getObjectId().equals(getChronicleId())) {

					DfLogger.debug(this, getLogId()+": we are root of the chain, creating copy object", null,null);

					// we're root of the chain

					IDfSysObject obj = (IDfSysObject) this.getSession().newObject(CUSTOMER_DOC_TYPE);

					DfLogger.debug(this, getLogId()+": created copy object + "+obj.getObjectId().getId(), null,null);

					copyChronicleId = obj.getObjectId();

					disableObjSync(obj);

					this.linkCopyToProjectFolder(obj);

					copyDataToObject(obj);

					obj.setString("a_status", this.getObjectId().getId());

					//System.err.print(obj.dump());

					DfLogger.debug(this, getLogId()+": saving copy object + "+obj.getObjectId().getId(), null,null);

					obj.save();

					DfLogger.debug(this, getLogId()+": copy object + "+obj.getObjectId().getId()+" saved", null,null);

					copyId = obj.getObjectId();

					enableObjSync(obj);

				} else  {

					DfLogger.debug(this, getLogId()+": we are on the tail of the chain, recursing up the chain", null,null);

					IDfId parentId = this.getAntecedentId();

					DfLogger.debug(this, getLogId()+": we are on the tail of the chain, recursing to parent "+parentId.getId(), null,null);

					ITfsPeulaDoc parent = (ITfsPeulaDoc)this.getSession().getObject(parentId);

					DfLogger.debug(this, getLogId()+": fetched parent object"+parentId.getId(), null,null);					

					boolean isImmutable= parent.isImmutable();

					

					IDfId parentCopyId;

					try {

						parent.disableSync();

						if (isImmutable) {

							DfLogger.debug(this, getLogId()+": clearing immutable flag on parent object"+parentId.getId(), null,null);

							setImmutable(parent, false);

							DfLogger.debug(this, getLogId()+": calling save after immutable on object "+parentId.getId(), null,null);

							parent.save();

							DfLogger.debug(this, getLogId()+": save done on object "+parentId.getId(), null,null);

						}

						

						DfLogger.debug(this, getLogId()+": calling do recursive on parent "+parentId.getId(), null,null);

						parentCopyId = parent.doSyncRecursive();

						DfLogger.debug(this, getLogId()+": id of parent copy is "+parentCopyId.getId(), null,null);

						DfLogger.debug(this, getLogId()+": saving parent "+parentId.getId(), null,null);

						parent.save();

						if (isImmutable) {

							DfLogger.debug(this, getLogId()+": setting back immutable on  parent "+parentId.getId(), null,null);

							setImmutable(parent, true);

							parent.save();

						}

						DfLogger.debug(this, getLogId()+": saved parent "+parentId.getId(), null,null);

					} finally {

						parent.enableSync();

					}

					DfLogger.debug(this, getLogId()+": checing out parent copy "+parentCopyId.getId(), null,null);

					IDfSysObject parentCopy = (IDfSysObject) this.getSession().getObject(parentCopyId);

					copyChronicleId = parentCopy.getChronicleId();

					DfLogger.debug(this, getLogId()+": fetched parent copy "+parentCopyId.getId()+" chrinicle id is "+copyChronicleId , null,null);

					disableObjSync(parentCopy);

					parentCopy.checkout();

					DfLogger.debug(this, getLogId()+": checout complete "+parentCopyId.getId(), null,null);

					copyDataToObject(parentCopy);

					parentCopy.setString("a_status",this.getObjectId().getId());

					DfLogger.debug(this, getLogId()+": doing checkin "+parentCopyId.getId(), null,null);

					copyId = parentCopy.checkin(false, makeCopyVersionLabels(this.getVersionLabels()));

					DfLogger.debug(this, getLogId()+": checkin complete"+parentCopyId.getId(), null,null);

					enableObjSync(parentCopy);

				}

				setLinkToCopy(copyId);

			}

			DfLogger.debug(this, getLogId()+": check/syncing reprint", null,null);

			syncReprint(copyChronicleId);

			//DfLogger.debug(this, getLogId()+": check/syncing events", null,null);

			//checkCopyEvents();

			success = true;

			return copyId;

		} finally {

			DfLogger.debug(this, getLogId()+": exited doSyncRecursive(): success="+success +" result="+copyId, null,null);

		}

		

	}



	protected void syncReprint(IDfId copyCronicleId) throws DfException {

		DfLogger.debug(this, getLogId()+"entered syncReprint(), copyChronicleId is+"+copyCronicleId.getId(), null,null);

		IDfRelation rel = (IDfRelation) getSession().getObjectByQualification(

				"dm_relation where relation_name='"+

				TFS_PEULA_REPRINT_RELATION +"' and parent_id='"+this.getChronicleId().getId()+"'");

		if (null == rel) {

			DfLogger.debug(this, getLogId()+": no reprints found", null,null);

			return;

		}

		DfLogger.debug(this, getLogId()+": reprint relation found: "+rel.getObjectId().getId(), null,null);

		String descr = rel.getDescription();

		if (null==descr || descr.trim().length() == 0) {

			IDfRelation copyRel = (IDfRelation) getSession().getObjectByQualification(

					"dm_relation where relation_name='"+

					CUSTOMER_DOC_REPRINT_RELATION +"' and parent_id='"+copyCronicleId.getId()+

					"' AND child_id='"+rel.getChildId()+"'");

			if (null == copyRel) {

				DfLogger.debug(this, getLogId()+": creating copy relation", null,null);

				copyRel = (IDfRelation) getSession().newObject("dm_relation");

				DfLogger.debug(this, getLogId()+": copy relation id is "+copyRel.getObjectId().getId(), null,null);

				copyRel.setRelationName(CUSTOMER_DOC_REPRINT_RELATION);

				copyRel.setChildId(rel.getChildId());

				copyRel.setParentId(copyCronicleId);

			}  else {

				DfLogger.debug(this, getLogId()+": found copy relation "+copyRel.getObjectId().getId(), null,null);

			}

			if (null == copyRel.getDescription() ||

					copyRel.isNew() ||

					copyRel.getDescription().trim().length()==0) {

				disableObjSync(copyRel);

				try {

					copyRel.setDescription(rel.getObjectId().getId());

					DfLogger.debug(this, getLogId()+": saving copy relation  "+copyRel.getObjectId().getId(), null,null);

					copyRel.save();

				} finally {

					enableObjSync(copyRel);

				}

				

			} else {

				DfLogger.debug(this, getLogId()+": no need to save copy relation  "+copyRel.getObjectId().getId(), null,null);

			}

			DfLogger.debug(this, getLogId()+": setting relation link to "+copyRel.getObjectId().getId(), null,null);

			rel.setDescription(copyRel.getObjectId().getId());

			rel.save();

			DfLogger.debug(this, getLogId()+": relation "+rel.getObjectId().getId() +" saved", null,null);

		} else {

			DfLogger.debug(this, getLogId()+": reprint relation already synced, copy id is "+descr, null,null);

		}

	}



	protected String makeCopyVersionLabels(IDfVersionLabels versionLabels) throws DfException {

		StringBuilder result = new StringBuilder();

		if (versionLabels.hasSymbolicVersionLabel()) {

			String implicit=versionLabels.getImplicitVersionLabel();

			for (int i = 0; i < versionLabels.getVersionLabelCount(); i++) {

				String label = versionLabels.getVersionLabel(i);

				if (label.equals(implicit) /*||"CURRENT".equals(label)*/) {

					continue;

				}

				if (result.length()>0) {

					result.append(",");

				}

				result.append(label);

			}

		}

		return result.length() > 0 ? result.toString():null;

	}



	protected void setImmutable(IDfSysObject obj, boolean value) throws DfException {

		obj.setBoolean("r_immutable_flag", value);

	}

	

	protected void setLinkToCopy(IDfId id) throws DfException {

		if (null!=id && !id.isNull() && id.isObjectId()) {

			DfLogger.debug(this, getLogId()+": setting link to copy '" + id.getId()+"'",null,null);

			this.setString("a_status",id.getId());

		} else {

			DfLogger.debug(this, getLogId()+": removing link to copy ("+id+")",null,null);

			this.setString("a_status","");

		}

	}

	

	public IDfId getLinkToCopy() throws DfException {

		IDfId result = null;

		String idStr = this.getString("a_status");



		if (null!=idStr && idStr.length()==16) {

			result = new DfId(idStr);

			if (result.isNull() || !result.isObjectId()) {

				result = null;

			}

		} 

		return result;

	}

	

	



	

	

	protected void copyDataToObject(IDfSysObject copyObj) throws DfException {

		DfLogger.debug(this, getLogId()+": copying attributes", null,null);

		copyAttrs(copyObj, docAttrMapping);

		if (hasAttrsSet(docAccountAttrMapping)) {

			copyAttrs(copyObj, docAccountAttrMapping);

		}

		if (hasAttrsSet(docCustomerAttrMapping)) {

			copyAttrs(copyObj, docCustomerAttrMapping);

			copyObj.setBoolean("occasional_customer_ind", false);

		}

		//if (getInt("kod_cheshbon_mushee")

		int kodCheshbonMushee = getInt("kod_cheshbon_mushee");

		copyObj.setString("custom_text", "kod_cheshbon_mushee="+kodCheshbonMushee);

		

		// new attributes

		copyObj.setInt("account_cluster_id", -1);



		

		DfLogger.debug(this, getLogId()+": copying aspect attributes", null,null);

		if (hasAttrsSet(docPaperDocMapping)) {

			if (!hasPaperDocAspect(copyObj)) {

				((IDfAspects)copyObj).attachAspect(PAPER_DOC_ASPECT_NAME, null);

				copyObj.fetch(null);

			}

			copyAttrs(copyObj, docPaperDocMapping);

		}

		if (isScanned()) {

			DfLogger.debug(this, getLogId()+": check/create scan event", null,null);

			checkCreateScanEvent(copyObj);

		} else {

			DfLogger.debug(this, getLogId()+": no need for scan event", null,null);

		}

		

		

		if (this.getContentSize() > 0) {

			DfLogger.debug(this, getLogId()+": binding content", null,null);

			ensureContentSaved();

			copyObj.bindFile(0, this.getObjectId(), 0);

		} else {

			DfLogger.debug(this, getLogId()+": no content to bind", null,null);

		}

	}



	protected void checkCopyEvents() throws DfException {

		boolean success = false;

		DfLogger.debug(this, getLogId()+": entering checkCopyEvents()", null,null);

		IDfQuery query = new DfQuery();

		String queryStr = "select r_object_id from "+TFS_PEULA_YOMAN_TYPE+ " WHERE doc_key='"

		+ DfUtil.escapeQuotedString(getString("doc_key"))+"'";

		query.setDQL(queryStr);

		IDfCollection col = null;

		try {

			col = query.execute(this.getSession(), IDfQuery.DF_QUERY);

			while (col.next()) {

				IDfId id = col.getId("r_object_id");

				DfLogger.debug(this, getLogId()+": processing event "+id, null,null);

				ITfsPeulaErua eventObj = (ITfsPeulaErua) getSession().getObject(id);

				if (!eventObj.hasBeenSynced()) {

					DfLogger.debug(this, getLogId()+": synchronizing event "+id, null,null);

					boolean disabledSync = false;

					try {

						if (!this.isSyncDisabled) {

							disabledSync = true;

							this.disableSync();

						}

						eventObj.setDirty(true);

						eventObj.save();

					} finally {

						if (disabledSync) {

							this.enableSync();

						}

					}

				} else {

					DfLogger.debug(this, getLogId()+": already synchronized "+id, null,null);

				}

			}

			success = true;

		} finally {

			if (null!=col) {

				try {col.close();} catch (DfException closeEx) {

					DfLogger.error(this, "error closing collection:", null, closeEx);

				};

			}

			DfLogger.debug(this, getLogId()+": exiting checkCopyEvents(): success="+success, null,null);

		}

		

		

	}



	protected void checkCreateScanEvent(IDfSysObject copyObj) throws DfException {

		String query =CUSTOMER_DOC_EVENT_TYPE +" WHERE event_category_code="+SCAN_EVENT_CATEGORY+" and r_object_id in ("+

		"select child_id from dm_relation where parent_id='"+this.getChronicleId().getId()+"' and order_no > 0)";

		IDfPersistentObject scanEv = getSession().getObjectByQualification(query);

		if (null == scanEv || !scanEventMatches(scanEv)) {

			IDfSysObject newEv = (IDfSysObject)getSession().newObject(CUSTOMER_DOC_EVENT_TYPE);

			copyAttrs(newEv, scanEvAttrMapping);

			newEv.setString("object_name", "scan of "+this.getChronicleId().getId());

			newEv.setBoolean("auto_event_ind", false);

			newEv.setInt("event_category_code",SCAN_EVENT_CATEGORY);

			newEv.setString("event_desc_text", getScanEventDescrByStatus(this.getInt("scan_status")));

			this.linkCopyToProjectFolder(newEv);

			newEv.save();

			

			IDfRelation scanRel = (IDfRelation) getSession().newObject(/*"dm_relation"*/CUSTOMER_DOC_REPRINT_RELATION_SUBTYPE);

			try {

			    disableObjSync(scanRel);

			    scanRel.setRelationName(CUSTOMER_DOC_REPRINT_RELATION);

				scanRel.setChildId(newEv.getObjectId());

				scanRel.setParentId(copyObj.getChronicleId());

				scanRel.setDescription("scan");

				scanRel.setPermanentLink(true);

				scanRel.save();

			} finally {

				enableObjSync(scanRel);

			}

		}

			

			//copyObj.addChildRelative( CUSTOMER_DOC_REPRINT_RELATION+"/"+CUSTOMER_DOC_REPRINT_RELATION_SUBTYPE,

			//		newEv.getObjectId(), null, true, null);

	}

			





	protected String getScanEventDescrByStatus(int code) {

		if (2 == code) {

			return "נסרק בארכיון";

		} else if (3 == code ) {

			return "נסרק בסניף";

		} else {

			return null;

		}

	}



	protected boolean scanEventMatches(IDfPersistentObject scanEv) throws DfException {

		return allAttrsMatch(scanEvAttrMatchMapping,scanEv);

	}



	private boolean allAttrsMatch(String[][] mappings,

			IDfPersistentObject scanEv) throws DfException {

		for (String [] mapping : mappings) {

			String value = getString(mapping[0]);

			String otherValue = scanEv.getString(mapping[0]);

			if ((null!=value && (!value.equals(otherValue))) || (

				value==null && otherValue!=null)) {

				return false;

			}

		}

		return true;

	}



	protected boolean isScanned() throws DfException {

		int scanStatus = getInt("scan_status");

		// scanned at branch or scanned at archive

		return (scanStatus == 2 || scanStatus==3);

	}



	protected void ensureContentSaved() throws DfException {

		DfLogger.debug(this, getLogId()+": checking if need to fix pending changes in content", null, null);

		ContentManager mgr = this.getContentManager();

		try {

			Method verifyNoUnsavedContentChanges = mgr.getClass().getDeclaredMethod("verifyNoUnsavedContentChanges");

			verifyNoUnsavedContentChanges.setAccessible(true);

			verifyNoUnsavedContentChanges.invoke(mgr, new Object[0]);

			DfLogger.debug(this, getLogId()+": no unsaved changes", null, null);

		} catch (InvocationTargetException ex) {

			if (ex.getTargetException()==null ||  !(ex.getTargetException() instanceof IllegalStateException)) {

				DfLogger.warn(this, getLogId()+": exception calling verifyNoUnsavedContentChanges: "+ex.getMessage(), null, null);

			} 

			DfLogger.debug(this, getLogId()+": calling super.doSave() to fix pending changes in content", null, null);

			super.doSave(false, null, null);

		} catch (Exception ex) {

			DfLogger.warn(this, getLogId()+": exception calling verifyNoUnsavedContentChanges: "+ex.getMessage(), null, null);

			DfLogger.debug(this, getLogId()+": calling super.doSave() to fix pending changes in content", null, null);

			super.doSave(false, null, null);

		}

		

	}

	

	protected void ensureContentSavedHack() throws DfException {

		DfLogger.debug(this, getLogId()+": checking if need to fix pending changes in content", null, null);

		IDfCollection col = null;

		Boolean needSave = null;

		try {

			IDfList args = new DfList();

			args.append("PAGE");

			IDfList types = new DfList();

			types.append("I");

			IDfList values = new DfList();

			values.append("0");

			col = this.getSession().apply(this.getObjectId().getId(),

					"ORIGINAL_CONTENT",

					args, types, values);

			while (col.next()) {

				IDfId id = col.getId("result");

				if (id.isNull()) {

					needSave = true;

				} else {

					if (null==needSave) {

						needSave=false;

					}

				}

			}

			if (null==needSave || needSave) {

				DfLogger.debug(this, getLogId()+": calling super.doSave() to fix pending changes in content", null, null);

				super.doSave(false, null, null);

			} else {

				DfLogger.debug(this, getLogId()+": skipping super.doSave()", null, null);

			}

		} finally {

			if (null!=col) {

				try {

					col.close();

				} catch(DfException dfex) {

					DfLogger.error(this, getLogId()+": failes to close collection: "+dfex.getMessage(), null, dfex);

				}

			}

		}

		

	}



	protected boolean hasAttrsSet(String[][] mappings) throws DfException {

		for (String [] mapping: mappings) {

			String srcAttr = mapping[0];

			IDfAttr attr = this.getAttr(srcAttr);

			if ((attr.getDataType() == IDfAttr.DM_INTEGER)) {

				if (this.getInt(srcAttr)>0) {

					return true;

				}

			} else if (attr.getDataType() == IDfAttr.DM_STRING) {

				String value = this.getString(srcAttr);

				if (null!=value && value.trim().length()>0) {

					return true;

				}

			} else if (attr.getDataType() == IDfAttr.DM_TIME) {

				IDfTime value = this.getTime(srcAttr);

				if (null!=value && (!value.isNullDate()) && value.isValid()) {

					return true;

				}

			}

		}

		return false;

	}



	private boolean hasPaperDocAspect(IDfSysObject copyObj) throws DfException {

		IDfAspects aspects = (IDfAspects) copyObj;

		IDfList aspectList = aspects.getAspects();

		return (null!=aspectList &&

				aspectList.findStringIndex(PAPER_DOC_ASPECT_NAME)>-1);

	}



	protected void copyAttrs(IDfPersistentObject copyObj, String[][] mappings) throws DfException {

		for (String [] mapping: mappings) {

			String srcAttr = mapping[0];

			String dstAttr = mapping[1];

			copyObj.setString(dstAttr, this.getString(srcAttr));

		}

	}



	@Override

	public boolean hasBeenSynced() throws DfException {

		return null!=this.getLinkToCopy();

	}



	

	@Override

	public IDfId doCheckin(boolean arg0, String arg1, String arg2, String arg3, String arg4, String arg5, Object [] extraArgs) 

	throws DfException {

		boolean openedTransaction = false;

		boolean success = false;

		boolean skipSync =  isSyncDisabled() || isSyncGloballyDisabled() ||!isDirty();

		try {

			if (!skipSync){

				if (!(this.getSessionManager().isTransactionActive() ||

						this.getSession().isTransactionActive())) {

					openedTransaction = true;

					getSession().beginTrans();

				}

			}

			IDfId newVersionId =  super.doCheckin(arg0, arg1, arg2, arg3, arg4, arg5, extraArgs);

			if (!skipSync) {

				ITfsPeulaDoc newVersionObj = (ITfsPeulaDoc) this.getSession().getObject(newVersionId) ;

				newVersionObj.setString("a_status", "");

				newVersionObj.setDirty(true);				

				newVersionObj.save();

			}

			success = true;

			return newVersionId;

		} finally {

			if (openedTransaction && this.getSession().isTransactionActive()) {

				if (success) {

					this.getSession().commitTrans();

				} else {

					this.getSession().abortTrans();

				}

			}

		} 

	}

	

}



file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\src\com\documentum\fc\client\tfspeula\tbo\TfsPeulaErua.java
-----------------------------------------------------
package com.documentum.fc.client.tfspeula.tbo;


import bnhp.infra.base.sbo.IMigrationConfigService;
import bnhp.infra.base.tbo.ISyncObject;

import com.documentum.fc.client.DfClient;
import com.documentum.fc.client.DfPersistentObject;
import com.documentum.fc.client.DfQuery;
import com.documentum.fc.client.IDfClient;
import com.documentum.fc.client.IDfCollection;
import com.documentum.fc.client.IDfFolder;
import com.documentum.fc.client.IDfPersistentObject;
import com.documentum.fc.client.IDfQuery;
import com.documentum.fc.client.IDfRelation;
import com.documentum.fc.client.IDfSysObject;
import com.documentum.fc.common.DfException;
import com.documentum.fc.common.DfId;
import com.documentum.fc.common.DfLogger;
import com.documentum.fc.common.DfUtil;
import com.documentum.fc.common.IDfId;

public class TfsPeulaErua extends DfPersistentObject implements ITfsPeulaErua {
	protected boolean isSyncDisabled;

	private static final String CUSTOMER_DOC_RELATION = "bnhp_cust_rel";
	private static final String CUSTOMER_DOC_RELATION_SUBTYPE = "bnhp_cust_rel_temp";
	private static final String CUSTOMER_DOC_EVENT_TYPE = "bnhp_doc_event";
	private static final String CUSTOMER_DOC_TYPE = "bnhp_customer_doc";
	private static final String TFS_PEULA_DOC_TYPE = "bnhp_tfs_peula_doc";
	private static final String DOC_KEY= "doc_key";
	
	
	
	private static String[][] eventAttrMapping = {
		//{"create_type_erua","auto_event_ind"},
		{"kod_sug_erua", "event_category_code"},
		{"taarich_erua", "legacy_event_entry_dttm"},
		{"kod_tavla_mekasheret", "concatenated_event_id"},
		{"mpr_zihuy_pakid_mevatzea", "executing_emp_id_code"},
		{"ofen_tipul_erua", "event_category_type_code"},
		{"mispar_tachana", "terminal_channel_id"},
		{"mispar_bankol", "bankol_id"},
		{"shem_pakid_mevatzea", "executing_emp_full_name"},
		{"heara","event_desc_text"}
	};
	
	
	@Override
	public void disableSync() {
		isSyncDisabled = true;
	}

	@Override
	public void enableSync() {
		isSyncDisabled = false;
	}

	@Override
	public boolean isSyncDisabled() {
		return this.isSyncDisabled;
	}

	@Override
	public boolean isSyncGloballyDisabled() throws DfException {
		String propVal = System.getProperty(GLOBAL_SYNC_DISABLED_PROP); 
		if ("true".equalsIgnoreCase(propVal)) {
			return true;
		}
		IDfClient client = DfClient.getLocalClient();
		IMigrationConfigService migrationConfigService = (IMigrationConfigService) client
				.newService(IMigrationConfigService.class.getName(),this.getSessionManager());
		return !migrationConfigService.isSyncOldToNewEnabled(this.getSession());
	}


	protected void linkCopyToProjectFolder(IDfSysObject copyObj, int projectId) throws DfException {
		String docKey = this.getString(DOC_KEY);
		IDfFolder targetFolder = (IDfFolder)getSession().getObjectByQualification("bnhp_doc_folder WHERE project_id="+projectId+" AND document_type='"+copyObj.getTypeName()+"'");
		if (null==targetFolder) {
			throw new DfException("Cannot find project folder for project id="+projectId+" type "+copyObj.getTypeName());
		}
		String linkPath = targetFolder.getFolderPath(0);
		copyObj.link(linkPath);
	}
	
	protected ITfsPeulaDoc getOwnerDoc(String docKey) throws DfException {
		ITfsPeulaDoc result = null;
		if (null!=docKey) {
			result = (ITfsPeulaDoc) this.getSession().getObjectByQualification(TFS_PEULA_DOC_TYPE+
					" WHERE doc_key='"+
					DfUtil.escapeQuotedString(getString(DOC_KEY))+"'");
		}
		return result;
	}
	
	
	protected void setLinkToCopy(IDfPersistentObject target, IDfId id) throws DfException {
		if (null!=id && !id.isNull() && id.isObjectId()) {
			target.setString("a_status",id.getId());
		} else {
			target.setString("a_status","");
		}
	}
	
	protected IDfId getLinkToCopy(IDfPersistentObject target) throws DfException {
		IDfId result = null;
		String idStr = target.getString("a_status");
		if (null!=idStr && idStr.length()==16) {
			result = new DfId(idStr);
			if (result.isNull() || !result.isObjectId()) {
				result = null;
			}
		} 
		return result;
	}
	
	
	
	@Override
	public IDfId doSyncRecursive() throws DfException {
		//boolean haveDisabledSync = false;
		try {
			String docKey = this.getString(DOC_KEY);
			DfLogger.debug(this, getLogId()+": syncronising event for doc_key "+docKey, null,null);
			ITfsPeulaDoc ownerDoc = this.getOwnerDoc(docKey);
			if (null == ownerDoc) {
				throw new DfException("failed to get yoman entry's parent doc with doc_key="+docKey);
			}
			if (!ownerDoc.hasBeenSynced()) {
				DfLogger.debug(this, getLogId()+": synchronizing parent "+docKey, null,null);
				disableSync();
				//haveDisabledSync = true;
				ownerDoc.setDirty(true);
				ownerDoc.save();
				enableSync();
			} else {
				DfLogger.debug(this, getLogId()+": parent "+docKey+" is already synced", null,null);
			}
			IDfId copyOwnerId = ownerDoc.getLinkToCopy();
			if (null == copyOwnerId) {
				throw  new DfException("failed to sync owner document for yoman entry "+this.getObjectId().getId());
			}
			int projectId = ownerDoc.getInt("project_id");
			IDfSysObject copy = null;
			IDfId  copyId = this.getLinkToCopy(this);
			if (null!= copyId) {
				copy = (IDfSysObject) this.getSession().getObject(copyId);
				DfLogger.debug(this, getLogId()+": got copy event "+copy.getObjectId().getId(), null,null);
			} else {
				copy = (IDfSysObject) this.getSession().newObject(CUSTOMER_DOC_EVENT_TYPE);
				DfLogger.debug(this, getLogId()+": created copy event "+copy.getObjectId().getId(), null,null);
				linkCopyToProjectFolder(copy, projectId);
				copyId = copy.getObjectId();
				setLinkToCopy(copy,this.getObjectId());
			}
			try {
				this.disableObjSync(copy);
				copyDataToObject(copy);
				createOrUpdateCopyRelation(copyOwnerId, copy );
				DfLogger.debug(this, getLogId()+": saving copy event "+copy.getObjectId().getId(), null,null);
				copy.save();		
				DfLogger.debug(this, getLogId()+": copy event saved"+copy.getObjectId().getId(), null,null);
				setLinkToCopy(this, copyId);
				return copyId;
			} finally {
				this.enableObjSync(copy);
			}
		} finally {
			if (this.isSyncDisabled) {
				enableSync();
			}
		}
	}

	protected IDfId getChronicleIdForId(IDfId id) throws DfException {
		IDfQuery query = new DfQuery();
		String queryStr = "select i_chronicle_id from dm_sysobject WHERE r_object_id='"+id.toString()+"'";
		query.setDQL(queryStr);
		IDfCollection col = null;
		try {
			col = query.execute(this.getSession(), IDfQuery.DF_QUERY);
			while (col.next()) {
				IDfId chronicleId = col.getId("i_chronicle_id");
				return chronicleId;
			}
			return null;
		} finally {
			if (null!=col) {
				try {col.close();} catch (DfException closeEx) {
					DfLogger.error(this, "error closing collection:", null, closeEx);
				};
			}
		}
	}
	
	protected void disableObjSync(IDfPersistentObject copy) {
		if (copy instanceof ISyncObject) {
			((ISyncObject) copy).disableSync();
		}
	}
	
	protected void enableObjSync(IDfPersistentObject copy) {
		if (copy instanceof ISyncObject) {
			((ISyncObject) copy).enableSync();
		}
	}
	
	
	protected void createOrUpdateCopyRelation(IDfId parent, IDfSysObject child) throws DfException {
		String getRelQuery="dm_relation where relation_name='"+
		CUSTOMER_DOC_RELATION +"' and "+
		"(parent_id='"+parent.getId()+
		"' OR parent_id=(SELECT i_chronicle_id FROM dm_document WHERE r_object_id='"+
		parent.getId()+"')) AND child_id='"+child.getObjectId().getId()+"'"; 
		IDfRelation copyRel = (IDfRelation) getSession().getObjectByQualification(getRelQuery);
		if (null == copyRel) {
			DfLogger.debug(this, getLogId()+ ": creating new relation", null,null);
			IDfId copyChronicleId = this.getChronicleIdForId(parent);		
			copyRel = (IDfRelation) getSession().newObject(CUSTOMER_DOC_RELATION_SUBTYPE);
			DfLogger.debug(this, getLogId()+ ": new relation id is "+copyRel.getObjectId().getId(), null,null);
			disableObjSync(copyRel);
			copyRel.setRelationName(CUSTOMER_DOC_RELATION);
			copyRel.setChildId(child.getObjectId());
			copyRel.setParentId(copyChronicleId);
			copyRel.save();
			DfLogger.debug(this, getLogId()+ ": new relation saved", null,null);
			enableObjSync(copyRel);
		}
	}

	protected void copyDataToObject(IDfSysObject copyObj) throws DfException {
		copyAttrs(copyObj, eventAttrMapping);
		copyObj.setBoolean("auto_event_ind", this.getInt("create_type_erua")==1);
	}
	
	protected void copyAttrs(IDfPersistentObject copyObj, String[][] mappings) throws DfException {
		for (String [] mapping: mappings) {
			String srcAttr = mapping[0];
			String dstAttr = mapping[1];
			copyObj.setString(dstAttr, this.getString(srcAttr));
		}
	}
	
	protected String getLogId() {
		return "bnhp_yoman_eruim_table "+this.getObjectId().getId();
	}
	
	
	@Override
	protected void doSave(boolean arg0, String arg1, Object[] arg2)
			throws DfException {
		boolean openedTransaction = false;
		boolean success = false;
		DfLogger.debug(this, getLogId()+": entered doSave()", null,null);
		boolean isSyncDisabled = false;
		if (isSyncDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, getLogId()+ ": sync disabled locally", null,null);
		}
		if (this.isSyncGloballyDisabled()) {
			isSyncDisabled = true;
			DfLogger.debug(this, getLogId()+ ": sync disabled globally", null,null);
		}
		if (!isDirty()){
			isSyncDisabled = true;
			DfLogger.debug(this, getLogId()+ ": sync disabled since is not dirty", null,null);
		}
		if (this.getString(DOC_KEY)==null ||
				this.getString(DOC_KEY).trim().length()==0){
			isSyncDisabled = true;
			DfLogger.debug(this, getLogId()+ ": sync disabled since doc_key is empty", null,null);
		}
		if (isSyncDisabled) {
			DfLogger.debug(this, getLogId()+ ": doing super.doSave() ", null,null);
			super.doSave(arg0, arg1, arg2);
			DfLogger.debug(this, getLogId()+ ": super.doSave() finished, all done", null,null);
			return;
		}
		
		try {
			if (!(this.getSessionManager().isTransactionActive() ||
					this.getSession().isTransactionActive())) {
				openedTransaction = true;
				DfLogger.debug(this, getLogId()+ ": opening transaction", null,null);
				getSession().beginTrans();
			}
			//if (this.isNew() && this.getContentSize() >0) {
			//	super.doSave(arg0, arg1, arg2);
			//}
			
			this.doSyncRecursive();
			super.doSave(arg0, arg1, arg2);
			success = true;
		}  finally {
			if (success) {
				DfLogger.debug(this, getLogId()+ ": syncronization succeeded", null,null);
			} else {
				DfLogger.debug(this, getLogId()+ ": syncronization failed, look for exceptions in log", null,null);
			}
			if (openedTransaction && this.getSession().isTransactionActive()) {
				if (success) {
					this.getSession().commitTrans();
				} else {
					this.getSession().abortTrans();
				}
			}
		}
	}
	
	@Override
	public boolean hasBeenSynced() throws DfException {
		return null!=this.getLinkToCopy(this);
	}
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\test\resources\__dbor.properties
-----------------------------------------------------
bnhp_tfs_peula_doc=type,com.documentum.fc.client.tfspeula.tbo.TfsPeulaDoc,2.0
bnhp_yoman_eruim_table=type,com.documentum.fc.client.tfspeula.tbo.TfsPeulaErua,2.0
#bnhp_customer_doc=type,com.documentum.fc.client.customerdoc.tbo.CustomerDoc,2.0
#bnhp_cust_rel_temp=type,bnhp.infra.base.tbo.CustomerRelation,2.0
#bnhp.infra.base.sbo.IMigrationConfigService=service,bnhp.infra.base.sbo.MigrationConfigService,2.0


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\BnhpTfsPeulaMigrBOF\version.properties
-----------------------------------------------------
#Sun Apr 26 14:20:23 GMT+02:00 2015
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\build.sh
-----------------------------------------------------
!/bin/sh
    
    echo "job build_and_deploy  started"
    set -x
    cd ./BnhpBusinessDivision  
    mvn -X clean install -DskipTests
    cd ..
    cd ./BnhpInfraDFArtifacts  
    mvn -X clean install -DskipTests
    cd ..


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\.template
-----------------------------------------------------
<?xml version="1.0" encoding="ASCII"?>
<projecttemplate:Template xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:projecttemplate="http:///com.emc.ide.model.ecore/projecttemplate" description="Default Project Template">
  <categoryMap>
    <categories key="com.emc.ide.artifact.acl">
      <value categoryId="com.emc.ide.artifact.acl" sourceRepositoryLocation="sourceRepoPermission Sets" projectLocation="Permission Sets" targetRepositoryLocation="targetRepoPermission Sets" fileName="newacl"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueskillinfo">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueskillinfo" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueskillinfo"/>
    </categories>
    <categories key="com.emc.ide.artifact.formschema">
      <value categoryId="com.emc.ide.artifact.formschema" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newSchema"/>
    </categories>
    <categories key="com.emc.ide.artifact.moduledef">
      <value categoryId="com.emc.ide.artifact.moduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.role">
      <value categoryId="com.emc.ide.artifact.role" sourceRepositoryLocation="sourceRepoRoles" projectLocation="Roles" targetRepositoryLocation="targetRepoRoles" fileName="newrole"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activityTemplateContainer">
      <value categoryId="com.emc.ide.artifact.bpm.activityTemplateContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.storageparameter">
      <value categoryId="com.emc.ide.storageparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyStorageParameter"/>
    </categories>
    <categories key="com.emc.ide.valueparameter">
      <value categoryId="com.emc.ide.valueparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="myvalueparameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.aliasset">
      <value categoryId="com.emc.ide.artifact.aliasset" sourceRepositoryLocation="sourceRepoAlias Sets" projectLocation="Alias Sets" targetRepositoryLocation="targetRepoAlias Sets" fileName="newaliasset"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.implementationJar">
      <value categoryId="com.emc.ide.artifact.jardef.implementationJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.process">
      <value categoryId="com.emc.ide.artifact.bpm.process" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.processContainer">
      <value categoryId="com.emc.ide.artifact.bpm.processContainer" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newbpm"/>
    </categories>
    <categories key="com.emc.ide.artifact.forminstance">
      <value categoryId="com.emc.ide.artifact.forminstance" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newFormInstance"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.interfaceJar">
      <value categoryId="com.emc.ide.artifact.jardef.interfaceJar" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.relation">
      <value categoryId="com.emc.ide.artifact.relation" sourceRepositoryLocation="sourceRepoRelation" projectLocation="Relation" targetRepositoryLocation="targetRepoRelation" fileName="newrelation"/>
    </categories>
    <categories key="com.emc.ide.artifact.daspect">
      <value categoryId="com.emc.ide.artifact.daspect" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newaspecttype"/>
    </categories>
    <categories key="com.emc.ide.artifact.formtemplate">
      <value categoryId="com.emc.ide.artifact.formtemplate" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformtemplate"/>
    </categories>
    <categories key="com.emc.ide.artifact.group">
      <value categoryId="com.emc.ide.artifact.group" sourceRepositoryLocation="sourceRepoGroups" projectLocation="Groups" targetRepositoryLocation="targetRepoGroups" fileName="newgroup"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuecategory">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuecategory" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuecategory"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.activity">
      <value categoryId="com.emc.ide.artifact.bpm.activity" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newactivity"/>
    </categories>
    <categories key="com.emc.ide.artifact.statetype">
      <value categoryId="com.emc.ide.artifact.statetype" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newstatetype"/>
    </categories>
    <categories key="com.emc.ide.artifact.format">
      <value categoryId="com.emc.ide.artifact.format" sourceRepositoryLocation="sourceRepoFormats" projectLocation="Formats" targetRepositoryLocation="targetRepoFormats" fileName="newformat"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.jardef">
      <value categoryId="com.emc.ide.artifact.jardef.jardef" sourceRepositoryLocation="sourceRepoJAR Definitions" projectLocation="JAR Definitions" targetRepositoryLocation="targetRepoJAR Definitions" fileName="newjardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.procedure">
      <value categoryId="com.emc.ide.artifact.procedure" sourceRepositoryLocation="sourceRepoProcedures" projectLocation="Procedures" targetRepositoryLocation="targetRepoProcedures" fileName="newprocedure"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlapplication">
      <value categoryId="com.emc.ide.artifact.xmlapplication" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlapp"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.jardef.javalibrary">
      <value categoryId="com.emc.ide.artifact.jardef.javalibrary" sourceRepositoryLocation="sourceRepoJava Libraries" projectLocation="Java Libraries" targetRepositoryLocation="targetRepoJava Libraries" fileName="newjavalibrary"/>
    </categories>
    <categories key="com.emc.ide.artifact.smartcontainer">
      <value categoryId="com.emc.ide.artifact.smartcontainer" sourceRepositoryLocation="sourceRepoSmart Containers" projectLocation="Smart Containers" targetRepositoryLocation="targetRepoSmart Containers" fileName="newsmartcontainer"/>
    </categories>
    <categories key="com.emc.ide.artifact.dclass">
      <value categoryId="com.emc.ide.artifact.dclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newtype"/>
    </categories>
    <categories key="com.emc.ide.principalparameter">
      <value categoryId="com.emc.ide.principalparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyPrincipalParameter"/>
    </categories>
    <categories key="com.emc.ide.userparameter">
      <value categoryId="com.emc.ide.userparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyUserParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.aspectmoduledef">
      <value categoryId="com.emc.ide.artifact.aspectmoduledef" sourceRepositoryLocation="sourceRepoModules" projectLocation="Modules" targetRepositoryLocation="targetRepoModules" fileName="newaspectmodule"/>
    </categories>
    <categories key="com.emc.ide.artifact.lifecycle">
      <value categoryId="com.emc.ide.artifact.lifecycle" sourceRepositoryLocation="sourceRepoLifecycles" projectLocation="Lifecycles" targetRepositoryLocation="targetRepoLifecycles" fileName="newlifecycle"/>
    </categories>
    <categories key="com.emc.ide.installparameter">
      <value categoryId="com.emc.ide.installparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="InstallParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.job">
      <value categoryId="com.emc.ide.artifact.job" sourceRepositoryLocation="sourceRepoJobs" projectLocation="Jobs" targetRepositoryLocation="targetRepoJobs" fileName="newjob"/>
    </categories>
    <categories key="com.emc.ide.artifact.relationtype">
      <value categoryId="com.emc.ide.artifact.relationtype" sourceRepositoryLocation="sourceRepoRelation Types" projectLocation="Relation Types" targetRepositoryLocation="targetRepoRelation Types" fileName="newrelationtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.report">
      <value categoryId="com.emc.ide.artifact.bam.report" sourceRepositoryLocation="sourceRepoBAM Reports" projectLocation="BAM Reports" targetRepositoryLocation="targetRepoBAM Reports" fileName="newBamReport"/>
    </categories>
    <categories key="com.emc.ide.artifact.foldersubtype">
      <value categoryId="com.emc.ide.artifact.foldersubtype" sourceRepositoryLocation="sourceRepoFolders" projectLocation="Folders" targetRepositoryLocation="targetRepoFolders" fileName="newfolder"/>
    </categories>
    <categories key="com.emc.ide.artifact.bam.config">
      <value categoryId="com.emc.ide.artifact.bam.config" sourceRepositoryLocation="sourceRepoBamConfiguration" projectLocation="BamConfiguration" targetRepositoryLocation="targetRepoBamConfiguration" fileName="BamConfiguration"/>
    </categories>
    <categories key="com.emc.ide.artifact.xmlconfig">
      <value categoryId="com.emc.ide.artifact.xmlconfig" sourceRepositoryLocation="sourceRepoXML Applications" projectLocation="XML Applications" targetRepositoryLocation="targetRepoXML Applications" fileName="newxmlconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.lwdclass">
      <value categoryId="com.emc.ide.artifact.lwdclass" sourceRepositoryLocation="sourceRepoTypes" projectLocation="Types" targetRepositoryLocation="targetRepoTypes" fileName="newlwtype"/>
    </categories>
    <categories key="com.emc.ide.artifact.method">
      <value categoryId="com.emc.ide.artifact.method" sourceRepositoryLocation="sourceRepoMethods" projectLocation="Methods" targetRepositoryLocation="targetRepoMethods" fileName="newmethod"/>
    </categories>
    <categories key="com.emc.ide.folderparameter">
      <value categoryId="com.emc.ide.folderparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyFolderParameter"/>
    </categories>
    <categories key="com.emc.ide.aclparameter">
      <value categoryId="com.emc.ide.aclparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyACLParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuedocprofile">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuedocprofile" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuedocprofile"/>
    </categories>
    <categories key="com.emc.ide.artifact.dardef">
      <value categoryId="com.emc.ide.artifact.dardef" sourceRepositoryLocation="sourceRepo../dar" projectLocation="../dar" targetRepositoryLocation="targetRepo../dar" fileName="newdardef"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.calendar">
      <value categoryId="com.emc.ide.artifact.bpm.calendar" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newcalendar"/>
    </categories>
    <categories key="com.emc.ide.artifact.formadaptorconfig">
      <value categoryId="com.emc.ide.artifact.formadaptorconfig" sourceRepositoryLocation="sourceRepoForms" projectLocation="Forms" targetRepositoryLocation="targetRepoForms" fileName="newformadaptorconfig"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueueuserskill">
      <value categoryId="com.emc.ide.artifact.bpm.workqueueuserskill" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueueuserskill"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueue">
      <value categoryId="com.emc.ide.artifact.bpm.workqueue" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueue"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.workqueuepolicy">
      <value categoryId="com.emc.ide.artifact.bpm.workqueuepolicy" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newworkqueuepolicy"/>
    </categories>
    <categories key="com.emc.ide.artifact.sysobjectsubtype">
      <value categoryId="com.emc.ide.artifact.sysobjectsubtype" sourceRepositoryLocation="sourceRepoSysObjects" projectLocation="SysObjects" targetRepositoryLocation="targetRepoSysObjects" fileName="newsysobject"/>
    </categories>
    <categories key="com.emc.ide.groupparameter">
      <value categoryId="com.emc.ide.groupparameter" sourceRepositoryLocation="sourceRepoInstallation Parameters" projectLocation="Installation Parameters" targetRepositoryLocation="targetRepoInstallation Parameters" fileName="MyGroupParameter"/>
    </categories>
    <categories key="com.emc.ide.artifact.bpm.sdt">
      <value categoryId="com.emc.ide.artifact.bpm.sdt" sourceRepositoryLocation="sourceRepoBPM" projectLocation="BPM" targetRepositoryLocation="targetRepoBPM" fileName="newsdtinfo"/>
    </categories>
  </categoryMap>
</projecttemplate:Template>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\bin\META-INF\maven\com.poalim.documentum\MismachiTipulBelikuim.jar\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Wed May 19 17:28:15 IDT 2021
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\MismachiTipulBelikuim
m2e.projectName=MismachiTipulBelikuim.jar
groupId=com.poalim.documentum
artifactId=MismachiTipulBelikuim.jar
version=1.0-SNAPSHOT


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\bin\META-INF\maven\com.poalim.documentum\MismachiTipulBelikuim.jar\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0-SNAPSHOT</version>
	</parent>

	<artifactId>MismachiTipulBelikuim.jar</artifactId>
	<packaging>jar</packaging>
	<name>MismachiTipulBelikuim</name>
	<description>MismachiTipulBelikuim</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<!--dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency-->

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\bin-dar\keepme.txt
-----------------------------------------------------
keep me


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\bof_package.xml
-----------------------------------------------------
<project name="bof_package" default="buildAndRefresh">
	<!--This scripts rebuild jar files of BOF and copies them into 
		proper place for composer to see updated files. It's 
		supposed to be run from within eclipse ant builder
		
		For DAR file to be properly built this script must be envoked 
		BEFORE building of dar, that is ant builder must be envoked before 
		Documentum Builder in the eclipse build configuration
    -->
	
 
 	<!-- this defines task for building/copiying individual jar -->
 	<macrodef name="build_jar_artifact">
 		<!-- filename of jar to be built -->
 		<attribute name="name"/>
 		<!-- files to put into jar (used by jar task) -->
 		<attribute name="classfilter"/>
 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
 		<attribute name="jardef"/>
 		<sequential>
 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
			<echo message="** rebuilding ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
			<echo message="   class filter: @{classfilter}"/>
			<!--  delete jar file in project root -->
			<delete verbose="false" file="@{name}" failonerror="false"/> 
			 
			<!--  make jar file in project root -->			 
			<jar destfile="@{name}" 
				basedir="target/classes" includes="@{classfilter}">
			</jar>
 			<!--  copy jar to proper location (content/...) that is known to composer -->
 				 			<!-- force tries to override read-only protection (those files must not be stored in VCS) --> 
 							<move file="@{name}" 
 								tofile="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"
 								overwrite="true" 
 								force="true" 
 								failonerror="true"
 				 				verbose="true"/>
 		</sequential>
 	</macrodef>
 		
	<!-- this defines task for deleting individual jar -->
	<macrodef name="clean_jar_artifact">
	 		<!-- filename of jar to be built -->
	 		<attribute name="name"/>
	 		<!-- full path of composer jardef file (usually in Artifacts/JAR Definitions/) -->
	 		<attribute name="jardef"/>
	 		<sequential>
	 			<xmlproperty prefix="@{name}" file="@{jardef}"/>
				<echo message="** cleaning ${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}"/>
							<!--  delete jar file in project root -->
				<delete file="@{name}" failonerror="true"/>
	 			<delete verbose="true" file="${@{name}.Artifact:Artifact.contentStore.contentEntries.value(filePath)}" failonerror="true"/> 
	 		</sequential>
	 	</macrodef>

	<target name="clean">
		<clean_jar_artifact 
			name="MismachiTipulBelikuim.jar" 
			jardef="Artifacts/JAR Definitions/mismachi_tipul_belikuim.jardef"/>
	</target>
	<target name="cleanAndRefresh">
			<antcall target="clean" />
			<!--eclipse.refreshLocal resource="/" depth="infinite"/-->
		</target>
	
	<target name="build">
		<build_jar_artifact 
			name="MismachiTipulBelikuim.jar" 
			classfilter="bnhp/infra/base/aspect/*"
			jardef="Artifacts/JAR Definitions/mismachi_tipul_belikuim.jardef"/>
		<!--<eclipse.refreshLocal resource="/" depth="infinite"/>-->
	</target>
	<target name="buildAndRefresh">
		<antcall target="build" />
		<!--eclipse.refreshLocal resource="/" depth="infinite"/-->
	</target>
</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0-SNAPSHOT</version>
	</parent>

	<artifactId>MismachiTipulBelikuim.jar</artifactId>
	<packaging>jar</packaging>
	<name>MismachiTipulBelikuim</name>
	<description>MismachiTipulBelikuim</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<!--dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency-->

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\src\bnhp\infra\base\aspect\MismachiTipulBelikuim.java
-----------------------------------------------------
package bnhp.infra.base.aspect;
import com.documentum.fc.client.DfDocument;

public class MismachiTipulBelikuim extends DfDocument {
	
}


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\target\classes\META-INF\maven\com.poalim.documentum\MismachiTipulBelikuim.jar\pom.properties
-----------------------------------------------------
#Generated by Maven Integration for Eclipse
#Thu May 20 19:17:44 IDT 2021
m2e.projectLocation=C\:\\Users\\AP068\\git\\documentum\\duecmcustomerdars\\MismachiTipulBelikuim
m2e.projectName=MismachiTipulBelikuim.jar
groupId=com.poalim.documentum
artifactId=MismachiTipulBelikuim.jar
version=1.0-SNAPSHOT


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\target\classes\META-INF\maven\com.poalim.documentum\MismachiTipulBelikuim.jar\pom.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<parent>
		<groupId>com.poalim.documentum</groupId>
		<artifactId>parent</artifactId>
		<version>1.0-SNAPSHOT</version>
	</parent>

	<artifactId>MismachiTipulBelikuim.jar</artifactId>
	<packaging>jar</packaging>
	<name>MismachiTipulBelikuim</name>
	<description>MismachiTipulBelikuim</description>


	<dependencies>
	<dependency>
			<groupId>junit</groupId>
			<artifactId>junit</artifactId>
		</dependency>
		<dependency>
			<groupId>com.poalim.dependencies</groupId>
			<artifactId>documentum-71-deps</artifactId>
			<type>pom</type>
			<scope>provided</scope>
		</dependency>

		<!--dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>BnhpStoragePolicy</artifactId>
		</dependency-->

		<dependency>
			<groupId>com.poalim.documentum</groupId>
			<artifactId>build-dar</artifactId>
			<version>1.0-SNAPSHOT</version>
			<type>xml</type>
			<scope>assembly</scope>
		</dependency>
	</dependencies>


	<build>

		<resources>
			<resource>
				<directory>src</directory>
				<excludes>
					<exclude>**/*.java</exclude>
				</excludes>
			</resource>
		</resources>
		
		<testSourceDirectory>tests/src</testSourceDirectory>
		<testResources>
			<testResource>
				<directory>tests/resources</directory>
			</testResource>
		</testResources>

		<plugins>
			<plugin>
				<artifactId>maven-clean-plugin</artifactId>
				<configuration>
					<filesets>
						<fileset>
							<directory>bin-dar</directory>
							<includes>
								<include>*.dar</include>
							</includes>
							<followSymlinks>false</followSymlinks>
						</fileset>
					</filesets>
				</configuration>
			</plugin>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-antrun-plugin</artifactId>
				<executions>
					<execution>
						<id>run-bof_package-clean</id>
						<phase>clean</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-bof_package-build</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
					<execution>
						<id>run-build-dar</id>
						<phase>package</phase>
						<goals>
							<goal>run</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>build-helper-maven-plugin</artifactId>
				<version>1.9.1</version>
				<executions>
					<execution>
						<id>attach-dar</id>
						<phase>package</phase>
						<goals>
							<goal>attach-artifact</goal>
						</goals>
					</execution>
				</executions>

			</plugin>
			<plugin>
				<artifactId>maven-resources-plugin</artifactId>
				<executions>
					<execution>
						<id>copy-resources</id>

						<phase>process-classes</phase>
						<goals>
							<goal>copy-resources</goal>
						</goals>
						<configuration>
							<outputDirectory>bin</outputDirectory>
							<resources>
								<resource>
									<directory>target/classes</directory>
								</resource>
							</resources>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>

	</build>

</project>


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\MismachiTipulBelikuim\version.properties
-----------------------------------------------------
#Sun Jan 07 08:20:18 IST 2018
composer.version=7.1.0150.0035


file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\README.md
-----------------------------------------------------
### This repo contains all DARs that are related to CustomerDoc Service
   
## How to setup build process

## how to deploy to environments

## how to manage changes




file Read:C:\Users\AP068\git\documentum\duecmcustomerdars\settings.xml
-----------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>
<settings xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd" xmlns="http://maven.apache.org/SETTINGS/1.0.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <servers>
    <server>
      <username>${env.ARTIFACTORY_USER}</username>
      <password>${env.ARTIFACTORY_PASSWORD}</password>
      <id>central</id>
      <configuration>
	<httpConfiguration>
		<get>
			<usePreemptive>true</usePreemptive>
		</get>
	</httpConfiguration>
      </configuration>
    </server>
    <server>
      <username>${env.ARTIFACTORY_USER}</username>
      <password>${env.ARTIFACTORY_PASSWORD}</password>
      <id>snapshots</id>
      <configuration>
	<httpConfiguration>
		<get>
			<usePreemptive>true</usePreemptive>
		</get>
	</httpConfiguration>
      </configuration>
    </server>
    <server>
      <username>${env.ARTIFACTORY_USER}</username>
      <password>${env.ARTIFACTORY_PASSWORD}</password>
      <id>artifactory</id>
      <configuration>
	<httpConfiguration>
		<get>
			<usePreemptive>true</usePreemptive>
		</get>
	</httpConfiguration>
      </configuration>
    </server>
    <server>
      <username>${env.ARTIFACTORY_USER}</username>
      <password>${env.ARTIFACTORY_PASSWORD}</password>
      <id>poalim-core</id>
      <configuration>
	<httpConfiguration>
		<get>
			<usePreemptive>true</usePreemptive>
		</get>
	</httpConfiguration>
      </configuration>
    </server>
  </servers>
  <profiles>
    <profile>
      <repositories>
        <repository>
          <snapshots>
            <enabled>true</enabled>
			<updatePolicy>always</updatePolicy>
          </snapshots>
          <id>central</id>
          <name>libs-release</name>
          <url>https://repo.devops.poalim.bank/artifactory/s-28008-repo</url>
        </repository>
        <repository>
          <snapshots />
          <id>snapshots</id>
          <name>libs-snapshot</name>
          <url>https://repo.devops.poalim.bank/artifactory/s-28008-repo</url>
        </repository>
        <repository>
          <snapshots />
          <id>poalim_core</id>
          <name>poalim-core</name>
          <url>https://repo.devops.poalim.bank/artifactory/poalim-core</url>
        </repository>
      </repositories>
      <pluginRepositories>
        <pluginRepository>
          <snapshots>
            <enabled>false</enabled>
          </snapshots>
          <id>central</id>
          <name>plugins-release</name>
          <url>https://repo.devops.poalim.bank/artifactory/s-28008-repo</url>
        </pluginRepository>
        <pluginRepository>
          <snapshots />
          <id>snapshots</id>
          <name>plugins-snapshot</name>
          <url>https://repo.devops.poalim.bank/artifactory/s-28008-repo</url>
        </pluginRepository>
      </pluginRepositories>
      <id>artifactory</id>
      <properties>
        <headlesscomposer.location>/home/FW0/java/HeadlessComposer/</headlesscomposer.location>
        <java.home.for.dfs.build>/home/FW0/java/jdk-1.6.0_27</java.home.for.dfs.build>
        <downloadSources>true</downloadSources>
        <downloadJavadocs>true</downloadJavadocs>
      </properties>
    </profile>

  </profiles>
  <activeProfiles>
    <activeProfile>artifactory</activeProfile>
  </activeProfiles>

</settings>


total files:122
